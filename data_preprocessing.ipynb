{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport spacy\nimport string\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nreviews_file = ''\nw2v_file = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('.csv'): \n            reviews_file = file_name\n        elif file_name.endswith('.bin') or ('.gz'):\n            w2v_file = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Amazon reviews file: {reviews_file}')\nprint(f'Google news word to vec file: {w2v_file}') #<< faster than doing api.load\n            \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('VERBOSE:', text)\n    return\n\nshowCellCompletion = False\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Only run once\n#>> Seems to need to be rerun after every Kaggle timeout.\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data from CSV\n#>>  3/13 needed to change the path as below\n# path ='/kaggle/input/Reviews.csv'#\"/kaggle/input/amazon-product-reviews/Reviews.csv\"\ndata = pd.read_csv(reviews_file) # Use pandas to analyze data\nshowD('Amazon reviews loaded into Panda')\n\n# print number of rows in our ratings column\nprintv(f'Number of reviews: {len(data[\"Score\"])}')\nprintv(f'Column names -\\n {data.columns}\\n') \nprintv(f'First five rows -\\n{data.head()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for empty entries or missing data in each column\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        print(f\"Column '{column}' has empty entries or missing data.\")\n    else:\n        print(f\"Column '{column}' has no empty entries or missing data.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get count of ratings \nrating_counts = data['Score'].value_counts()\n\n# Sort counts by index ascending\nrating_counts = rating_counts.sort_index()  \n\n# Create bar plot\nax = rating_counts.plot(kind = 'bar')\n\nax.set_title(\"Ratings Distribution\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Occurrences\")\n\n# Fix x-axis tick labels\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0) \n\nfor rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show() #<< show the rating in each of the 5 categories","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_data_size = 10 #25000<< number of reviews in each rating category, tailored for CPU capacity\n# Specify the column for sorting and balancing\nsort_column = 'Score'  # This is one the rating column\n\n# Sort the data by the rating values\nsorted_data = data.sort_values(by = sort_column)\n\n# Create a balanced dataset with 25,000 samples from each class\n#balanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n=25000))\n\n#>> DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. \n#>> This behavior is deprecated, and in a future version of pandas the grouping \n#>> columns will be excluded from the operation. \n#>> Either pass `include_groups=False` to exclude the groupings or \n#>> explicitly select the grouping columns after groupby to silence this warning.\n#\nbalanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n = balanced_data_size))\n\n#>> Does this mean to reset the row numbers?? ##Columns Numbers\nbalanced_data.reset_index(drop = True, inplace = True)\n\nprintv(f\"The number of reviews equally distributed across all ratings is {len(balanced_data['Score'])}\")\n\n# Get count of ratings\nrating_counts = balanced_data['Score'].value_counts()\n\n# Create bar plot\nax = rating_counts.plot(kind='bar')\n\nax.set_title(\"Ratings Distribution After Balancing\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Samples\")\n\n# Fix x-axis ticks  \nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)\n\n# Print number of reviews per rating\n\nif DEV:\n    for rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the English tokenizer model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Compile the regular expressions\nhtml_tags = re.compile(\"<.*?>\")\npunct_and_symbols = re.compile(r'[^\\w\\s\\']+')\n\n# custom stop words to remove\ncustom_stop_words = ['a', 'of']\n\ndef tokenizer(text, nlp):\n    \"\"\"\n    Tokenizes a text string and removes stop words.\n    \n    text (str): The text string to tokenize.\n    nlp: The Spacy language model.\n\n    Returns:\n        list: The tokenized text string.\n    \"\"\"\n    text = text.lower()  # Convert text to lowercase\n    text = html_tags.sub(\"\", text)  # Remove HTML tags\n    text = punct_and_symbols.sub(\"\", text)  # Remove punctuation and symbols\n\n    # Tokenize the text\n    doc = nlp(text)\n\n    # Remove stop words and punctuation tokens\n    tokens = [token.text for token in doc if not token.is_punct \n              and token.text not in custom_stop_words]\n\n    return tokens\n\n# Assuming balanced_data is a pandas DataFrame and 'Text' is a column containing the reviews\n# Apply the function to the Text column and store it in a new column\nbalanced_data['CleanedReview'] = balanced_data['Text'].apply(lambda x: tokenizer(x, nlp))\n\n# Show that cell has finished executing\nshowC(f'{tokenizer} defined, and then used to create CleanedReview column')\n\n# Store the Rating column\nrating = balanced_data['Score']  \n\n# Store the CleanedReview column\ntokenized_review = balanced_data['CleanedReview']\nshowD(f'specify the columns that will be used to train the classifier')\n\nprint(balanced_data['Text'][0],'\\n')\nprint(tokenized_review[0], '\\n')\nprint(len(tokenized_review[0]), '\\n')\nprint(tokenized_review.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\ndef analyze_review_tokens(reviews):\n    token_counts = [len(review) for review in reviews]\n\n    mean = np.mean(token_counts)\n    median = np.median(token_counts)\n    mode = stats.mode(token_counts)\n\n    print(f\"Mean: {mean:.2f}\")\n    print(f\"Median: {median:.2f}\")\n    print(f\"Mode: {mode}\")\n\n    plt.figure(figsize=(8, 6))\n    plt.hist(token_counts, bins=20, edgecolor='black')\n    plt.xlabel('Number of Tokens')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Token Counts per Review')\n    plt.grid(True)\n    plt.show()\n\nanalyze_review_tokens(tokenized_review)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# >> 3/29/24 Took less than a minute to run without the accelerator.\n# Load Word2Vec model\n#w2v = api.load('word2vec-google-news-300')\nw2v = KeyedVectors.load_word2vec_format (w2v_file, binary=True)\n\n# Define the aimum sequence length (adjust as needed)\n#>> Will increasing max_sequence_length impact performance?\nmax_sequence_length = 100\n\nshowD(f'{w2v} can map words onto vectors with 300 dimensions')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n    accelerator = True\n\nelse:\n    accelerator = False\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n    print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#>> 3/29/24 Completed in less than 30 seconds using 1347 as max_sequence_length \n\n# Assume you have a list of tokenized review called tokenized_review\n# Each element in tokenized_review is a list of tokens for a single review\n\nlengths = []\nfor review_tokens in tokenized_review:\n    lengths.append(len(review_tokens))\n\nlengths = sorted(lengths)\nlengths = lengths[-1:0:-1]\nshowD(f'Lengths of 100 longest reviews: {lengths[0:100]}')        \n\nmax_sequence_length = 100 #<< 3/29/24 1347 was the longest review length in sample\n\n# Initialize an empty tensor for padded reviews on the GPU\npadded_reviews = torch.zeros((len(tokenized_review), max_sequence_length, 300))\n\n# Initialize a mask tensor of the same shape as padded_reviews\n# Set it to False (or 0) initially, representing that all positions are padding\n# mask = torch.zeros_like(padded_reviews, dtype=torch.bool)\n\n# Initialize a mask tensor of the same shape as padded_reviews but with only sequence length\nmask = torch.zeros((len(tokenized_review), max_sequence_length), dtype=torch.bool)\n\nout_words = {}\nwords_in = 0\nwords_out = 0\n\n# Now, during the padding and embedding conversion loop, update the mask as well\nfor i, review_tokens in enumerate(tokenized_review):\n    review_length = min(len(review_tokens), max_sequence_length)\n    for j in range(review_length):\n        word = review_tokens[j]\n        if word in w2v:\n            words_in += 1\n            # Use Word2Vec vector if available\n            padded_reviews[i, j, :] = torch.tensor(w2v[word])\n            mask[i, j] = True  # Update the mask to indicate the presence of a word\n        else:\n            words_out += 1\n            out_words[word] = out_words.get(word, 0) + 1\n        # Otherwise, the mask remains False (or 0) for padding\n\n# Now you have a mask tensor that you can use later in your processing to ignore padded values\n# For example, if you want to apply max pooling only on non-padded values, you could use:\n# review_embeddings = torch.max(padded_reviews * mask.unsqueeze(-1).float(), dim=1)[0]\n\n# Ensure the mask is treated as a float for any operations that require it\nmask = mask.float()\n\n# Now you can use this mask tensor to exclude the padding from any subsequent computations\n\nprintv(f'{words_in} words found with vector representations, {words_out} without')\nprintv(f'Number of unique words without vector representations: {len(out_words)}')\nprintd('Sample of words without vector representations')\nkwords = list(out_words.keys())\nfor idx in range(0,len(kwords)-1,100):\n    kword = kwords[idx]\n    printd(f'{kword} - {out_words[kword]}')\n# Apply max pooling to aggregate embeddings along the sequence dimension\n# review_embeddings = torch.max(padded_reviews, dim=1)[0]\n\n# Now,review_embeddings contains the aggregated Word2Vec \n# embeddings for each review on the GPU\n\nshowC(f\"Created zero-padded, standard length reviews\")\n\nprint(padded_reviews[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#>> padded_reviews are 100 x 300 tensors, zero padded if necessary\n#>> to get the standard lenth\ntext_embeddings_tensors = padded_reviews.to(device)\n\n# Rating labels\nrating_labels_tensors = torch.tensor(rating.values).to(device)\n\n# Dataset\ndataset = TensorDataset(text_embeddings_tensors, rating_labels_tensors)\nshowC(f'{dataset} defined')\n\nprint(text_embeddings_tensors.shape)\nprint(rating_labels_tensors.shape)\nprint(text_embeddings_tensors.device)\nprint(rating_labels_tensors.device)\nprint(dataset)\n\nprintM() # print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\nprintNv() #!nvidia-smi\nprint('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\nif accelerator and showNv:\n    print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lengths \ntrain_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\n\n# Random split\ntrain_data, val_data = random_split(dataset, [train_len, val_len])\n\nprintv(f\"The amount of data we have to train with is {len(train_data)} revieww\") \nprintv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n#print(f\"The amount of data we have to validate with is on {train_data.device}\")\n#print(f\"The amount of data we have to validate with is on {val_data.device}\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}