{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n\nelse:\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nreviews_file = ''\nw2v_file = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('.csv'): \n            reviews_file = file_name\n        elif file_name.endswith('.bin') or ('.gz'):\n            w2v_file = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Amazon reviews file: {reviews_file}')\nprint(f'Google news word to vec file: {w2v_file}')\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = False\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('VERBOSE:', text)\n    return\n\nshowCellCompletion = False\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = False\naccelerator = False\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data from CSV\n#>>  3/13 needed to change the path as below\n# path ='/kaggle/input/Reviews.csv'#\"/kaggle/input/amazon-product-reviews/Reviews.csv\"\ndata = pd.read_csv(reviews_file) # Use pandas to analyze data\nshowD('Amazon reviews loaded into Panda')\n\n# print number of rows in our ratings column\nprintv(f'Number of reviews: {len(data[\"Score\"])}')\nprintv(f'Column names -\\n {data.columns}\\n') \nprintv(f'First five rows -\\n{data.head()}')\n\n# Check for empty entries or missing data in each column\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        print(f\"Column '{column}' has empty entries or missing data.\")\n    else:\n        print(f\"Column '{column}' has no empty entries or missing data.\")\n\n# Get count of ratings \nrating_counts = data['Score'].value_counts()\n\n# Sort counts by index ascending\nrating_counts = rating_counts.sort_index()  \n\n# Create bar plot\nax = rating_counts.plot(kind = 'bar')\n\nax.set_title(\"Ratings Distribution\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Occurrences\")\n\n# Fix x-axis tick labels\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0) \n\nfor rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show() #<< show the rating in each of the 5 categories\n\nbalanced_data_size = 5000 #25000<< number of reviews in each rating category, tailored for CPU capacity\n# Specify the column for sorting and balancing\nsort_column = 'Score'  # This is one the rating column\n\n# Sort the data by the rating values\nsorted_data = data.sort_values(by = sort_column)\n\n# Create a balanced dataset with 25,000 samples from each class\n#balanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n=25000))\n\n#>> DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. \n#>> This behavior is deprecated, and in a future version of pandas the grouping \n#>> columns will be excluded from the operation. \n#>> Either pass `include_groups=False` to exclude the groupings or \n#>> explicitly select the grouping columns after groupby to silence this warning.\n#\nbalanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n = balanced_data_size))\n\n#>> Does this mean to reset the row numbers?? ##Columns Numbers\nbalanced_data.reset_index(drop = True, inplace = True)\n\nprintv(f\"The number of reviews equally distributed across all ratings is {len(balanced_data['Score'])}\")\n\n# Get count of ratings\nrating_counts = balanced_data['Score'].value_counts()\n\n# Create bar plot\nax = rating_counts.plot(kind='bar')\n\nax.set_title(\"Ratings Distribution After Balancing\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Samples\")\n\n# Fix x-axis ticks  \nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)\n\n# Print number of reviews per rating\n\nif DEV:\n    for rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show()\n\n\nimport spacy\nimport re\nimport pandas as pd\n\n# Load the English tokenizer model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Compile the regular expressions\nhtml_tags = re.compile(\"<.*?>\")\npunct_and_symbols = re.compile(r'[^\\w\\s\\']+')\n\ndef tokenizer(text, nlp):\n    \"\"\"\n    Tokenizes a text string and removes stop words.\n    \n    text (str): The text string to tokenize.\n    nlp: The Spacy language model.\n\n    Returns:\n        list: The tokenized text string.\n    \"\"\"\n    text = text.lower()  # Convert text to lowercase\n    text = html_tags.sub(\"\", text)  # Remove HTML tags\n    text = punct_and_symbols.sub(\"\", text)  # Remove punctuation and symbols\n\n    # Tokenize the text\n    doc = nlp(text)\n\n    # Remove stop words and punctuation tokens\n    # tokens = [token.text for token in doc if not token.is_punct]# and not token.is_stop]\n    tokens = [token.text for token in doc if not token.is_punct]# and not token.is_stop]\n\n    return tokens\n\n# Assuming balanced_data is a pandas DataFrame and 'Text' is a column containing the reviews\n# Apply the function to the Text column and store it in a new column\nbalanced_data['CleanedReview'] = balanced_data['Text'].apply(lambda x: tokenizer(x, nlp))\n\n# Show that cell has finished executing\nshowC(f'{tokenizer} defined, and then used to create CleanedReview column')\n\n# Store the Rating column\nrating = balanced_data['Score']  \n\n# Store the CleanedReview column\ntokenized_review = balanced_data['CleanedReview']\nshowD(f'specify the columns that will be used to train the classifier')\n\nprint(balanced_data['Text'][0],'\\n')\nprint(tokenized_review[0], '\\n')\nprint(len(tokenized_review[0]), '\\n')\nprint(tokenized_review.shape)\n\nfrom scipy import stats\n\ndef analyze_review_tokens(reviews):\n    token_counts = [len(review) for review in reviews]\n\n    mean = np.mean(token_counts)\n    median = np.median(token_counts)\n    mode = stats.mode(token_counts)\n\n    print(f\"Mean: {mean:.2f}\")\n    print(f\"Median: {median:.2f}\")\n    print(f\"Mode: {mode}\")\n\n    plt.figure(figsize=(8, 6))\n    plt.hist(token_counts, bins=20, edgecolor='black')\n    plt.xlabel('Number of Tokens')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Token Counts per Review')\n    plt.grid(True)\n    plt.show()\n\nanalyze_review_tokens(tokenized_review)\n\n# >> 3/29/24 Took less than a minute to run without the accelerator.\n# Load Word2Vec model\n#w2v = api.load('word2vec-google-news-300')\nw2v = KeyedVectors.load_word2vec_format (w2v_file, binary=True)\n\n# Define the aimum sequence length (adjust as needed)\n#>> Will increasing max_sequence_length impact performance?\nmax_sequence_length = 100\n\nshowD(f'{w2v} can map words onto vectors with 300 dimensions')\n\n\n#>> 3/29/24 Completed in less than 30 seconds using 1347 as max_sequence_length \n\n# Assume you have a list of tokenized review called tokenized_review\n# Each element in tokenized_review is a list of tokens for a single review\n\nlengths = []\nfor review_tokens in tokenized_review:\n    lengths.append(len(review_tokens))\n\nlengths = sorted(lengths)\nlengths = lengths[-1:0:-1]\nshowD(f'Lengths of 100 longest reviews: {lengths[0:100]}')        \n\nmax_sequence_length = 100 #<< 3/29/24 1347 was the longest review length in sample\n\n# Initialize an empty tensor for padded reviews on the GPU\npadded_reviews = torch.zeros((len(tokenized_review), max_sequence_length, 300))\n\n# Initialize a mask tensor of the same shape as padded_reviews\n# Set it to False (or 0) initially, representing that all positions are padding\n# mask = torch.zeros_like(padded_reviews, dtype=torch.bool)\n\n# Initialize a mask tensor of the same shape as padded_reviews but with only sequence length\nmask = torch.zeros((len(tokenized_review), max_sequence_length), dtype=torch.bool)\n\nout_words = {}\nwords_in = 0\nwords_out = 0\n\n# Now, during the padding and embedding conversion loop, update the mask as well\nfor i, review_tokens in enumerate(tokenized_review):\n    review_length = min(len(review_tokens), max_sequence_length)\n    for j in range(review_length):\n        word = review_tokens[j]\n        if word in w2v:\n            words_in += 1\n            # Use Word2Vec vector if available\n            padded_reviews[i, j, :] = torch.tensor(w2v[word])\n            mask[i, j] = True  # Update the mask to indicate the presence of a word\n        else:\n            words_out += 1\n            out_words[word] = out_words.get(word, 0) + 1\n        # Otherwise, the mask remains False (or 0) for padding\n\n# Now you have a mask tensor that you can use later in your processing to ignore padded values\n# For example, if you want to apply max pooling only on non-padded values, you could use:\n# review_embeddings = torch.max(padded_reviews * mask.unsqueeze(-1).float(), dim=1)[0]\n\n# Ensure the mask is treated as a float for any operations that require it\nmask = mask.float()\n\n# Now you can use this mask tensor to exclude the padding from any subsequent computations\n\nprintv(f'{words_in} words found with vector representations, {words_out} without')\nprintv(f'Number of unique words without vector representations: {len(out_words)}')\nprintd('Sample of words without vector representations')\nkwords = list(out_words.keys())\nfor idx in range(0,len(kwords)-1,100):\n    kword = kwords[idx]\n    printd(f'{kword} - {out_words[kword]}')\n# Apply max pooling to aggregate embeddings along the sequence dimension\n# review_embeddings = torch.max(padded_reviews, dim=1)[0]\n\n# Now,review_embeddings contains the aggregated Word2Vec \n# embeddings for each review on the GPU\n\nshowC(f\"Created zero-padded, standard length reviews\")\n\nprint(padded_reviews[1])\n\n#>> padded_reviews are 100 x 300 tensors, zero padded if necessary\n#>> to get the standard lenth\ntext_embeddings_tensors = padded_reviews.to(device)\n\n# Rating labels\nrating_labels_tensors = torch.tensor(rating.values).to(device)\n\n# Dataset\ndataset = TensorDataset(text_embeddings_tensors, rating_labels_tensors)\nshowC(f'{dataset} defined')\n\n# shape of the tensors\nprint(text_embeddings_tensors.shape)\nprint(rating_labels_tensors.shape)\n\n# location of the tensors\nprint(text_embeddings_tensors.device)\nprint(rating_labels_tensors.device)\n\nprint(dataset)\n\n# DataLoader for training data\ndata_loader = DataLoader(dataset, batch_size = 32, shuffle = True)  # Use shuffle for training\n\nprintM() # print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\nprintNv() #!nvidia-smi\nprint('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\nif accelerator and showNv:\n    print(torch.cuda.memory_summary())\n\n# #>> padded_reviews are 100 x 300 tensors, zero padded if necessary\n# #>> to get the standard lenth\n# text_embeddings_tensors = padded_reviews.to(device)\n\n# # Rating labels\n# rating_labels_tensors = torch.tensor(rating.values).to(device)\n\n# # Dataset\n# dataset = TensorDataset(text_embeddings_tensors, rating_labels_tensors)\n# showC(f'{dataset} defined')\n\n# print(text_embeddings_tensors.shape)\n# print(rating_labels_tensors.shape)\n# print(text_embeddings_tensors.device)\n# print(rating_labels_tensors.device)\n# print(dataset)\n\n# # Lengths \n# train_len = int(0.8 * len(dataset))\n# val_len = len(dataset) - train_len\n\n# # Random split\n# train_data, val_data = random_split(dataset, [train_len, val_len])\n\n# printv(f\"The amount of data we have to train with is {len(train_data)} revieww\") \n# printv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n# #print(f\"The amount of data we have to validate with is on {train_data.device}\")\n# #print(f\"The amount of data we have to validate with is on {val_data.device}\")\n\n# # DataLoader for training data\n# train_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# # DataLoader for validation data\n# val_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100  # Maximum sequence length\ninput_size = d_model  # based on the output size of your feed-forward network\n\nnum_layers = 1 # 4 # Number of encoder layers\nh       = 1 # 10   # number of attention head\nd_ffn   = 1024 # dimension of the feedforward layer\n\ndropout = 0.0 # 0.1  # You can adjust the dropout if needed\neps     = 1e-6 # epsilon value to prevent the standard deviation from becoming zero\nlearning_rate = 0.01\n\n\"\"\"\nTo ensure compatibility, it's important to choose the \nnumber of attention heads (h) such that d_model is \nevenly divisible by h in the multi-head attention \nmodule's self.d_k. This allows for a clean distribution \nof the model dimensionality across the attention heads.\n\"\"\"\n\n# d_model / attn.h = 300 / 10 = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model: int, dropout: float, seq_len: int):\n        \"\"\"\n        Initialize the PositionalEncoding module.\n\n        Args:\n            d_model (int): The dimensionality of the model (embedding size).\n            dropout (float): The dropout rate to be applied.\n            seq_len (int): The maximum sequence length.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(p = dropout)\n\n        # Create a tensor of shape (seq_len, 1) representing the positions\n        position = torch.arange(seq_len).unsqueeze(1)\n        \n        # Compute the denominator term for the sinusoidal positional encoding\n        # The denominator is 10000^(2i/d_model), where i is the position index\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        \n        # Create a tensor of shape (seq_len, 1, d_model) to store the positional encodings\n        pe = torch.zeros(seq_len, 1, d_model)\n        \n        # Compute the sinusoidal positional encodings for even dimensions\n        # pe[:, 0, 0::2] selects the even dimensions (0, 2, 4, ...) of the positional encoding tensor\n        # position * div_term computes the angle for each position and even dimension\n        # torch.sin applies the sine function element-wise to compute the positional encoding values\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        \n        # Compute the sinusoidal positional encodings for odd dimensions\n        # pe[:, 0, 1::2] selects the odd dimensions (1, 3, 5, ...) of the positional encoding tensor\n        # torch.cos applies the cosine function element-wise to compute the positional encoding values\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        \n        # Register the positional encoding tensor as a buffer in the module\n        # This ensures that the positional encoding tensor is saved and loaded with the module\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Perform the forward pass of the PositionalEncoding module.\n\n        Args:\n            x (Tensor): The input tensor of shape (seq_len, batch_size, embedding_dim).\n\n        Returns:\n            Tensor: The input tensor with positional encodings added.\n        \"\"\"\n        # Add positional encodings to the input tensor\n        # x.size(0) returns the sequence length dimension of the input tensor\n        # self.pe[:x.size(0)] selects the positional encodings corresponding to the sequence length\n        x = x + self.pe[:x.size(0)]\n        \n        # Apply dropout to the tensor with positional encodings\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=dropout):\n        super().__init__()\n        \n        # Number of attention heads\n        self.h = h\n        \n        # Dimensionality of each attention head\n        self.d_k = d_model // h\n        \n        # Linear layers for query, key, value, and output projections\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p = dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n        # Compute the dimensionality of each attention head\n        d_k = query.size(-1)\n        \n        # Compute the attention scores using the dot product between query and key\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        # Apply the mask to the attention scores (if provided)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Apply softmax to obtain the attention probabilities\n        p_attn = torch.softmax(scores, dim = -1)\n        \n        # Apply dropout to the attention probabilities (if specified)\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n        \n        # Compute the weighted sum of values using the attention probabilities\n        # Return the attended values and attention probabilities\n        return torch.matmul(p_attn, value), p_attn\n\n    def forward(self, query, key, value, mask = None):\n        # Get the number of batches\n        nbatches = query.size(0)\n        \n        # Project and reshape the query, key, and value for multi-head attention\n        # The projections are done using the linear layers defined in __init__\n        query, key, value = [\n            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n        \n        # Perform scaled dot-product attention on the projected query, key, and value\n        x, attn = self.scaled_dot_product_attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        # Reshape the attended output and concatenate the attention heads\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        \n        # Apply a final linear projection to the concatenated output\n        return self.linears[-1](x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nn.ModuleList is a container provided by PyTorch that holds a list of modules. It allows you to store multiple layers or submodules in a list-like structure while still keeping them as separate modules.\n\nnn.Linear(d_model, d_model) creates a fully connected linear layer. It takes an input of size d_model and produces an output of the same size d_model. In other words, it performs a linear transformation on the input using a weight matrix and a bias vector.\n\n[nn.Linear(d_model, d_model) for _ in range(4)] is a list comprehension that creates a list of four identical linear layers. The underscore _ is used as a placeholder variable since we don't need to use the loop variable.\n\nself.linears = ... assigns the ModuleList containing the four linear layers to the linears attribute of the class instance (self).\n\nSo, in summary, this line of code creates a ModuleList named self.linears that contains four separate fully connected linear layers. Each linear layer takes an input of size d_model and produces an output of the same size d_model.\n\nThis is commonly used in transformer-based architectures, such as the transformer encoder or decoder, where multiple identical linear layers are applied in parallel or sequentially to the input. The specific use case would depend on the surrounding code and the architecture being implemented.\n\n\n\nWhat are the linear layers? Are they matrices since they have two arguments/parameters of size d_model? Are we creating 4 matrices of d_model by d_model dimensions?\n\n\n\nYes, you're correct! In PyTorch, a linear layer (nn.Linear) represents a fully connected layer that performs a linear transformation on the input using a weight matrix and a bias vector.\n\nWhen you create a linear layer with nn.Linear(d_model, d_model), you are essentially creating a weight matrix of size (d_model, d_model) and a bias vector of size (d_model,). The weight matrix is randomly initialized, and the bias vector is initialized to zeros by default.\n\nSo, in the code [nn.Linear(d_model, d_model) for _ in range(4)], you are creating a list of four separate linear layers, each with its own weight matrix and bias vector. Each matrix has dimensions (d_model, d_model).\n\nDuring the forward pass, when you pass an input tensor through one of these linear layers, the layer performs the following operation:\n\noutput = input @ weight^T + bias\n\nHere, input is a tensor of shape (..., d_model), weight is the weight matrix of shape (d_model, d_model), and bias is the bias vector of shape (d_model,). The @ symbol represents matrix multiplication, and ^T denotes the transpose operation.\n\nSo, each linear layer applies a linear transformation to the input tensor using its own weight matrix and bias vector, producing an output tensor of the same shape as the input.\n\nBy creating four separate linear layers and storing them in a ModuleList, you can apply these transformations independently or combine them as needed in your model architecture.","metadata":{}},{"cell_type":"code","source":"def add_layer(x, y):\n    \"\"\"Adds two tensors together.\n\n    Args:\n    x: A torch.Tensor of shape (batch_size, seq_len, hidden_size).\n    y: A torch.Tensor of the same shape as x.\n\n    Returns:\n    A torch.Tensor of the same shape as x and y, containing the sum of the two tensors.\n    \"\"\"\n\n    return torch.add(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\"\n    Construct a layernorm module (See citation for details).\n    \n    Layer normalization is a technique to normalize the activations of a layer.\n    It helps stabilize the training process and can lead to faster convergence.\n    \n    This implementation follows the original paper:\n    \"Layer Normalization\" by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton\n    https://arxiv.org/abs/1607.06450\n    \"\"\"\n    \n    def __init__(self, features, eps = eps):\n        \"\"\"\n        Initialize the LayerNorm module.\n        \n        Args:\n            features (int): The number of features (channels) in the input tensor.\n            eps (float): A small value added to the variance for numerical stability.\n                         Default is 1e-6.\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        \n        # Create learnable parameters for scaling and shifting\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        \n        self.eps = eps\n    \n    def forward(self, x):\n        \"\"\"\n        Perform layer normalization on the input tensor.\n        \n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, ..., features).\n        \n        Returns:\n            torch.Tensor: Normalized tensor of the same shape as the input.\n        \"\"\"\n        # Compute the mean across the last dimension (features)\n        mean = x.mean(-1, keepdim=True)\n        \n        # Compute the standard deviation across the last dimension (features)\n        std = x.std(-1, keepdim=True)\n        \n        # Normalize the input tensor\n        x_normalized = (x - mean) / (std + self.eps)\n        \n        # Scale and shift the normalized tensor\n        out = self.a_2 * x_normalized + self.b_2\n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Initializes the PositionwiseFeedForward module.\n\n        Args:\n            d_model (int): The dimension of the input embeddings.\n            d_ffn (int): The dimension of the hidden layer in the feed-forward network.\n            dropout (float): The probability of dropout occurring.\n        \"\"\"\n        super().__init__()\n        \n        # Linear layer that maps from the input dimension (d_model) to the hidden dimension (d_ffn)\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        \n        # Linear layer that maps from the hidden dimension (d_ffn) back to the input dimension (d_model)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        \n        # Dropout layer with the specified dropout probability\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the PositionwiseFeedForward module.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model),\n                              representing the output from the attention mechanism.\n\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, seq_length, d_model),\n                          representing the expanded-and-contracted representation.\n        \"\"\"\n        \n        # Apply the first linear transformation (w_1) to the input tensor (x)\n        # This maps the input from the embedding dimension (d_model) to the hidden dimension (d_ffn)\n        hidden = self.w_1(x)\n        \n        # Apply the ReLU activation function to the hidden representation\n        activated = torch.relu(hidden)\n        \n        # Apply dropout to the activated hidden representation\n        dropped = self.dropout(activated)\n        \n        # Apply the second linear transformation (w_2) to the dropped representation\n        # This maps the hidden dimension (d_ffn) back to the embedding dimension (d_model)\n        output = self.w_2(dropped)\n        \n        # Return the final output tensor\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize an encoder layer.\n\n        Args:\n            d_model (int): The dimension of the input and output of the layer.\n            num_heads (int): The number of attention heads.\n            d_ffn (int): The dimension of the feedforward network.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadedAttention(num_heads, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.sublayer = nn.ModuleList([LayerNorm(d_model) for _ in range(2)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the encoder layer.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the encoder layer.\n        \"\"\"\n        sublayer_output = self.self_attn(x, x, x, mask)\n        x = x + self.dropout(sublayer_output)  # Apply dropout to the sublayer output before adding it to the input\n        x = self.sublayer[0](x)  # Apply normalization after the residual connection\n        sublayer_output = self.feed_forward(x)\n        x = x + self.dropout(sublayer_output)  # Again, apply dropout before the residual connection\n        x = self.sublayer[1](x)  # Apply normalization after the residual connection\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StackedEncoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize a stacked encoder.\n\n        Args:\n            num_layers (int): The number of encoder layers.\n            d_model (int): The dimension of the input and output of each layer.\n            num_heads (int): The number of attention heads in each layer.\n            d_ffn (int): The dimension of the feedforward network in each layer.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ffn, dropout) for _ in range(num_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the stacked encoder.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the stacked encoder.\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)  # Apply normalization to the output of the last layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_encoder = StackedEncoder(num_layers, d_model, \n                                 h, d_ffn, dropout).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder information\nprint('Below is the complete encoder: \\n')\nprint(stacked_encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_encoded_batches = []  # List to store encoded batches\ntotal_y_batches = []        # List to store corresponding y batches\ni = 0\n\nfor x_batch, y_batch in data_loader:\n    print(f'Size of batch: {x_batch.shape}')\n    i += 1\n    print(i)\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n    \n    # Move the batch to the device\n    x_batch = x_batch.to(device)\n    y_batch = y_batch.to(device)\n    \n    # Create a mask of ones for each sequence in the batch\n    #mask = torch.ones(x_batch.size(0), seq_len, seq_len, device=device)\n    \n    # Encode the batch using the stacked_encoder\n    encoded_batch = stacked_encoder(x_batch, mask = None)\n    \n    # Append the encoded batch to the list\n    total_encoded_batches.append(encoded_batch)#.detach().cpu())\n    total_y_batches.append(y_batch)#.detach().cpu())\n    \n    print(f'Current Size of Reviews: {len(total_encoded_batches)} tensors')\n    print(f'Current Size of Ratings: {len(total_y_batches)} tensors')\n\n# Concatenate all the encoded batches into a single tensor\ntotal_encoded_batch = torch.cat(total_encoded_batches, dim = 0)\n\n# Concatenate all the corresponding y batches into a single tensor\ntotal_y_batch = torch.cat(total_y_batches, dim = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the tensors\nprint(total_encoded_batch.shape)\nprint(total_y_batch.shape)\nprint('\\n')\n\n# size in MB\nprint(f'total_encoded_batch in bytes: { total_encoded_batch.nelement() * total_encoded_batch.element_size() }')\nprint(f'total_y_batch in bytes: { total_y_batch.nelement() * total_y_batch.element_size() }')\nprint('\\n')\n\n# position of the tensors\nprint(f\"The Encoded batch is on: {total_encoded_batch.device}\")\nprint(f\"The Y batch is on: {total_y_batch.device}\")\nprint('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attention info\n\nprint(f\"Query tensor: {query}\")\nprint(f\"Key tensor: {key}\")\nprint(f\"Value tensor: {value}\")\nprint('\\n')\n\nprint(f\"Attention scores: {attn}\")\nprint('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training approaches like masked-language modeling next sentence prediction, permutative language modeling, contrastive learning\n\n1. Visualization: You can visualize the input and output tensors to see if there are any noticeable patterns or differences. Techniques like heatmaps, scatter plots, or dimensionality reduction methods (e.g., t-SNE, PCA) can help you visualize high-dimensional data in a more interpretable way.\n\n2. Comparing input and output: You can directly compare the input and output tensors element-wise to see how the values have changed. This can give you an idea of the magnitude and direction of the transformations applied by the encoder.\n\n3. Analyzing attention weights: If your transformer encoder uses attention mechanisms, you can examine the attention weights to understand which parts of the input the model is focusing on. Higher attention weights indicate that the model is paying more attention to those specific positions or features.\n\n4. Probing the learned representations: You can train a separate model (e.g., a classifier or regressor) on top of the encoded representations to see if they capture meaningful information for a downstream task. If the model performs well, it suggests that the encoder has learned useful representations.\n\n5. Ablation studies: You can systematically remove or modify certain components of the transformer encoder (e.g., attention layers, normalization) and observe how the output changes. This can help you understand the role and impact of each component on the encoding process.\n\n6. Analyzing the distribution of values: You can compute statistics like mean, variance, and range of the input and output tensors to see how the distribution of values has changed. This can provide insights into the overall effect of the encoder on the data.\n\n7. Gradient-based methods: If you have access to the gradients of the encoder, you can use techniques like saliency maps or gradient-based attribution methods to identify which input features have the most influence on the output.\n\nKeep in mind that interpreting the behavior of deep learning models can be challenging, especially for complex architectures like transformers. It often requires a combination of different analysis techniques and domain knowledge to gain meaningful insights.\n\nRemember to normalize or scale the input and output tensors appropriately before visualization or comparison, as the raw floating-point values may have different scales and ranges.","metadata":{}}]}