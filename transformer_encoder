{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n\nelse:\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nreviews_file = ''\nw2v_file = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('.csv'): \n            reviews_file = file_name\n        elif file_name.endswith('.bin'):\n            w2v_file = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Amazon reviews file: {reviews_file}')\nprint(f'Google news word to vec file: {w2v_file}')\n            \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = False\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('VERBOSE:', text)\n    return\n\nshowCellCompletion = False\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = False\naccelerator = False\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T16:13:44.149533Z","iopub.execute_input":"2024-04-02T16:13:44.150170Z","iopub.status.idle":"2024-04-02T16:13:57.073064Z","shell.execute_reply.started":"2024-04-02T16:13:44.150135Z","shell.execute_reply":"2024-04-02T16:13:57.072069Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CUDA is available!\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100  # Maximum sequence length\ninput_size = d_model  # based on the output size of your feed-forward network\n\nnum_layers = 4 # Number of encoder layers\nh       = 10   # number of attention head\nd_ffn   = 1024 # dimension of the feedforward layer\n\ndropout = 0.0#0.1  # You can adjust the dropout if needed\neps     = 1e-6 # epsilon value to prevent the standard deviation from becoming zero\nepochs  = 10\nlearning_rate = 0.01\n\n\"\"\"\nTo ensure compatibility, it's important to choose the \nnumber of attention heads (h) such that d_model is \nevenly divisible by h in the multi-head attention \nmodule's self.d_k. This allows for a clean distribution \nof the model dimensionality across the attention heads.\n\"\"\"\n\n# d_model / attn.h = 300 / 10 = 30","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.075194Z","iopub.execute_input":"2024-04-02T16:13:57.076086Z","iopub.status.idle":"2024-04-02T16:13:57.084684Z","shell.execute_reply.started":"2024-04-02T16:13:57.076057Z","shell.execute_reply":"2024-04-02T16:13:57.083793Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"\"\\nTo ensure compatibility, it's important to choose the \\nnumber of attention heads (h) such that d_model is \\nevenly divisible by h in the multi-head attention \\nmodule's self.d_k. This allows for a clean distribution \\nof the model dimensionality across the attention heads.\\n\""},"metadata":{}}]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model: int, dropout: float, seq_len: int):\n        \"\"\"\n        Initialize the PositionalEncoding module.\n\n        Args:\n            d_model (int): The dimensionality of the model (embedding size).\n            dropout (float): The dropout rate to be applied.\n            seq_len (int): The maximum sequence length.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Create a tensor of shape (seq_len, 1) representing the positions\n        position = torch.arange(seq_len).unsqueeze(1)\n        \n        # Compute the denominator term for the sinusoidal positional encoding\n        # The denominator is 10000^(2i/d_model), where i is the position index\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        \n        # Create a tensor of shape (seq_len, 1, d_model) to store the positional encodings\n        pe = torch.zeros(seq_len, 1, d_model)\n        \n        # Compute the sinusoidal positional encodings for even dimensions\n        # pe[:, 0, 0::2] selects the even dimensions (0, 2, 4, ...) of the positional encoding tensor\n        # position * div_term computes the angle for each position and even dimension\n        # torch.sin applies the sine function element-wise to compute the positional encoding values\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        \n        # Compute the sinusoidal positional encodings for odd dimensions\n        # pe[:, 0, 1::2] selects the odd dimensions (1, 3, 5, ...) of the positional encoding tensor\n        # torch.cos applies the cosine function element-wise to compute the positional encoding values\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        \n        # Register the positional encoding tensor as a buffer in the module\n        # This ensures that the positional encoding tensor is saved and loaded with the module\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Perform the forward pass of the PositionalEncoding module.\n\n        Args:\n            x (Tensor): The input tensor of shape (seq_len, batch_size, embedding_dim).\n\n        Returns:\n            Tensor: The input tensor with positional encodings added.\n        \"\"\"\n        # Add positional encodings to the input tensor\n        # x.size(0) returns the sequence length dimension of the input tensor\n        # self.pe[:x.size(0)] selects the positional encodings corresponding to the sequence length\n        x = x + self.pe[:x.size(0)]\n        \n        # Apply dropout to the tensor with positional encodings\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.086138Z","iopub.execute_input":"2024-04-02T16:13:57.086502Z","iopub.status.idle":"2024-04-02T16:13:57.097774Z","shell.execute_reply.started":"2024-04-02T16:13:57.086469Z","shell.execute_reply":"2024-04-02T16:13:57.096997Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=dropout):\n        super().__init__()\n        \n        # Number of attention heads\n        self.h = h\n        \n        # Dimensionality of each attention head\n        self.d_k = d_model // h\n        \n        # Linear layers for query, key, value, and output projections\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p=dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n        # Compute the dimensionality of each attention head\n        d_k = query.size(-1)\n        \n        # Compute the attention scores using the dot product between query and key\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        # Apply the mask to the attention scores (if provided)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Apply softmax to obtain the attention probabilities\n        p_attn = torch.softmax(scores, dim=-1)\n        \n        # Apply dropout to the attention probabilities (if specified)\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n        \n        # Compute the weighted sum of values using the attention probabilities\n        # Return the attended values and attention probabilities\n        return torch.matmul(p_attn, value), p_attn\n\n    def forward(self, query, key, value, mask=None):\n        # Get the number of batches\n        nbatches = query.size(0)\n        \n        # Project and reshape the query, key, and value for multi-head attention\n        # The projections are done using the linear layers defined in __init__\n        query, key, value = [\n            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n        \n        # Perform scaled dot-product attention on the projected query, key, and value\n        x, attn = self.scaled_dot_product_attention(query, key, value, mask=mask, dropout=self.dropout)\n        \n        # Reshape the attended output and concatenate the attention heads\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        \n        # Apply a final linear projection to the concatenated output\n        return self.linears[-1](x)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.099778Z","iopub.execute_input":"2024-04-02T16:13:57.100062Z","iopub.status.idle":"2024-04-02T16:13:57.112626Z","shell.execute_reply.started":"2024-04-02T16:13:57.100038Z","shell.execute_reply":"2024-04-02T16:13:57.111859Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def add_layer(x, y):\n    \"\"\"Adds two tensors together.\n\n    Args:\n    x: A torch.Tensor of shape (batch_size, seq_len, hidden_size).\n    y: A torch.Tensor of the same shape as x.\n\n    Returns:\n    A torch.Tensor of the same shape as x and y, containing the sum of the two tensors.\n    \"\"\"\n\n    return torch.add(x, y)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.113548Z","iopub.execute_input":"2024-04-02T16:13:57.113794Z","iopub.status.idle":"2024-04-02T16:13:57.124375Z","shell.execute_reply.started":"2024-04-02T16:13:57.113762Z","shell.execute_reply":"2024-04-02T16:13:57.123517Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\"\n    Construct a layernorm module (See citation for details).\n    \n    Layer normalization is a technique to normalize the activations of a layer.\n    It helps stabilize the training process and can lead to faster convergence.\n    \n    This implementation follows the original paper:\n    \"Layer Normalization\" by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton\n    https://arxiv.org/abs/1607.06450\n    \"\"\"\n    \n    def __init__(self, features, eps=1e-6):\n        \"\"\"\n        Initialize the LayerNorm module.\n        \n        Args:\n            features (int): The number of features (channels) in the input tensor.\n            eps (float): A small value added to the variance for numerical stability.\n                         Default is 1e-6.\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        \n        # Create learnable parameters for scaling and shifting\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        \n        self.eps = eps\n    \n    def forward(self, x):\n        \"\"\"\n        Perform layer normalization on the input tensor.\n        \n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, ..., features).\n        \n        Returns:\n            torch.Tensor: Normalized tensor of the same shape as the input.\n        \"\"\"\n        # Compute the mean across the last dimension (features)\n        mean = x.mean(-1, keepdim=True)\n        \n        # Compute the standard deviation across the last dimension (features)\n        std = x.std(-1, keepdim=True)\n        \n        # Normalize the input tensor\n        x_normalized = (x - mean) / (std + self.eps)\n        \n        # Scale and shift the normalized tensor\n        out = self.a_2 * x_normalized + self.b_2\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.125685Z","iopub.execute_input":"2024-04-02T16:13:57.125944Z","iopub.status.idle":"2024-04-02T16:13:57.139116Z","shell.execute_reply.started":"2024-04-02T16:13:57.125922Z","shell.execute_reply":"2024-04-02T16:13:57.138118Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Initializes the PositionwiseFeedForward module.\n\n        Args:\n            d_model (int): The dimension of the input embeddings.\n            d_ffn (int): The dimension of the hidden layer in the feed-forward network.\n            dropout (float): The probability of dropout occurring.\n        \"\"\"\n        super().__init__()\n        \n        # Linear layer that maps from the input dimension (d_model) to the hidden dimension (d_ffn)\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        \n        # Linear layer that maps from the hidden dimension (d_ffn) back to the input dimension (d_model)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        \n        # Dropout layer with the specified dropout probability\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the PositionwiseFeedForward module.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model),\n                              representing the output from the attention mechanism.\n\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, seq_length, d_model),\n                          representing the expanded-and-contracted representation.\n        \"\"\"\n        \n        # Apply the first linear transformation (w_1) to the input tensor (x)\n        # This maps the input from the embedding dimension (d_model) to the hidden dimension (d_ffn)\n        hidden = self.w_1(x)\n        \n        # Apply the ReLU activation function to the hidden representation\n        activated = torch.relu(hidden)\n        \n        # Apply dropout to the activated hidden representation\n        dropped = self.dropout(activated)\n        \n        # Apply the second linear transformation (w_2) to the dropped representation\n        # This maps the hidden dimension (d_ffn) back to the embedding dimension (d_model)\n        output = self.w_2(dropped)\n        \n        # Return the final output tensor\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.140327Z","iopub.execute_input":"2024-04-02T16:13:57.140634Z","iopub.status.idle":"2024-04-02T16:13:57.151249Z","shell.execute_reply.started":"2024-04-02T16:13:57.140603Z","shell.execute_reply":"2024-04-02T16:13:57.150454Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"###################################### TEST ##########################################\n###################################### TEST ##########################################\n###################################### TEST ##########################################\n\n## Dropout - During training, randomly zeroes some of the elements of the input\n## tensor with probability p using samples from a Bernoulli distribution. Each\n## channel will be zeroed out independently on every forward call.\n\n###################################### embedding #####################################\n###################################### embedding #####################################\n###################################### embedding #####################################\n\nword_embeddings = torch.randn(32, 500, 300).to(device)\n\nprint (f\"Word Embedding Shape: {word_embeddings.size}\")\nprint (f\"Word Embedding Shape: {word_embeddings.shape}\")\nprint(f\"The Word Embeddings are on: {word_embeddings.device}\")\n      \n####################################### p.e. #########################################\n####################################### p.e. #########################################\n####################################### p.e. #########################################\n\n# Instantiate the PositionalEncoding module\npositional_encoder = PositionalEncoding(d_model, dropout, seq_len).to(device)\n\n# Apply the positional encoding to your word embeddings\nencoded_embeddings = positional_encoder(word_embeddings)\n      \n# Print the dimensions of the encoded embeddings\nprint(f\"Encoded Embeddings Shape: {encoded_embeddings.shape}\")\nprint(f\"The Encoded Embeddings are on: {encoded_embeddings.device}\")\n    \n####################################### attn ##########################################\n####################################### attn ##########################################\n####################################### attn ##########################################\n      \n# Create an instance of the MultiHeadedAttention class\nmulti_head_attention = MultiHeadedAttention(h, d_model, dropout).to(device)\n      \n# Define your query, key, and value tensors (they can be the same for self-attention)\nquery = encoded_embeddings.to(device)\nkey = encoded_embeddings.to(device)\nvalue = encoded_embeddings.to(device)\n\nprint(f\"The Query Tensor is on: {query.device}\")\nprint(f\"The Key Tensor is on: {key.device}\")\nprint(f\"The Value Tensor is on: {value.device}\")\n\n# Optional: Define an attention mask or use None if not needed\nattn_mask = None\n\n# Apply the MultiHeadedAttention\\n\nattention_output = multi_head_attention(query, key, value)\n\n# Print the dimensions of the attention output\\n\nprint(f\"Attention Output Shape: {attention_output.shape}\")\nprint(f\"The Attention Output is on: {attention_output.device}\")\n\n######################################## add ##########################################\n######################################## add ##########################################\n######################################## add ##########################################\n\nresidual_connection = add_layer(attention_output, encoded_embeddings)\n\nprint(f\"Residual Connection Shape: {residual_connection.shape}\")\nprint(f\"The Residual Connection is on: {residual_connection.device}\")\n\n######################################## norm #########################################\n######################################## norm #########################################\n######################################## norm #########################################\n\nnorm = LayerNorm(d_model, eps).to(device)\nnormalized_values = norm(residual_connection)\n      \nprint(f\"Normalized Values Shape: {normalized_values.shape}\")\nprint(f\"The Normalized Values are on: {normalized_values.device}\")\n\n######################################## fc ##########################################\n######################################## fc ##########################################\n######################################## fc ##########################################\n      \nfeedforward = PositionwiseFeedForward(d_model, d_ffn, dropout).to(device)\nff_output = feedforward(normalized_values)\n\nprint(f\"FF Output Shape: {ff_output.shape}\")\nprint(f\"The Output is on: {ff_output.device}\")\nprint(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:57.152563Z","iopub.execute_input":"2024-04-02T16:13:57.152865Z","iopub.status.idle":"2024-04-02T16:13:58.467636Z","shell.execute_reply.started":"2024-04-02T16:13:57.152840Z","shell.execute_reply":"2024-04-02T16:13:58.466288Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Word Embedding Shape: <built-in method size of Tensor object at 0x7b360926b1f0>\nWord Embedding Shape: torch.Size([32, 500, 300])\nThe Word Embeddings are on: cuda:0\nEncoded Embeddings Shape: torch.Size([32, 500, 300])\nThe Encoded Embeddings are on: cuda:0\nThe Query Tensor is on: cuda:0\nThe Key Tensor is on: cuda:0\nThe Value Tensor is on: cuda:0\nAttention Output Shape: torch.Size([32, 500, 300])\nThe Attention Output is on: cuda:0\nResidual Connection Shape: torch.Size([32, 500, 300])\nThe Residual Connection is on: cuda:0\nNormalized Values Shape: torch.Size([32, 500, 300])\nThe Normalized Values are on: cuda:0\nFF Output Shape: torch.Size([32, 500, 300])\nThe Output is on: cuda:0\nTotal allocated memory: 1113831936 bytes\nTue Apr  2 16:13:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P0              31W / 250W |   1496MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize an encoder layer.\n\n        Args:\n            d_model (int): The dimension of the input and output of the layer.\n            num_heads (int): The number of attention heads.\n            d_ffn (int): The dimension of the feedforward network.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadedAttention(num_heads, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.sublayer = nn.ModuleList([LayerNorm(d_model) for _ in range(2)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the encoder layer.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the encoder layer.\n        \"\"\"\n        sublayer_output = self.self_attn(x, x, x, mask)\n        x = x + self.dropout(sublayer_output)  # Apply dropout to the sublayer output before adding it to the input\n        x = self.sublayer[0](x)  # Apply normalization after the residual connection\n        sublayer_output = self.feed_forward(x)\n        x = x + self.dropout(sublayer_output)  # Again, apply dropout before the residual connection\n        x = self.sublayer[1](x)  # Apply normalization after the residual connection\n        return x\n\nclass StackedEncoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize a stacked encoder.\n\n        Args:\n            num_layers (int): The number of encoder layers.\n            d_model (int): The dimension of the input and output of each layer.\n            num_heads (int): The number of attention heads in each layer.\n            d_ffn (int): The dimension of the feedforward network in each layer.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ffn, dropout) for _ in range(num_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the stacked encoder.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the stacked encoder.\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)  # Apply normalization to the output of the last layer","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:58.469391Z","iopub.execute_input":"2024-04-02T16:13:58.469704Z","iopub.status.idle":"2024-04-02T16:13:58.482418Z","shell.execute_reply.started":"2024-04-02T16:13:58.469676Z","shell.execute_reply":"2024-04-02T16:13:58.481314Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"stacked_encoder = StackedEncoder(num_layers, d_model, \n                                 h, d_ffn, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:58.485476Z","iopub.execute_input":"2024-04-02T16:13:58.485829Z","iopub.status.idle":"2024-04-02T16:13:58.554103Z","shell.execute_reply.started":"2024-04-02T16:13:58.485804Z","shell.execute_reply":"2024-04-02T16:13:58.553132Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"word_embeddings = torch.randn(100, 500, 300).to(device)\nratings = torch.randn(100, 1, 1).to(device)\n\ndataset = TensorDataset(word_embeddings, ratings)\n\ndata_loader = DataLoader(dataset, batch_size = 32, shuffle = True)\n\nprint (f\"Word Embedding Shape: {word_embeddings.shape}\")\nprint (f\"Rating Shape: {ratings.shape}\")\nprint(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:58.555748Z","iopub.execute_input":"2024-04-02T16:13:58.556123Z","iopub.status.idle":"2024-04-02T16:13:59.791377Z","shell.execute_reply.started":"2024-04-02T16:13:58.556088Z","shell.execute_reply":"2024-04-02T16:13:59.790001Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Word Embedding Shape: torch.Size([100, 500, 300])\nRating Shape: torch.Size([100, 1, 1])\nTotal allocated memory: 1174664192 bytes\nTue Apr  2 16:13:59 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0              31W / 250W |   1502MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"total_encoded_batches = []  # List to store encoded batches\ntotal_y_batches = []        # List to store corresponding y batches\ni = 0\n\nfor x_batch, y_batch in data_loader:\n    print(f'Size of batch: {x_batch.shape}')\n    i += 1\n    print(i)\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n    \n    # Move the batch to the device\n    x_batch = x_batch.to(device)\n    y_batch = y_batch.to(device)\n    \n    # Create a mask of ones for each sequence in the batch\n    #mask = torch.ones(x_batch.size(0), seq_len, seq_len, device=device)\n    \n    # Encode the batch using the stacked_encoder\n    encoded_batch = stacked_encoder(x_batch, mask = None)\n    \n    # Append the encoded batch to the list\n    total_encoded_batches.append(encoded_batch.detach().cpu())\n    total_y_batches.append(y_batch.detach().cpu())\n    \n    print(f'Current Size of Reviews: {len(total_encoded_batches)} tensors')\n    print(f'Current Size of Ratings: {len(total_y_batches)} tensors')\n\n\n# Concatenate all the encoded batches into a single tensor\ntotal_encoded_batch = torch.cat(total_encoded_batches, dim = 0)\n\n# Concatenate all the corresponding y batches into a single tensor\ntotal_y_batch = torch.cat(total_y_batches, dim = 0)\n\nprint(f'Concatenated Reviews Size: {total_encoded_batch.shape}')\nprint(f'Concatenated Ratings Size: {total_y_batch.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:13:59.792873Z","iopub.execute_input":"2024-04-02T16:13:59.793204Z","iopub.status.idle":"2024-04-02T16:14:00.087554Z","shell.execute_reply.started":"2024-04-02T16:13:59.793174Z","shell.execute_reply":"2024-04-02T16:14:00.086556Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Size of batch: torch.Size([32, 500, 300])\n1\nTotal allocated memory: 1193864704 bytes\nCurrent Size of Reviews: 1 tensors\nCurrent Size of Ratings: 1 tensors\nSize of batch: torch.Size([32, 500, 300])\n2\nTotal allocated memory: 5709290496 bytes\nCurrent Size of Reviews: 2 tensors\nCurrent Size of Ratings: 2 tensors\nSize of batch: torch.Size([32, 500, 300])\n3\nTotal allocated memory: 5708554752 bytes\nCurrent Size of Reviews: 3 tensors\nCurrent Size of Ratings: 3 tensors\nSize of batch: torch.Size([4, 500, 300])\n4\nTotal allocated memory: 5692618752 bytes\nCurrent Size of Reviews: 4 tensors\nCurrent Size of Ratings: 4 tensors\nConcatenated Reviews Size: torch.Size([100, 500, 300])\nConcatenated Ratings Size: torch.Size([100, 1, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(total_encoded_batch.shape)  # Check the shape of the combined encoded tensor\nprint(total_y_batch.shape)\nprint('\\n')\n\n# size in MB\nprint(f'total_encoded_batch in bytes: { total_encoded_batch.nelement() * total_encoded_batch.element_size() }')\nprint(f'total_y_batch in bytes: { total_y_batch.nelement() * total_y_batch.element_size() }')\nprint('\\n')\n\nprint(f\"The Encoded batch is on: {total_encoded_batch.device}\")\nprint(f\"The Y batch is on: {total_y_batch.device}\")\nprint('\\n')\nprint(stacked_encoder)\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-02T16:14:00.088822Z","iopub.execute_input":"2024-04-02T16:14:00.089118Z","iopub.status.idle":"2024-04-02T16:14:00.096548Z","shell.execute_reply.started":"2024-04-02T16:14:00.089092Z","shell.execute_reply":"2024-04-02T16:14:00.095338Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"torch.Size([100, 500, 300])\ntorch.Size([100, 1, 1])\n\n\ntotal_encoded_batch in bytes: 60000000\ntotal_y_batch in bytes: 400\n\n\nThe Encoded batch is on: cpu\nThe Y batch is on: cpu\n\n\nStackedEncoder(\n  (layers): ModuleList(\n    (0-3): 4 x EncoderLayer(\n      (self_attn): MultiHeadedAttention(\n        (linears): ModuleList(\n          (0-3): 4 x Linear(in_features=300, out_features=300, bias=True)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (feed_forward): PositionwiseFeedForward(\n        (w_1): Linear(in_features=300, out_features=1024, bias=True)\n        (w_2): Linear(in_features=1024, out_features=300, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (sublayer): ModuleList(\n        (0-1): 2 x LayerNorm()\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (norm): LayerNorm()\n)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_reviews = total_encoded_reviews.requires_grad_(True)\nencoded_ratings = total_encoded_ratings.requires_grad_(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Visualization: You can visualize the input and output tensors to see if there are any noticeable patterns or differences. Techniques like heatmaps, scatter plots, or dimensionality reduction methods (e.g., t-SNE, PCA) can help you visualize high-dimensional data in a more interpretable way.\n\n2. Comparing input and output: You can directly compare the input and output tensors element-wise to see how the values have changed. This can give you an idea of the magnitude and direction of the transformations applied by the encoder.\n\n3. Analyzing attention weights: If your transformer encoder uses attention mechanisms, you can examine the attention weights to understand which parts of the input the model is focusing on. Higher attention weights indicate that the model is paying more attention to those specific positions or features.\n\n4. Probing the learned representations: You can train a separate model (e.g., a classifier or regressor) on top of the encoded representations to see if they capture meaningful information for a downstream task. If the model performs well, it suggests that the encoder has learned useful representations.\n\n5. Ablation studies: You can systematically remove or modify certain components of the transformer encoder (e.g., attention layers, normalization) and observe how the output changes. This can help you understand the role and impact of each component on the encoding process.\n\n6. Analyzing the distribution of values: You can compute statistics like mean, variance, and range of the input and output tensors to see how the distribution of values has changed. This can provide insights into the overall effect of the encoder on the data.\n\n7. Gradient-based methods: If you have access to the gradients of the encoder, you can use techniques like saliency maps or gradient-based attribution methods to identify which input features have the most influence on the output.\n\nKeep in mind that interpreting the behavior of deep learning models can be challenging, especially for complex architectures like transformers. It often requires a combination of different analysis techniques and domain knowledge to gain meaningful insights.\n\nRemember to normalize or scale the input and output tensors appropriately before visualization or comparison, as the raw floating-point values may have different scales and ranges.","metadata":{}}]}