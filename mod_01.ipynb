{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\" style=\"color:blue;font-size: 3em;\" >Sentiment Analysis of Amazon Reviews using Transformers</h1>\n\n## This program is broken down into 3 sections:\n\n**1. We will work on data cleaning and preprocessing.**\n\n**2. We will then work on creating the sentiment analyzer and train it on the data we have preprocessed.**\n\n**3. Finally, we will evauate our model and visualize the results.**\n\n***Heads Up: Some of the cells might not have an output after they finish running. This is completely normal. You can always hover over the play button you click to run them and it will let you know if the cell has been executed and how long the process took. If there ever is an error, the cell will print out the stack trace and the issue it encountered.***","metadata":{}},{"cell_type":"markdown","source":"## Let us begin by importing the necessary libraries\n\n***Heads Up: You might see a warning when importing the packages, but you may ignore this, as it won't cause any issues for our needs.***","metadata":{}},{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This magic command we are running below gives the notebook access to parts of the NLTK library. When ran correctly, it should unzip the wordnet file and copy over its contents.**\n\n**Please only run this command once when you start the notebook. If you run it again, it will prompt you to replace the existing files. After you run it once, you may comment it out. If you happen to run it again, and you see are prompted for a response, simply stop the cell and move on to the next section.** \n\n<span style=\"color:green\">\"prompt as a response\" should be \"prompted for a response\"?</span>","metadata":{}},{"cell_type":"code","source":"## Only run once\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 1: Data Cleaning and Preprocessing\n\n**One thing that you will here when often working with data is that preprocessing it is the most important part. If the data is not cleaned and prepared for the model, you will always get sub-par results.**\n\n**The image below serves to illustrate what we aim to achieve in this section. It may look confusing now, but things will be clearer as we go through the section.**\n\n**For this project, we will be using over 568,000 reviews collected from Amazon. The cell below the image contains the code that will give us access to the dataset. You can also find the dataset linked here.**\n\n<span style=\"color:green\">Note that a word embedding is a representation of a word in an n-dimensonal vector space, n â‰¥ 2</span>\n\n[Amazon Reviews](https://www.kaggle.com/datasets/arhamrumi/amazon-product-reviews)","metadata":{}},{"cell_type":"markdown","source":"![Example of Embeddings](https://www.researchgate.net/publication/340825443/figure/fig6/AS:882927785238529@1587517796128/Word-embeddings-map-words-in-a-corpus-of-text-to-vector-space-Linear-combinations-of.png)\n\n* Source: [Word embeddings map words in a corpus of text to vector space](https://figshare.com/articles/figure/Word_embeddings_map_words_in_a_corpus_of_text_to_vector_space_/12169047/1)","metadata":{}},{"cell_type":"markdown","source":"**Let's access the dataset and see how many reviews we actually have.**","metadata":{}},{"cell_type":"code","source":"# Load data from CSV\npath = \"/kaggle/input/amazon-product-reviews/Reviews.csv\"\ndata = pd.read_csv(path) # Use pandas to analyze data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we can use a pandas data object methods to sample the data and display the elements of it.**","metadata":{}},{"cell_type":"code","source":"# print number of rows in our ratings column\n#printv(f'Number of reviews: {len(data[\"Score\"])}')\n#printv(f'Column names -\\n {data.columns}\\n') \n#printv(f'First five rows -\\n{data.head()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data has 1O columns, including \"Score,\" indicating the review's sentiment, and \"Text,\" the product review the score is based on. In Machine Learing, Text would be called the FEATURE, the term for the column(s) used for inference.**\n\n\n**There is far too much data to print, but we can print the total number of reviews per rating and use the functions provided to us by matplotlib to visualize the distribution as a bar graph.**","metadata":{}},{"cell_type":"code","source":"# Get count of ratings \nrating_counts = data['Score'].value_counts()\n\n# Sort counts by index ascending\nrating_counts = rating_counts.sort_index()  \n\n# Print number of reviews per rating\n#for rating, count in rating_counts.items():\n#    print(f\"{count:,} reviews with a rating score of {rating}\", \"\\n\")\n\n# Get count of ratings\n#>> Seems to work even if we do not redefine rating_count\n#>> rating_counts = data['Score'].value_counts().sort_index() \n\n# Create bar plot\nax = rating_counts.plot(kind = 'bar')\n\nax.set_title(\"Ratings Distribution\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Occurrences\")\n\n# Fix x-axis tick labels\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0) \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The distribution looks heavily skewered to the positive side, with 5 star reviews making up more than half of our dataset. While that's great for Amazon and its customers, it could bias our model and starve it of adequate negative and neutral reviews to train on.**\n\n**To work around this, we can try to balance out our data. Given how we have the least number 2 rating reviews, let's take 25,000 reviews from each rating to give our model an even dataset, giving us a total of 125,000 reviews to work with. But fret not, because that is still a vast amount of data to work with.**\n\n**To do this, we can sort the data in ascending order and take 25,000 reviews from each rating class.**","metadata":{}},{"cell_type":"code","source":"# Specify the column for sorting and balancing\nsort_column = 'Score'  # This is one the rating column\n\n# Sort the data by the rating values\nsorted_data = data.sort_values(by = sort_column)\n\n# Create a balanced dataset with 25,000 samples from each class\n#balanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n=25000))\nbalanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n = 50))\n\n#>> Does this mean to reset the row numbers?? ##Columns Numbers\nbalanced_data.reset_index(drop = True, inplace = True)\n\nprintv(f\"The number of reviews equally distributed across all ratings is {len(balanced_data['Score'])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can use matplotlib to see that reviews are equally distributed across all rating categories in the balanced data.**","metadata":{}},{"cell_type":"code","source":"# Get count of ratings\nrating_counts = balanced_data['Score'].value_counts()\n\n# Create bar plot\nax = rating_counts.plot(kind='bar')\n\nax.set_title(\"Ratings Distribution After Balancing\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Samples\")\n\n# Fix x-axis ticks  \nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)\n\n# Print number of reviews per rating\nif DEV:\n    for rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That looks much better. We see that each rating class is sufficiently represented, and we can expect to get a more well-rounded analyzer. While this is a good way to even things out, our model can still be biased due to the reviews themselves. People leave comments when they have stronger opinions about a product. This is why balancing the types of reviews we give our model is necessary, so that the model can get a complete representation of the range of opinions**\n\n**We will also encounter things like typos, improper punctuation and spacing, along with other grammatical issues. While we will leave the dataset as is for now, it is important to try other approaches to identify these issues and filter them out.**\n\n**Now that we have our dataset ready, let's get to work on preprocessing it. Our reviews contain things like punctuations that we don't need to work with. To get rid of these, we use regular expressions to filter them out, and convert every character to lowercase to achieve uniformity.**\n\n**Something else we can do is a process called Lemmatization. This is the process of replacing variants of a word by the base word itself. An example of this would be to change the words knew, knowing, and known to their base word: know. This will help us achieve better uniformity by avoiding the misplacement of a word's variants.** <span style=\"color:blue\">Do you actually use the lemmatizer?","metadata":{}},{"cell_type":"code","source":"stop = stopwords.words('english') # Imported from nltk.corpus\nlem = WordNetLemmatizer()         # Imported from nltk.stem","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printd (f'{stop}\\n')\n#printd (f'{lem}\\n')\n\n# test lemmatizer\n#word = \"dogs\"\n#lemmatized_word = lem.lemmatize(word)\n#print(lemmatized_word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's create a function that takes our reviews and returns a lemmanized, lower-case, punctuation-free version of it.** **We can then store this new version in a new column called 'CleanedReview' for easy access.**\n\n***Heads up: This cell takes a little longer to execute.***","metadata":{}},{"cell_type":"code","source":"# Takes some time to run\n\ndef tokenizer(text):\n    \"\"\"\n    Tokenizes a text string and removes stop words.\n\n    Args:\n        text (str): The text string to tokenize.\n\n    Returns:\n        list: The tokenized text string.\n    \"\"\"\n    text = text.lower()  # Convert text to lowercase.\n    text = re.sub(\"<.*?>\", \"\", text)  # Remove HTML tags.\n    text = re.sub('[^\\w\\s\\']+', \"\", text)  # Remove punctuation and symbols.\n    # text = text.split('\\n')  # Split text on new lines.\n    text = [lem.lemmatize(word) for word in text.split()]#if word not in stop]  # Remove stop words. \n\n    return text\n\n# Apply the function to the Text column and store it in a new column\nbalanced_data['CleanedReview'] = balanced_data['Text'].apply(tokenizer)\n\n# show that cell has finished executing\n#printv(\"Cell has finished!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Another look at the data after the addition of the CleanedReview column.**","metadata":{}},{"cell_type":"code","source":"# Store the Rating column\nrating = balanced_data['Score']  \n\n# Store the CleanedReview column\ntokenized_review = balanced_data['CleanedReview']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print dimensions of dataset\n#printv(f'num rows, columns in balance data - {balanced_data.shape}\\n') \n\n# print number of rows\n#printd(f\"Data has {tokenized_review.shape[0]} rows\")\n\n# print number of columns\n#printd(f\"Data has {balanced_data.shape[1]} columns\\n\") \n\n# print names of columns\n#printv(f'Column names in balanced data - {balanced_data.columns}\\n')\n\n# Print first row of 'Text' column\n#printv(f'Sample review: ')\n#printv(f\"{balanced_data['Text'].iloc[0]}\\n\")  \n\n# Print first row of 'CleanedReview' column\n#printv('Sample review tokenized: ')\n#printv(f'{tokenized_review.iloc[0]}\\n')\n\n# print the total length, and the first and last 5 rows\n#printd(f\"Number of ratings: {len(rating)}\\n\")\n#printd(\"The rating values of the first 5 reviews are: \")\n#printd(f'{rating.head()}\\n')\n#printd(\"The rating values of the last 5 reviews are: \")\n#printd(f'rating.tail()\\n')\n\n# print the total length, and the first and last 5 rows\n#printv(f\"Number of tokenized reviews: {len(tokenized_review)}\\n\")\n#printv(\"The tokenized versions of the first 5 reviews are: \")\n#printv(f\"{tokenized_review.head()}\\n\")\n#printv(\"The tokenized versions of the last 5 reviews are: \")\n#printv(f\"{tokenized_review.tail()}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see that our data now has 11 columns, with the addition of CleanReview, and that the CleanedReview column is the product review mapped to a sequence of standardized words.**","metadata":{}},{"cell_type":"markdown","source":"**There are just a few more steps remaining before we can load the data. We have to tokenize the words from the reviews because neural networks, much like computers, don't really understand words. Instead, our tokenizer is going to associate every word with a number that can be used to look up the word's embeddings.**\n\n**Think of embeddings as vector representations of the word in space.**\n\n**There are many approaches we can take to vectorizing our reviews, so much so in fact that word embeddings are a big part of Natural Language Processing. Without getting too bogged down in details, some approaches use statistical approaches like bag-of-words to figure out context, and others can use neural network-based models that are trained to find features themselves.** \n\n**The quickest way would be to take an already trained tokenizer made for established transformer-based models. For example, the BertTokenizer is a tokenizer that was trained and used by BERT. BERT is a massive Transformer-based model created by Google in 2018. It was a significant achievement because it was able to perform up to 11 NLP tasks, including summarization, text-generation(think ChatGPT), and even sentiment analysis.**\n\n**Word2vec-google-news-300 is a pre-trained deep-learning word embedding model that was trained on a massive dataset of 100 billion words from the Google News corpus. The model contains 300-dimensional vectors for 3 million words and phrases. That is what we will be using for this project**\n\n**Word2Vec extracts both semantic and contextual representations of words. The semantic representation of a word captures the meaning of the word itself, while the contextual representation captures the meaning of the word in the context of the surrounding words.**\n\n**To explain what's happening in the Word2Vec embedding process:**\n\n1. **The tokenizer splits the raw text into tokens/words.**\n2. **Word2Vec has IDs that serve as indices into a lookup table that stores the vector representations, meaning that each ID corresponds to a unique vector representation in the table.**\n3. **For each token, we lookup the corresponding ID in the Word2Vec vocabulary using .key_to_index.**\n4. **This maps the tokens to existing Word2Vec IDs.**\n5. **We pass these IDs into the embedding layer.**\n6. **The embedding layer has a 300-dim vector for each ID.**\n7. **So each token gets replaced by its pre-trained 300-dim Word2Vec vector.**\n\n**To summarize:**\n\n* **Tokenizer splits text into words.**\n* **Word2Vec vocab provides ID for each word.**\n* **Embedding layer maps IDs to 300-dim vectors.**\n* **So tokens are replaced by 300-dim pretrained embeddings.**","metadata":{}},{"cell_type":"markdown","source":"**Let's start by downloading our word2vec model and set a variable we will use to limit the number of out tokens per review to 100 tokens. We will later zero-pad these reviews so we can have a uniform number of tokens per review.**\n\n\n***Heads up: This cell will take some time to execute.***","metadata":{}},{"cell_type":"code","source":"# Load Word2Vec model\nw2v = api.load('word2vec-google-news-300')\n\n# Define the maximum sequence length (adjust as needed)\nmax_sequence_length = 100\n\nprint(\"Cell finished executing!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Word2Vec model\n#filename = '/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin'\n#w2v = KeyedVectors.load_word2vec_format(filename, binary=True)\n#max_sequence_length = 100\n#>> word_vec deprecated, use get_vector instead\n#>>print(f\"Embedding vector length: {len(w2v.word_vec('king'))}\")\n#print(f\"Embedding vector length: {len(w2v.get_vector('king'))}\") #<<\n#print(\"Word2Vec api loaded!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that we have downloaded our model, let's use our CUDA-enabled GPU to take our token and look for its corresponding vector from our model. Once we find it, we swap it out with the new vector and if we don't, we pad that token with zeros.**","metadata":{}},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n\nelse:\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After ensuring that we have our GPUs available, we can go onto the next step. The code continues even if a CUDA is not available.**\n\n**In the cell below, we will do the following:**\n\n* **Create a new tensor and populate it with zeros**\n* **Compare the length of our review to the maximum length we allow (100) and take the minimum of the two**\n* **We lookup the token against what's in the w2v model. If we find it, we replace it with the embedding.**\n* **If we can't find it, we leave it as is (zero vectors).**","metadata":{}},{"cell_type":"markdown","source":"***Heads Up: Cell below takes about 5 minutes to run***","metadata":{}},{"cell_type":"code","source":"# Assume you have a list of tokenized review called tokenized_review\n# Each element in tokenized_review is a list of tokens for a single review\n\n# Initialize an empty tensor for padded reviews on the GPU\npadded_reviews = torch.zeros((len(tokenized_review), max_sequence_length, 300))#, device = device\n\n# Pad shorter reviews and convert tokens to Word2Vec embeddings\nfor i, review_tokens in enumerate(tokenized_review):\n    review_length = min(len(review_tokens), max_sequence_length)\n    for j in range(review_length):\n        word = review_tokens[j]\n        if word in w2v:\n            # Use Word2Vec vector if available and move to GPU\n            padded_reviews[i, j, :] = torch.tensor(w2v[word])#, device = device\n        # Otherwise, it remains as zeros (padding)\n\n# Apply max pooling to aggregate embeddings along the sequence dimension\n# review_embeddings = torch.max(padded_reviews, dim=1)[0]\n\n# Now,review_embeddings contains the aggregated Word2Vec \n# embeddings for each review on the GPU\n\nprint(\"Cell has finished!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Once we have our embeddings ready, we can prepare them as inputs for the model. The next step is to convert our inputs, now converted to embedding-rich vectors, to tensors. Tensors are a kind of data structure that store information in multidimensional space. They are a generalization of vectors and matrices, and can have any number of dimensions.**\n\n**Converting vectors to tensors is computationally expensive, which is one of the reasons why GPUs are the main processors used in AI. GPUs are specialized for performing parallel computations, which makes them ideal for tasks such as tensor conversion.**\n\n**But first, a quick introduction to PyTorch. PyTorch is a popular deep learning framework that provides a lot of resources for building deep learning architectures, including popular pretrained models. It has become prevalent in academia, research, and industry, being utilized by Tesla, Uber, Hugging Face, and many more.**\n\n**PyTorch will help us convert our vectors to tensors. It also support GPU acceleration. CUDA is a firmware developed by NVIDIA that allows GPUs to be used for general-purpose computing, such as tensor conversion.**\n\n**Once the embeddings have been converted to tensors, we can print out the dimensions of our new dataset. This will tell us how many data points we have and how many features each data point has. We can then load the dataset to our data loaders, which will prepare it for training the neural network.**","metadata":{}},{"cell_type":"markdown","source":"![Tensors](https://hkilter.com/images/7/7a/Tensors.png)\n\n* Source: [What is Tensor](https://hkilter.com/index.php?title=What_is_Tensor%3F)","metadata":{}},{"cell_type":"markdown","source":"**When we utilize tokenization and convert them to their embeddings, we are only doing this for one instance of them to avoid redundancy. So for example, if the word \"the\" appears 2000 times, it would only count once for unique tokens.**\n\n*Thus, a total of 39,753 unique tokens were converted to vectors and 9,373,966 was the total number of tokens including duplicates were converted into tensors and stored in our GPU.*\n\n**Since this Word2Vec model is trained on a huge corpus, it likely has vectors for the vast majority of tokens we extracted. The only ones missing would be very rare or irregular words.**\n\n***Note: It is imporant to remember that we are not just using this vocabulary as a look-up dictionary. That's just a small part of the step. What we are doing is far more advanced. The embeddings are rich contextual vectors that will highlight the relationships between words in different ways. For example, lexicographically speaking, the word 'car' is more similar to 'cat', but when viewed semantically, and/or in terms of its context, it is far more similar to 'dog'. Same goes for the word 'bank', which can be used to refer to a financial institution or the side of a river. These kinds of relationships are the foundation for our model and will serve as the basis for how it perceives them.***","metadata":{}},{"cell_type":"code","source":"print(torch.cuda.memory_summary())\nprint(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#>> padded_reviews are 100 x 300 tensors, zero padded if necessary\n#>> to get the standard lenth\ntext_embeddings_tensors = padded_reviews.to(device)\n\n# Rating labels\nrating_labels_tensors = torch.tensor(rating.values).to(device)\n\n# Dataset\ndataset = TensorDataset(text_embeddings_tensors, rating_labels_tensors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\nprint('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\nprint(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# location of our tensors\n#printv(\"Location of our tensors: \")\n#printv(f\"The review tensor is on: {text_embeddings_tensors.device}\")\n#printv(f\"The label tensor is on: {rating_labels_tensors.device}\\n\")\n\n# information of our embedding\n#printv(\"Information of our embedding: \")\n#printv(f\"Embeddings shape: {text_embeddings_tensors.shape}\")\n#printd(f\"Our embedding: {text_embeddings_tensors[:]}\\n\")\n\n# information about our ratings\n#printv(\"Information about our ratings: \")\n#printv(f\"Ratings shape: {rating_labels_tensors.shape}\") \n#printd(f\"Our rating: {rating_labels_tensors[:]}\")\n#printv(f\"The mean value of our ratings is: {rating_labels_tensors.float().mean()}\\n\")\n\n# information of our dataset object(review + rating)\n#printv(f\"Dataset length: {len(dataset)}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our output confirms that we have successfully converted our data to tensors and is now loaded on our GPU. The output shows that CUDA is available and the GPU name is Tesla P100-PCIE-16GB. The text_tensor and rating_tensor are both on the GPU (device='cuda:0'). The shape of the text_tensor is torch.Size([125000, 100, 300]), which means that it has 125000 rows, 100 columns, with 300 embeddings per column. The shape of the rating_tensor is torch.Size([125000]), which means that it has 125000 rows (ratings). The mean of the rating_tensor is 3 meaning that the we have perfectly divided all our ratings.**","metadata":{}},{"cell_type":"markdown","source":"![numeric_tensors](https://miro.medium.com/v2/resize:fit:1400/1*Shsgt3h9yxlQwjkfIptfYQ.png)\n\n* Source: [From Vectors to Tensors: Exploring the Mathematics of Tensor Algebra](https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff)","metadata":{}},{"cell_type":"markdown","source":"**We can now split our dataset into training and validation sets. When working on machine learning or deep learning models, we want to train the model on a subset of the data, and then test the accuracy of the model's predictions on the remaining data. This is called data splitting. We typically split the data into an 80/20 split, where we train the model on 80% of the data and test it on the remaining 20%.**","metadata":{}},{"cell_type":"code","source":"# Lengths \n#train_len = int(0.8 * len(dataset))\n#val_len = len(dataset) - train_len\n\n# Random split\n#train_data, val_data = random_split(dataset, [train_len, val_len])\n\n#printv(f\"Training data length = {len(train_data)}\") \n#printv(f\"Validation data length = {len(val_data)}\")\n\n#train_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n## >> Why no shuffle for validation\n# DataLoader for validation data\n#val_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation\n\n# Iterate over the data loader\n#i = 0\n#for x_batch, y_batch in train_loader:\n    # The reviews will be in the x_batch tensor\n    # The ratings will be in the y_batch tensor\n    #i += 1\n    # print(f\"batch number: {i}, loaded\")\n    \n#printv(f'x_batch shape: {x_batch.shape}')\n    \n#printv(f\"Total number of training batches: {i}\")\n    \n# Check the shape of x_batch (number of reviews)\n#num_reviews_in_batch = x_batch.shape[0]\n\n# Check the shape of y_batch (number of ratings)\n#num_ratings_in_batch = y_batch.shape[0]\n\n#printd(f\"Number of reviews in x_batch: {num_reviews_in_batch}\")\n#printd(f\"Number of ratings in y_batch: {num_ratings_in_batch}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And there we have it! It looks like we have successfully split our data 80/20, and are now going to load the tensors in batches to our training and validation loaders. We will use these to load our data into the model once we have finished building it. We have also stored the data in batches of 32. This will make it easier to process the data instead of dealing with the whole thing at once. To confirm that it is all loaded, we can see that the number of batches, 3,125, multiplied by the batch size, 32, will indeed return 125,000**","metadata":{}},{"cell_type":"markdown","source":"**We have successfully completed the preprocessing, splitting, and loading of our data. This section may have felt overwhelming, but it is important to remember that preparing the data properly is just as important as building the model. In fact, it is often said that 80% of the work in machine learning is in data preparation. This is because the quality of the data will have a significant impact on the performance of the model.**","metadata":{}},{"cell_type":"markdown","source":"**And now for the really exciting part: the Transformer, the T in Chat-GPT. Arguably one of the most impactful architectures in recent history, the Transformer has been the state of the art in natural language processing (NLP) and other sequence-to-sequence tasks. In this section, we will break down and build on the individual components of this architecture, and use it to create our sentiment analyzer.**","metadata":{}},{"cell_type":"markdown","source":"# Section 2: The Sentiment Analyzer","metadata":{}},{"cell_type":"markdown","source":"![Detailed Architecture Diagram](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)\n\n* Source: [Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)","metadata":{}},{"cell_type":"markdown","source":"**Above is a diagram of what the Transformer architecture looks like. It might look complicated, but each cell will work on one component of the layer so we can isolate and understand the various sections of our model.**\n\n***Heads Up: Since we are building a sentiment analyzer, we will not be generating any outputs. Instead we will just use a simple classifier to make our classifications. Thus, we will not be using the decoder layer (the longer section on the right side of the diagram). But do note that, while most of the components are indeed similar, the two layers function somewhat differently.***\n\n## Here is a quick rundown of what our Transformer model will contain the following:\n\n## 1. Embedding Layer:\n\n### a. Word Embeddings: \n**These are the learned dense vectors that represent each word in \nyour vocabulary. These vectors capture the semantic meaning of words. Of course, for our use case, we are utilizing the embeddings from word2vec**\n\n### b. Positional Embeddings: \n**These are vectors that encode the position of each word in the \nsequence. They help the attention mechanism also take the positoning of the words when factoring in the attention scores.**\n\n## 2. Encoder Layer:\n\n### a. Multi-Head Attention: \n**This mechanism allows the model to attend to different parts of the\ninput sequence while capturing various relationships between words.**\n\n### b. Addition (Residual) and Normalization Layer: \n**After multi-head attention, you typically have a residual connection \n(addition) followed by layer normalization. This helps with stable \ntraining and information flow.**\n\n### c. Feed-Forward Neural Network (Pointwise Feed-Forward Layer): \n**This network applies a simple feed-forward transformation to each \nposition separately, allowing the model to capture non-linear \nrelationships between words.**\n\n## 3. Classifier Layer:\n\n### a. Linear Layer: \n**This layer maps the output of the encoder to the number of classes \nyou have in your sentiment analysis task. For example, if you have \nthree sentiment classes (negative, neutral, positive), this layer\nwill output logits for each class.**\n\n### b. Softmax Activation: \n**This activation function is applied to the logits to convert them \ninto probabilities for each class. It makes the model's output \ninterpretable as class probabilities.**","metadata":{}},{"cell_type":"markdown","source":"**When following the diagram as a pipeline, we see that the first thing we do with the inputs is include the embedding. The embedding is layer involves replacing the tokens from our reviews with their vectors. SInce we have used Word2Vec to create the embeddings from our tokens, we just need may proceed to the next step.** **The next step in our diagram is the Positional Encoding. This is a clever approach that the creaters of the Transformer architecture utilized to include even more information into our input embeddings. While the first set of vectors helped give our tokens their contextual vectors, the positional encoder assigns a sin or cosine value to these vectors depending on whether they are located in odd of even positions. This adds to our vectors by highlighting the location of the token on top of its meaning, thus further enriching the information it carries.**\n\n**We will wrap all this up in one function and call that Embeddings.**","metadata":{}},{"cell_type":"markdown","source":"**Now we delve into what truly makes the Transformer architecture exceptional: the Attention mechanism. This mechanism, often referred to as self-attention, has been a transformative development in NLP. It serves as a way to allocate \"focus\" or \"attention\" values to the different embedded vectors that pass through the model's embedding layer. This allocation is achieved by segmenting the input vectors into three key components: queries, keys, and values.**\n\n**The process begins with the transformation of the input embeddings into three distinct matrices: the Query matrix (Q), Key matrix (K), and Value matrix (V). These matrices are obtained through linear transformations of the original embeddings.**\n\n**The heart of the attention mechanism involves calculating dot products between the Query matrix and the Transposed Key matrix. The resulting dot products are then scaled by the square root of the dimension of the key vectors. This scaling factor plays a crucial role in maintaining stable gradients during training.**\n\n**The next step in the process employs a softmax operation on the scaled dot products. The softmax function normalizes the values, converting them into attention weights. These attention weights indicate the \"importance\" or \"weight\" assigned to each word in relation to the others. The summation of the attention weights for each query-key pair ensures that the model focuses on the most relevant information.**\n\n**The weighted sum of the Value matrix, determined by the attention weights, generates an attended representation. In essence, the attention mechanism enables the model to capture the contextual relationships between words, determining how much each word contributes to the overall understanding of the sequence.**\n\n**One innovation that distinguishes the Transformer architecture is multi-head attention. Instead of relying on a single attention mechanism, multiple parallel attention mechanisms, or \"heads,\" are employed. Each head attends to the input independently and learns different aspects of the relationships between words. The outputs of the different heads are concatenated and linearly transformed, resulting in a comprehensive representation that encompasses various perspectives on the input data.**\n\n**Example:**\n\n**Consider the sentence: \"Diana wanted to visit Seattle in the Winter, but was ill-prepared for the cold, Pacific Northwest weather.\"**\n\n**In this sentence, the attention mechanism discerns connections between words, prioritizing specific words when analyzing others. For instance, in the context of \"visit Seattle,\" the word \"Seattle\" would attract more attention when considering the word \"visit.\" Similarly, the words \"ill-prepared\" and \"cold\" would stand out in the presence of the term \"Winter.\"**","metadata":{}},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"# pytorch implementation\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float, seq_len: int):\n        super().__init__()\n        self.dropout = nn.Dropout(p = dropout)\n\n        position = torch.arange(seq_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(seq_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![image.png](attachment:dcbbcd14-b671-403b-903f-86a9c94add2b.png)\n\n* Source: [Transformer Tutorial - Tensorflow](https://www.tensorflow.org/text/tutorials/transformer)","metadata":{},"attachments":{"dcbbcd14-b671-403b-903f-86a9c94add2b.png":{"image/png":"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"}}},{"cell_type":"code","source":"def scaled_dot_product_attention(query, key, value, attn_mask = None, dropout_p = float, is_causal = False, scale = None) -> torch.Tensor:\n    # Efficient implementation equivalent to the following:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype).to(device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal = 0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_mask.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim =- 1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train = True)\n    return attn_weight @ value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention + Multi-head","metadata":{}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout = 0.1):\n        super().__init__()\n\n        self.h = h\n        self.d_k = d_model // h\n        self.attention_heads = [nn.Linear(d_model, d_model // h).to(device) for _ in range(self.h)]\n\n        \n    def forward(self, query, key, value, attn_mask = None, dropout_p=0.0):\n        attention_outputs = []\n        for attention_head in self.attention_heads:\n            attention_output = scaled_dot_product_attention(\n                attention_head(query),\n                attention_head(key),\n                attention_head(value),\n                attn_mask = attn_mask,\n                dropout_p = dropout_p,\n            )\n            attention_outputs.append(attention_output)\n            \n        seq_len = query.size(1)\n        attention_output = torch.cat(attention_outputs, dim =- 1).view(query.size(0), seq_len, d_model)\n        return attention_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Self-Attention Diagram](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_explained-min.png)\n\n![Components of Self-Attention](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_attention_formula-min.png)\n\n* Source: [Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)","metadata":{}},{"cell_type":"markdown","source":"# Add/Residual layer + Normalization Layer","metadata":{}},{"cell_type":"code","source":"def add_layer(x, y):\n    \"\"\"Adds two tensors together.\n\n    Args:\n    x: A torch.Tensor of shape (batch_size, seq_len, hidden_size).\n    y: A torch.Tensor of the same shape as x.\n\n    Returns:\n    A torch.Tensor of the same shape as x and y, containing the sum of the two tensors.\n    \"\"\"\n\n    return torch.add(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps = float):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim = True)\n        std = x.std(-1, keepdim = True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Position-wise Feed Forward Network","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Args:\n            d_model:      dimension of embeddings\n            d_ffn:        dimension of feed-forward network\n            dropout:      probability of dropout occurring\n        \"\"\"\n        super().__init__()\n\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x:            output from attention (batch_size, seq_length, d_model)\n\n        Returns:\n            expanded-and-contracted representation (batch_size, seq_length, d_model)\n        \"\"\"\n        \n        return self.w_2(self.dropout(self.w_1(x).relu()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100  # Maximum sequence length\ndropout = 0.1  # Adjust the dropout if needed\n\nh       = 4    # number of attention head\nnum_heads = 4    # number of attention head\n\nd_ffn   = 1024 # dimension of the feedforward layer\neps     = 1e-6 # epsilon value to prevent the standard deviation from becoming zero\nnum_classes = 5  # Replace with your number of classes\nepochs = 100\nlearning_rate = 0.01\nnum_layers = 6\n\ninput_size = d_model  # Adjust this based on the output size of your feed-forward network\n# input_size = len(train_data[0])  # Adjust based on your input size (should match the output size of your model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequential layer-by-layer test","metadata":{}},{"cell_type":"code","source":"","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Layer","metadata":{}},{"cell_type":"code","source":"class StackedEncoder(nn.Module):\n    def __init__(self, num_layers, d_model, seq_len, dropout, num_heads, d_ffn):\n        super(StackedEncoder, self).__init__()\n\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, seq_len, dropout, num_heads, d_ffn) for _ in range(num_layers)\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, seq_len, dropout, num_heads, d_ffn):\n        super().__init__()\n\n        self.positional_encoding = PositionalEncoding(d_model, dropout, seq_len)\n        self.multihead_attention = MultiHeadedAttention(num_heads, d_model, dropout)\n        self.norm1 = LayerNorm(d_model, eps)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.norm2 = LayerNorm(d_model, eps)\n\n    def forward(self, x):\n        x_pe = self.positional_encoding(x)\n        attn_output = self.multihead_attention(x_pe, x_pe, x_pe)\n        x_attn = x + attn_output # Residual connection from multi-head attention\n        x_norm1 = self.norm1(x_attn)\n        ff_output = self.feed_forward(x_norm1)\n        x_ff = x_attn + ff_output # Residual connection from feed-forward network\n        x_norm2 = self.norm2(x_ff)\n\n        return x_norm2 # Return the normalized output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Test","metadata":{}},{"cell_type":"markdown","source":"**We can now split our dataset into training and validation sets. When working on machine learning or deep learning models, we want to train the model on a subset of the data, and then test the accuracy of the model's predictions on the remaining data. This is called data splitting. We typically split the data into an 80/20 split, where we train the model on 80% of the data and test it on the remaining 20%.**","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_encoder = StackedEncoder(num_layers, d_model, \n                                 seq_len, dropout, num_heads, d_ffn).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Was encoding just the training set when the whole dataset should be encoded","metadata":{}},{"cell_type":"code","source":"total_encoded_batch = []\ntotal_y_batch = []\n\nfor x_batch, y_batch in dataset:  # Iterate over the training batches\n    x_batch_encoded = stacked_encoder(x_batch).to(device)  # Encode the reviews using the stacked encoder\n    total_encoded_batch.append(x_batch_encoded)\n    total_y_batch.append(y_batch)\n\ntotal_encoded_batch = torch.cat(total_encoded_batch, dim = 0)  # Concatenate all the encoded batches into a single tensor\ntotal_y_batch = torch.cat(total_y_batch, dim = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(total_encoded_batch.shape)  # Check the shape of the combined encoded tensor\nprint(total_y_batch.shape)\nprint(f\"The Encoded batch is on: {total_encoded_batch.device}\")\nprint(f\"The Y batch is on: {total_y_batch.device}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Realized that I was spliting the data before encoding and thus the training data was only being encoded. Might also explain why the validation score was so low (along with how small the dataset is and how the large epochs are definitely overfitting)","metadata":{}},{"cell_type":"code","source":"# Lengths \ntrain_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\n\n# Random split\ntrain_data, val_data = random_split(dataset, [train_len, val_len])\n\nprint(f\"The amount of data we have to train with is {len(train_data)}\") \nprint(f\"The amount of data we have to validate with is {len(val_data)}\")\n#print(f\"The amount of data we have to validate with is on {train_data.device}\")\n#print(f\"The amount of data we have to validate with is on {val_data.device}\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Moved input_size here","metadata":{}},{"cell_type":"code","source":"# new dataset object post encoding\nencoded_review_dataset = TensorDataset(total_encoded_batch, total_y_batch)\n\n# Print the sliced dataset\nprint(encoded_review_dataset[:5])\n\n# Should match the output size of your model\ninput_size = len(train_data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classifier","metadata":{}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Changing reshaping format to check on the input size. Hardcoding the reshaping dimensions as 32 was breaking the classifier (presumably due to the data not being perfectly divisible by 32)\n### Moved the hyperparameters a few cells up where the encoder's hyperparameters were since they were similar","metadata":{}},{"cell_type":"code","source":"# Define Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss() # nn.CategoricalCrossentropy() #nn.Softmax() \n\n# Define SGD optimizer\n# Is Adam better?\noptimizer = optim.SGD(classifier.parameters(), lr=learning_rate)\n\nDEV = True\n# Training loop (adjust this to match your data and DataLoader)\nfor epoch in range(epochs):\n    for inputs, targets in train_loader :  # Assuming you have a DataLoader\n        # for batch_data in train_loader:  # Assuming you have a DataLoader\n        # inputs, targets = batch_data  # Assuming your DataLoader provides input data and targets    \n    \n        #printd(f'inputs shape: {inputs.shape}')\n        #printd(f'targets shape: {targets.shape}')\n        #printd(f'targets: {targets}')\n\n        optimizer.zero_grad()\n    \n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        inputs = torch.reshape(inputs, (inputs.size(0), -1)) # get current batch size\n        #inputs = torch.reshape(inputs, (32,30000))\n        \n        #printd(f'Reshaped inputs: {inputs.shape}')\n        \n        outputs = classifier(inputs)\n        #printd(f'outputs shape {outputs.shape}')\n        \n        # output is a 32 x 6 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        loss = criterion(outputs, targets)\n        # print(f'loss.item: {loss.item()}')\n        loss.backward(retain_graph=True)\n        optimizer.step()\n     \n    print(f'Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n    \nDEV = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import cuda\n\n\n\ncuda.select_device(device)\n\ncuda.close()\n\n\n\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T22:41:33.202185Z","iopub.execute_input":"2023-12-10T22:41:33.202568Z","iopub.status.idle":"2023-12-10T22:41:33.338984Z","shell.execute_reply.started":"2023-12-10T22:41:33.202537Z","shell.execute_reply":"2023-12-10T22:41:33.337745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n#print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n#print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put model in evaluation mode\nclassifier.eval() \n\n# Tracking variables\npredictions = []\nactuals = []\n\n# Evaluate on validation set\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.reshape(inputs.shape[0], -1)\n        \n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n        \n        predictions.extend(predicted.tolist())\n        actuals.extend(targets.tolist())\n        \n# Print sample outputs        \nprint(\"Predicted | Actual\")\nfor i in range (val_loader):\n    print(f\"{predictions[i]} | {actuals[i]}\")\n    \n# Calculate validation accuracy\nnum_correct = sum([p == a for p, a in zip(predictions, actuals)]) \nval_accuracy = num_correct / len(predictions)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# The next two lines were an attempt to fix the obscure CUDA assert error --\n# didn't work. But there's a workaround, just run without the accelerator.\n# You can get away with that if you just have 5,000 reviews in each category.\n#\nCUDA_LAUNCH_BLOCKING=1 \nTORCH_USE_CUDA_DSA=1\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes+1)#.to(device)\n\n# Define Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss() # nn.CategoricalCrossentropy() #nn.Softmax() \n\n# Define SGD optimizer\n# Is Adam better?\noptimizer = optim.SGD(classifier.parameters(), lr=learning_rate)\n\n# DEV = True\n# Training loop (adjust this to match your data and DataLoader)\nfor epoch in range(epochs):\n    for x_batch, y_batch in encoded_review_dataset:  # Assuming you have a DataLoader\n        inputs, targets = x_batch, y_batch  # Assuming your DataLoader provides input data and targets\n       \n        printd(f'targets shape: {targets.shape}')\n        printd(f'targets: {targets}')\n        printd(f'inputs shape: {inputs.shape}')\n        printd(f'inputs: {inputs}')\n\n        optimizer.zero_grad()\n    \n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        #\n        inputs = torch.reshape(inputs, (32,30000))\n        printd(f'Reshaped inputs: {inputs.shape}')\n        \n        outputs = classifier(inputs)\n        printd(f'outputs shape {outputs.shape}')\n        \n        #dummy_output = torch.randn(32, 5, requires_grad=True).to(device)\n        # printd(f'dummy_output shape {dummy_output.shape}')\n        # printd(f'dummy_output {dummy_output}')\n        # output should be a 32 x 5 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    \n    print(f'Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n    \nDEV = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"# Define the Classifier class\nclass Classifier(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(Classifier, self).__init__()\n        \n        self.linear_1 = nn.Linear(input_size, num_classes)\n        self.linear_2 = nn.Linear(1, num_classes)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x1):\n        \n        linear_1_output = self.linear_1(x_batch)\n        linear_2_output = self.linear_2(y_batch)\n        combined_output = linear_1_output + linear_2_output\n        output = self.sigmoid(combined_output)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the classifier\nclassifier = Classifier(input_size, num_classes)#.to(device)\n\n# Define Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss()\n\n# Define SGD optimizer for both classifier\noptimizer = optim.SGD(list(classifier.parameters()), lr = learning_rate)\n\n# Training loop (adjust this to match your data and DataLoader)\nfor epoch in range(epochs):\n    for batch_data in :  # Assuming you have a DataLoader\n        inputs, targets = x_batch, y_batch  # Assuming your DataLoader provides input data and targets\n        optimizer.zero_grad()\n        \n        outputs = classifier()  \n        loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch + 1}/{epochs}] Loss: {loss.item()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can think of the loss function as the valley, with the steepest parts being where the loss is highest and the flatter parts being where the loss is lower. The hiker is the set of model parameters, which are being updated with each step (iteration). SGD is like the GPS for the hiker, helping it navigate toward the lowest point in the valley (i.e. the global minimum of the loss function) with each step. The key difference is that SGD is an iterative process that updates the parameters in small steps, rather than finding the global minimum in a single shot. The learning rate is essentially the \"speed limit\" for the hiker (the model parameters). The learning rate would be how far the GPS would tell the hiker to go before it iterates/updates. A higher learning rate means that the hiker can make larger updates to the parameters with each step, while a lower learning rate means that the updates are smaller. This is important because it determines not only how quickly the model can find the global minimum, but also how stable the descent is - if the learning rate is too high, the hiker could get lost in the valley, but if it's too low, it could take forever to reach the bottom. So, the learning rate controls the speed of the descent and helps the model find the optimal set of parameters without overshooting or taking too long.\n\nSome additional points to augment the analogy:\n\nThe terrain can be rugged with many hills, valleys, and obstacles - like the high-dimensional, non-convex loss function landscape.\n\nMomentum in SGD helps the hiker build up speed in consistently downhill directions, like momentum in GD.\n\nAdaptive LR is like the GPS dynamically recommending faster or slower speeds depending on terrain.\n\nRegularization is like constraining the hiker's path to stay on trails and avoid dangerous cliffs (to prevent overfitting).\n\nBatch size is like the number of hikers traversing together (to average gradient noise).\n\nEpochs are like hiking to the bottom, then restarting from the top to repeat the descent.\n\nEarly stopping is like stopping the hike once you reach a good enough point close to the valley floor.\n\nHyperparameter tuning is like tweaking the GPS settings for optimal navigation.\n\nVanishing gradients are like certain paths becoming too steep or hitting dead-ends.\n\nLocal minima: The terrain can be rugged with many local minima, which are small valleys that are not as deep as the global minimum. SGD can get stuck in local minima, but there are various techniques that can help to avoid this, such as momentum and adaptive learning rate.\n\nLearning rate schedule: The learning rate can be adjusted over time to improve the performance of SGD. This is known as a learning rate schedule. For example, the learning rate can be decreased as the model converges to the global minimum, in order to prevent overshooting.\n\nNoise: The gradient calculation can be noisy, especially when the batch size is small. This can cause SGD to take a more erratic path towards the global minimum. However, the noise can be averaged out by using a larger batch size.","metadata":{}}]}