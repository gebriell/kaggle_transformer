{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8259057,"sourceType":"datasetVersion","datasetId":4901680}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2372.211165,"end_time":"2024-04-07T06:44:42.316126","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T06:05:10.104961","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndataset_file_name = ''\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('hdf5'):\n            dataset_file_name = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Preprocessed data file: {dataset_file_name}')\n\n# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n    accelerator = True\n\nelse:\n    accelerator = False\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n    print(device)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = False\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n    return\n\nshowCellCompletion = True  #<< 4/12/24 set default to True\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":18.682261,"end_time":"2024-04-07T06:05:31.518053","exception":false,"start_time":"2024-04-07T06:05:12.835792","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-29T19:39:08.600068Z","iopub.execute_input":"2024-04-29T19:39:08.600497Z","iopub.status.idle":"2024-04-29T19:39:29.605022Z","shell.execute_reply.started":"2024-04-29T19:39:08.600458Z","shell.execute_reply":"2024-04-29T19:39:29.604115Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Preprocessed data file: /kaggle/input/80000/extracted_data.hdf5\nCUDA is available!\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import h5py\n\nfile_path = '/kaggle/input/80000/extracted_data.hdf5'\nwith h5py.File(file_path, 'r') as hf:\n    # Access the datasets within the HDF5 file\n    text_reviews_dataset = hf['text_reviews']\n    ratings_dataset = hf['ratings']\n\n    # Convert the datasets to PyTorch tensors\n    text_reviews = torch.from_numpy(text_reviews_dataset[:])\n    ratings = torch.from_numpy(ratings_dataset[:])\n\n# Use the loaded tensors as needed\nprint(text_reviews.shape)\nprint(ratings.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:39:29.606826Z","iopub.execute_input":"2024-04-29T19:39:29.607316Z","iopub.status.idle":"2024-04-29T19:40:58.044672Z","shell.execute_reply.started":"2024-04-29T19:39:29.607289Z","shell.execute_reply":"2024-04-29T19:40:58.043626Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"torch.Size([80000, 100, 300])\ntorch.Size([80000])\n","output_type":"stream"}]},{"cell_type":"code","source":"# import pickle\n# with open(dataset_file_name, 'rb') as dataset_file:\n#     dataset = pickle.load(dataset_file)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T19:40:58.045924Z","iopub.execute_input":"2024-04-29T19:40:58.046224Z","iopub.status.idle":"2024-04-29T19:40:58.050377Z","shell.execute_reply.started":"2024-04-29T19:40:58.046198Z","shell.execute_reply":"2024-04-29T19:40:58.049414Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = TensorDataset(text_reviews, ratings)\n\n# Assuming your data is stored in 'dataset' as a PyTorch dataset object\nX = dataset.tensors[0]  # Assuming the reviews are stored at index 1 in the dataset tensors\ny = dataset.tensors[1]  # Assuming the ratings are stored at index 0 in the dataset tensors\n\nprint(\"Shape of X (reviews):\", X.shape)\nprint(\"Shape of y (ratings):\", y.shape)\n\nX_train = []\nX_val = []\ny_train = []\ny_val = []\n\n# Perform stratified splitting for each rating class\nfor rating in torch.unique(y):\n    X_rating = X[y == rating]\n    y_rating = y[y == rating]\n\n    X_train_rating, X_val_rating, y_train_rating, y_val_rating = train_test_split(\n        X_rating, y_rating, test_size = 0.2, random_state = 42, stratify = y_rating)\n\n    X_train.append(X_train_rating)\n    X_val.append(X_val_rating)\n    y_train.append(y_train_rating)\n    y_val.append(y_val_rating)\n\n# Combine the split data from all rating classes\nX_train = torch.cat(X_train)\nX_val = torch.cat(X_val)\ny_train = torch.cat(y_train)\ny_val = torch.cat(y_val)\n\n# Create new datasets using the split data\ntrain_data = TensorDataset(X_train, y_train)\nval_data = TensorDataset(X_val, y_val)\n\nprint(\"Training Set:\")\nprint(\"Number of ratings:\", len(y_train))\nprint(\"Number of reviews:\", len(X_train))\nprint(\"Number of reviews per rating:\")\nfor rating in torch.unique(y_train):\n    count = torch.sum(y_train == rating).item()\n    print(f\"Rating {rating}: {count} reviews\")\n\nprint(\"\\nValidation Set:\")\nprint(\"Number of ratings:\", len(y_val))\nprint(\"Number of reviews:\", len(X_val))\nprint(\"Number of reviews per rating:\")\nfor rating in torch.unique(y_val):\n    count = torch.sum(y_val == rating).item()\n    print(f\"Rating {rating}: {count} reviews\")\n\nprintv(f\"The amount of data we have to train with is {len(train_data)} reviews\") \nprintv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"papermill":{"duration":0.025623,"end_time":"2024-04-07T06:22:13.305100","exception":false,"start_time":"2024-04-07T06:22:13.279477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-29T19:40:58.052513Z","iopub.execute_input":"2024-04-29T19:40:58.052836Z","iopub.status.idle":"2024-04-29T19:41:11.062007Z","shell.execute_reply.started":"2024-04-29T19:40:58.052812Z","shell.execute_reply":"2024-04-29T19:41:11.061039Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Shape of X (reviews): torch.Size([80000, 100, 300])\nShape of y (ratings): torch.Size([80000])\nTraining Set:\nNumber of ratings: 64000\nNumber of reviews: 64000\nNumber of reviews per rating:\nRating 1: 12800 reviews\nRating 2: 12800 reviews\nRating 3: 12800 reviews\nRating 4: 12800 reviews\nRating 5: 12800 reviews\n\nValidation Set:\nNumber of ratings: 16000\nNumber of reviews: 16000\nNumber of reviews per rating:\nRating 1: 3200 reviews\nRating 2: 3200 reviews\nRating 3: 3200 reviews\nRating 4: 3200 reviews\nRating 5: 3200 reviews\nVERBOSE: The amount of data we have to train with is 64000 reviews\nVERBOSE: The amount of data we have to validate with is 16000 reviews\n","output_type":"stream"}]},{"cell_type":"code","source":"# HyperParameters for the model\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100 #<< 4/13/24 100  # Maximum sequence length\ndropout = 0.1  # Adjust the dropout if needed\n\nnum_layers = 15 # depth of our network\ninput_size = d_model  # match the output dim of your ff_net\nnum_classes = 5  # our ratings (1 - 5)\nhidden_size = 2048 # 2^n\n\neps    = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\nepochs = 60 #<< 100\nlearning_rate = 0.001\nweight_decay  = 0.01\n\nshowC('Hyperparameters defined')","metadata":{"papermill":{"duration":0.018663,"end_time":"2024-04-07T06:22:13.334973","exception":false,"start_time":"2024-04-07T06:22:13.316310","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-29T19:41:11.063181Z","iopub.execute_input":"2024-04-29T19:41:11.063471Z","iopub.status.idle":"2024-04-29T19:41:11.069869Z","shell.execute_reply.started":"2024-04-29T19:41:11.063447Z","shell.execute_reply":"2024-04-29T19:41:11.068833Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cell complete: Hyperparameters defined\n","output_type":"stream"}]},{"cell_type":"code","source":"class NeuralNetClassifier(nn.Module):\n    def __init__(self, r_size, v_size, num_classes, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout):\n        super(NeuralNetClassifier, self).__init__()\n        \n        self.hidden_layers = nn.ModuleList()\n        self.hidden_layers.append(nn.Linear(r_size * v_size, hidden_size))\n        self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n        for _ in range(num_layers - 1):\n            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n        self.output_layer = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.LeakyReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = layer(x)\n            x = self.relu(x)\n            x = self.dropout(x)\n        \n        x = self.output_layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:41:11.071082Z","iopub.execute_input":"2024-04-29T19:41:11.071332Z","iopub.status.idle":"2024-04-29T19:41:11.085311Z","shell.execute_reply.started":"2024-04-29T19:41:11.071311Z","shell.execute_reply":"2024-04-29T19:41:11.084530Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"classifier = NeuralNetClassifier(seq_len, d_model, num_classes, hidden_size, num_layers, dropout)\nclassifier = classifier.to(device)\n\nprint(classifier)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:41:11.086322Z","iopub.execute_input":"2024-04-29T19:41:11.086615Z","iopub.status.idle":"2024-04-29T19:41:12.507358Z","shell.execute_reply.started":"2024-04-29T19:41:11.086594Z","shell.execute_reply":"2024-04-29T19:41:12.506246Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"NeuralNetClassifier(\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=30000, out_features=2048, bias=True)\n    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): Linear(in_features=2048, out_features=2048, bias=True)\n    (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=2048, out_features=2048, bias=True)\n    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Linear(in_features=2048, out_features=2048, bias=True)\n    (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): Linear(in_features=2048, out_features=2048, bias=True)\n    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=2048, out_features=2048, bias=True)\n    (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): Linear(in_features=2048, out_features=2048, bias=True)\n    (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): Linear(in_features=2048, out_features=2048, bias=True)\n    (15): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): Linear(in_features=2048, out_features=2048, bias=True)\n    (17): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (18): Linear(in_features=2048, out_features=2048, bias=True)\n    (19): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (20): Linear(in_features=2048, out_features=2048, bias=True)\n    (21): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): Linear(in_features=2048, out_features=2048, bias=True)\n    (23): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (24): Linear(in_features=2048, out_features=2048, bias=True)\n    (25): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): Linear(in_features=2048, out_features=2048, bias=True)\n    (27): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (28): Linear(in_features=2048, out_features=2048, bias=True)\n    (29): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (output_layer): Linear(in_features=2048, out_features=5, bias=True)\n  (relu): LeakyReLU(negative_slope=0.01)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# neural net with rnn","metadata":{}},{"cell_type":"code","source":"# class RecurrentNeuralNetClassifier(nn.Module):\n#     def __init__(self, r_size, v_size, num_classes, hidden_size = hidden_size, \n#                  num_layers = num_layers, dropout = dropout, rnn_hidden_size = 256, rnn_num_layers = 1):\n#         super(RecurrentNeuralNetClassifier, self).__init__()\n\n#         self.rnn = nn.RNN(r_size * v_size, rnn_hidden_size, rnn_num_layers, batch_first=True)\n        \n#         self.hidden_layers = nn.ModuleList()\n#         self.hidden_layers.append(nn.Linear(rnn_hidden_size, hidden_size))\n#         self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n#         for _ in range(num_layers - 1):\n#             self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n#             self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n#         self.output_layer = nn.Linear(hidden_size, num_classes)\n#         self.relu = nn.LeakyReLU()\n#         self.dropout = nn.Dropout(dropout)\n        \n#     def forward(self, x):\n#         # Reshape the input to match the expected shape for RNN\n#         x = x.view(x.size(0), -1, x.size(-1))\n        \n#         # Pass the input through the RNN layer\n#         x, _ = self.rnn(x)\n        \n#         # Take the last output of the RNN\n#         x = x[:, -1, :]\n        \n#         for layer in self.hidden_layers:\n#             x = layer(x)\n#             x = self.relu(x)\n#             x = self.dropout(x)\n        \n#         x = self.output_layer(x)\n#         return x","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T19:41:12.508511Z","iopub.execute_input":"2024-04-29T19:41:12.508806Z","iopub.status.idle":"2024-04-29T19:41:12.513835Z","shell.execute_reply.started":"2024-04-29T19:41:12.508780Z","shell.execute_reply":"2024-04-29T19:41:12.512947Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# classifier = RecurrentNeuralNetClassifier(seq_len, d_model, num_classes, hidden_size, num_layers, dropout)\n# classifier = classifier.to(device)\n\n# print(classifier)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T19:41:12.515083Z","iopub.execute_input":"2024-04-29T19:41:12.515488Z","iopub.status.idle":"2024-04-29T19:41:12.526770Z","shell.execute_reply.started":"2024-04-29T19:41:12.515459Z","shell.execute_reply":"2024-04-29T19:41:12.525916Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# early neural nets","metadata":{}},{"cell_type":"code","source":"# class NeuralNetClassifier(nn.Module):\n#     def __init__(self, r_size, v_size, num_classes):\n#         # r_size is the number of tokens in a review, 100.\n#         # v_size is the number of values in an embedding vector, 300.\n#         super(NeuralNetClassifier, self).__init__()\n        \n#         # The input to fc will be a 2D tensor with with n rows and\n#         # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n#         # with n rows and num_classes columns.\n#         self.hidden_layer1 = nn.Linear(r_size * v_size, hidden_size)\n#         self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n#         self.hidden_layer3 = nn.Linear(hidden_size, num_classes)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(dropout) #>> 0.2 seems to work OK\n#         #self.softmax = nn.Softmax(dim=1)  # Softmax with dim=1 for class probabilities\n\n#     def forward(self, x):\n#         x = self.hidden_layer1(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         x = self.hidden_layer2(x)\n#         x = self.relu(x)\n#         x = self.dropout(x) #>> dropout rate 0.1 may result in underfitting?\n#         #x = self.softmax(x)  # Apply softmax after the output layer\n#         x = self.hidden_layer3(x)\n#         return x\n    \n# classifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1).to(device)\n# showC(f'{classifier} defined')\n\n'''\nclass Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\n\nshowC(f'{Classifier} defined')\n'''","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T19:41:12.530056Z","iopub.execute_input":"2024-04-29T19:41:12.530616Z","iopub.status.idle":"2024-04-29T19:41:12.541238Z","shell.execute_reply.started":"2024-04-29T19:41:12.530592Z","shell.execute_reply":"2024-04-29T19:41:12.540377Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"\\nclass Classifier(nn.Module):\\n    def __init__(self, r_size,v_size, num_classes):\\n        # r_size is the number of tokens in a review, 100.\\n        # v_size is the number of values in an embedding vector, 300.\\n        super(Classifier, self).__init__()\\n        \\n        # The input to fc will be a 2D tensor with with n rows and\\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\\n        # with n rows and num_classes columns.\\n        self.fc = nn.Linear(r_size * v_size, num_classes)\\n\\n    def forward(self, x1):\\n        # Pass input through the linear layer\\n        return self.fc(x1)\\n\\n# Create the classifier\\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\\n\\nshowC(f'{Classifier} defined')\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"# stoppage","metadata":{}},{"cell_type":"code","source":"# class EarlyStopping():\n#     \"\"\"\n#     Early stopping to stop the training when the loss does not improve after\n#     certain epochs.\n#     Credit:\n#     https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n#     \"\"\"\n#     def __init__(self, patience=5, min_delta=0):\n#         \"\"\"\n#         :param patience: how many epochs to wait before stopping when loss is\n#                not improving\n#         :param min_delta: minimum difference between new loss and old loss for\n#                new loss to be considered as an improvement\n#         \"\"\"\n#         self.patience = patience\n#         self.min_delta = min_delta\n#         self.counter = 0\n#         self.best_loss = None\n#         self.early_stop = False\n        \n#     def __call__(self, val_loss):\n#         if self.best_loss == None:\n#             self.best_loss = val_loss\n#         elif self.best_loss - val_loss > self.min_delta:\n#             self.best_loss = val_loss\n#             # reset counter if validation loss improves\n#             self.counter = 0\n#         elif self.best_loss - val_loss < self.min_delta:\n#             self.counter += 1\n#             #printd(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n#             if self.counter >= self.patience:\n#                 printv(f'Early stopping: counter={self.counter}; patience={self.patience}')\n#                 self.early_stop = True\n# early_stopping = EarlyStopping(patience=20) #patience=30, min_delta=.01) \n# showC(f'{EarlyStopping} object defined')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T19:41:12.542305Z","iopub.execute_input":"2024-04-29T19:41:12.542640Z","iopub.status.idle":"2024-04-29T19:41:12.553158Z","shell.execute_reply.started":"2024-04-29T19:41:12.542610Z","shell.execute_reply":"2024-04-29T19:41:12.552290Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\n# Is Adam better? Didn't seem so based on 4/23 heuristics\n# optimizer = optim.SGD(classifier.parameters(), lr = learning_rate)\noptimizer = optim.AdamW(classifier.parameters(), \n                        lr = learning_rate, weight_decay = weight_decay)\n\nDEV = True\n\n# Training loop\nlosses = {} #<< track losses\nfor epoch in range(epochs):\n    for inputs, targets in train_loader : \n\n        optimizer.zero_grad()\n\n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        inputs.to(device)\n        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n\n        outputs = classifier(inputs).to(device)\n\n        # output is a 32 x 6 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        # loss = criterion(outputs.to(device), targets.to(device))\n        loss = criterion(outputs.to(device), targets.to(device).long())\n        loss.backward(retain_graph = True)\n\n        optimizer.step()\n\n    losses[loss.item()] = epoch + 1\n#    early_stopping(loss)\n\n#     if early_stopping.early_stop:\n#         printv(f'Stopping early at epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n#         break    \n\n    # if epoch % 50 == 0:\n    printv(f'Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n\nif VERBOSE:\n    printv(f'Last loss: Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n    smallest_losses = sorted(list(losses.keys()))\n    printv('Smallest losses')\n    for idx in range(3):\n        l = smallest_losses[idx]\n        printv(f'    Loss: {l}, epoch = {losses[l]}')\nshowC(f'training complete')","metadata":{"papermill":{"duration":1344.87285,"end_time":"2024-04-07T06:44:38.254791","exception":false,"start_time":"2024-04-07T06:22:13.381941","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-29T19:41:12.554197Z","iopub.execute_input":"2024-04-29T19:41:12.554592Z","iopub.status.idle":"2024-04-29T20:36:02.767714Z","shell.execute_reply.started":"2024-04-29T19:41:12.554540Z","shell.execute_reply":"2024-04-29T20:36:02.766554Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"VERBOSE: Epoch [1 / 60] Loss: 1.8524051904678345\nVERBOSE: Epoch [2 / 60] Loss: 1.7462939023971558\nVERBOSE: Epoch [3 / 60] Loss: 1.6739366054534912\nVERBOSE: Epoch [4 / 60] Loss: 1.646699070930481\nVERBOSE: Epoch [5 / 60] Loss: 1.5220354795455933\nVERBOSE: Epoch [6 / 60] Loss: 1.397395372390747\nVERBOSE: Epoch [7 / 60] Loss: 1.466392159461975\nVERBOSE: Epoch [8 / 60] Loss: 1.365941047668457\nVERBOSE: Epoch [9 / 60] Loss: 1.3313837051391602\nVERBOSE: Epoch [10 / 60] Loss: 1.3672443628311157\nVERBOSE: Epoch [11 / 60] Loss: 1.2212635278701782\nVERBOSE: Epoch [12 / 60] Loss: 1.0839457511901855\nVERBOSE: Epoch [13 / 60] Loss: 0.9267241358757019\nVERBOSE: Epoch [14 / 60] Loss: 0.6322123408317566\nVERBOSE: Epoch [15 / 60] Loss: 0.5656606554985046\nVERBOSE: Epoch [16 / 60] Loss: 0.5727630853652954\nVERBOSE: Epoch [17 / 60] Loss: 0.5357504487037659\nVERBOSE: Epoch [18 / 60] Loss: 0.7478423714637756\nVERBOSE: Epoch [19 / 60] Loss: 0.3914888799190521\nVERBOSE: Epoch [20 / 60] Loss: 0.08481483906507492\nVERBOSE: Epoch [21 / 60] Loss: 0.3177870810031891\nVERBOSE: Epoch [22 / 60] Loss: 0.2213154137134552\nVERBOSE: Epoch [23 / 60] Loss: 0.2453247606754303\nVERBOSE: Epoch [24 / 60] Loss: 0.13933345675468445\nVERBOSE: Epoch [25 / 60] Loss: 0.3246261775493622\nVERBOSE: Epoch [26 / 60] Loss: 0.3528943359851837\nVERBOSE: Epoch [27 / 60] Loss: 0.1687975525856018\nVERBOSE: Epoch [28 / 60] Loss: 0.08050837367773056\nVERBOSE: Epoch [29 / 60] Loss: 0.147923544049263\nVERBOSE: Epoch [30 / 60] Loss: 0.5097746253013611\nVERBOSE: Epoch [31 / 60] Loss: 0.3844163119792938\nVERBOSE: Epoch [32 / 60] Loss: 0.06841545552015305\nVERBOSE: Epoch [33 / 60] Loss: 0.15326452255249023\nVERBOSE: Epoch [34 / 60] Loss: 0.09746985882520676\nVERBOSE: Epoch [35 / 60] Loss: 0.35331863164901733\nVERBOSE: Epoch [36 / 60] Loss: 0.2549917995929718\nVERBOSE: Epoch [37 / 60] Loss: 0.3937443494796753\nVERBOSE: Epoch [38 / 60] Loss: 0.11036628484725952\nVERBOSE: Epoch [39 / 60] Loss: 0.08987334370613098\nVERBOSE: Epoch [40 / 60] Loss: 0.1005808413028717\nVERBOSE: Epoch [41 / 60] Loss: 0.07662157714366913\nVERBOSE: Epoch [42 / 60] Loss: 0.20554059743881226\nVERBOSE: Epoch [43 / 60] Loss: 0.020261291414499283\nVERBOSE: Epoch [44 / 60] Loss: 0.029401104897260666\nVERBOSE: Epoch [45 / 60] Loss: 0.22699028253555298\nVERBOSE: Epoch [46 / 60] Loss: 0.2578917145729065\nVERBOSE: Epoch [47 / 60] Loss: 0.023899482563138008\nVERBOSE: Epoch [48 / 60] Loss: 0.29311656951904297\nVERBOSE: Epoch [49 / 60] Loss: 0.04071715474128723\nVERBOSE: Epoch [50 / 60] Loss: 0.03987956419587135\nVERBOSE: Epoch [51 / 60] Loss: 0.41296833753585815\nVERBOSE: Epoch [52 / 60] Loss: 0.021443229168653488\nVERBOSE: Epoch [53 / 60] Loss: 0.01665652170777321\nVERBOSE: Epoch [54 / 60] Loss: 0.007621457800269127\nVERBOSE: Epoch [55 / 60] Loss: 0.1341133564710617\nVERBOSE: Epoch [56 / 60] Loss: 0.009668158367276192\nVERBOSE: Epoch [57 / 60] Loss: 0.08044975996017456\nVERBOSE: Epoch [58 / 60] Loss: 0.012059289030730724\nVERBOSE: Epoch [59 / 60] Loss: 0.015652701258659363\nVERBOSE: Epoch [60 / 60] Loss: 0.017690785229206085\nVERBOSE: Last loss: Epoch [60 / 60] Loss: 0.017690785229206085\nVERBOSE: Smallest losses\nVERBOSE:     Loss: 0.007621457800269127, epoch = 54\nVERBOSE:     Loss: 0.009668158367276192, epoch = 56\nVERBOSE:     Loss: 0.012059289030730724, epoch = 58\nCell complete: training complete\n","output_type":"stream"}]},{"cell_type":"code","source":"# Put model in evaluation mode\nclassifier.eval() \n\n# Tracking variables\npredictions = []\nactuals = []\n\n# Evaluate on validation set\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n\n        predictions.extend(predicted.tolist())\n        actuals.extend(targets.tolist())\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(actuals, predictions)\nprecision = precision_score(actuals, predictions, average='weighted')\nrecall = recall_score(actuals, predictions, average='weighted')\nf1 = f1_score(actuals, predictions, average='weighted')\n\n# Print evaluation metrics\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\nprint(f\"Validation Precision: {precision:.4f}\")\nprint(f\"Validation Recall: {recall:.4f}\")\nprint(f\"Validation F1-score: {f1:.4f}\")\n\n# Calculate confusion matrix\ncm = confusion_matrix(actuals, predictions)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# # Analyze predictions by category\n# num_categories = len(cm)\n# for idx in range(num_categories):\n#     print(f\"Category {idx+1} predictions actual results:\")\n#     for j in range(num_categories):\n#         print(f\"{j+1}. {cm[idx][j]}\")\n\n# Assess bias and variance\nif accuracy < 0.7:  # Adjust the threshold as per your requirements\n    print(\"The model may have high bias (underfitting). Consider increasing model complexity.\")\nelif accuracy > 0.95:  # Adjust the threshold as per your requirements\n    print(\"The model may have high variance (overfitting). Consider regularization techniques.\")\nelse:\n    print(\"The model seems to have a good balance between bias and variance.\")\n    \n# r_by_category = [0,0,0,0,0]\n# r = list('12345')\n\n# for idx in range(5):\n#     r[idx] = r_by_category[:]\n\n# for p,a in zip(predictions, actuals):\n#     r[p-1][a-1] += 1 # Record the actual results for each category prediction\n\n# num_correct = 0\n# for idx in range (5):\n#     printv(f'Categrory {idx+1} predictions actual results: ' +\\\n#            f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n#     num_correct += r[idx][idx]\n\n# # num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \n# val_accuracy = num_correct / len(predictions)\n# print(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:36:02.768975Z","iopub.execute_input":"2024-04-29T20:36:02.769573Z","iopub.status.idle":"2024-04-29T20:36:05.702582Z","shell.execute_reply.started":"2024-04-29T20:36:02.769534Z","shell.execute_reply":"2024-04-29T20:36:05.701573Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.5134\nValidation Precision: 0.5236\nValidation Recall: 0.5134\nValidation F1-score: 0.5066\nConfusion Matrix:\n[[2106  271  411   86  326]\n [ 787 1342  568  170  333]\n [ 509  313 1584  306  488]\n [ 319  186  626 1102  967]\n [ 294  112  373  341 2080]]\nThe model may have high bias (underfitting). Consider increasing model complexity.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Put model in evaluation mode\n# classifier.eval() \n\n# # Tracking variables\n# predictions = []\n# actuals = []\n\n# # Evaluate on validation set\n# with torch.no_grad():\n#     for inputs, targets in val_loader:\n#         inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n\n#         outputs = classifier(inputs)\n#         _, predicted = torch.max(outputs, 1)\n\n#         predictions.extend(predicted.tolist())\n#         actuals.extend(targets.tolist())\n\n# # Print predicted and actual values for all samples\n# #>> print(\"Predicted | Actual\")\n# #>>for pred, actual in zip(predictions, actuals):\n# #>>    pass #printd(f\"{pred} | {actual}\")\n\n# # Calculate validation accuracy\n# #>> 4/12/24 Maybe it would help to see how close we came in each category?\n# #>> For example, for category 5 predictions, show the actual results in each \n# #>> category. And where's there's a large disrepancy, show  the reviews.\n# r_by_category = [0,0,0,0,0]\n# r = list('12345')\n\n# for idx in range(5):\n#     r[idx] = r_by_category[:]\n\n# for p,a in zip(predictions, actuals):\n#     r[p-1][a-1] += 1 # Record the actual results for each category prediction\n\n# num_correct = 0\n# for idx in range (5):\n#     printv(f'Categrory {idx+1} predictions actual results: ' +\\\n#            f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n#     num_correct += r[idx][idx]\n\n# # num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \n# val_accuracy = num_correct / len(predictions)\n# print(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"papermill":{"duration":0.381905,"end_time":"2024-04-07T06:44:38.725263","exception":false,"start_time":"2024-04-07T06:44:38.343358","status":"completed"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-29T20:36:05.703953Z","iopub.execute_input":"2024-04-29T20:36:05.704840Z","iopub.status.idle":"2024-04-29T20:36:05.710408Z","shell.execute_reply.started":"2024-04-29T20:36:05.704811Z","shell.execute_reply":"2024-04-29T20:36:05.709502Z"},"trusted":true},"execution_count":14,"outputs":[]}]}