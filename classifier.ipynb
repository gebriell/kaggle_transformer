{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8182344,"sourceType":"datasetVersion","datasetId":4844516}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2372.211165,"end_time":"2024-04-07T06:44:42.316126","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T06:05:10.104961","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndataset_file_name = ''\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('dataset'):\n            dataset_file_name = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Preprocessed data file: {dataset_file_name}')      \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = False\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n    return\n\nshowCellCompletion = True  #<< 4/12/24 set default to True\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":18.682261,"end_time":"2024-04-07T06:05:31.518053","exception":false,"start_time":"2024-04-07T06:05:12.835792","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:16:15.538357Z","iopub.execute_input":"2024-04-23T19:16:15.538776Z","iopub.status.idle":"2024-04-23T19:16:46.814199Z","shell.execute_reply.started":"2024-04-23T19:16:15.538744Z","shell.execute_reply":"2024-04-23T19:16:46.813173Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found unexpected file: /kaggle/input/preprocessed-dataset/preprocessed_dataset.json\nPreprocessed data file: /kaggle/input/preprocessed-dataset/preprocessed_dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n    accelerator = True\n\nelse:\n    accelerator = False\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n    print(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:16:46.816249Z","iopub.execute_input":"2024-04-23T19:16:46.816931Z","iopub.status.idle":"2024-04-23T19:16:46.827061Z","shell.execute_reply.started":"2024-04-23T19:16:46.816885Z","shell.execute_reply":"2024-04-23T19:16:46.826027Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CUDA is available!\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nwith open(dataset_file_name, 'rb') as dataset_file:\n    dataset = pickle.load(dataset_file)\n    \ntrain_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\n\n# Random split\ntrain_data, val_data = random_split(dataset, [train_len, val_len])\n\nprintv(f\"The amount of data we have to train with is {len(train_data)} reviews\") \nprintv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n#print(f\"The amount of data we have to validate with is on {train_data.device}\")\n#print(f\"The amount of data we have to validate with is on {val_data.device}\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"papermill":{"duration":0.025623,"end_time":"2024-04-07T06:22:13.305100","exception":false,"start_time":"2024-04-07T06:22:13.279477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:16:46.828572Z","iopub.execute_input":"2024-04-23T19:16:46.829622Z","iopub.status.idle":"2024-04-23T19:16:51.259811Z","shell.execute_reply.started":"2024-04-23T19:16:46.829593Z","shell.execute_reply":"2024-04-23T19:16:51.258578Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"VERBOSE: The amount of data we have to train with is 4000 reviews\nVERBOSE: The amount of data we have to validate with is 1000 reviews\n","output_type":"stream"}]},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100 #<< 4/13/24 100  # Maximum sequence length\ndropout = 0.1  # Adjust the dropout if needed\n\neps     = 1e-6 # epsilon value to prevent the standard deviation from becoming zero\nnum_classes = 5  # Replace with your number of classes\nepochs = 200 #<< 1000\nlearning_rate = 0.01\nnum_layers = 6\nhidden_size = d_model\n\ninput_size = d_model  # Adjust this based on the output size of your feed-forward network\n# input_size = len(train_data[0])  # Adjust based on your input size (should match the output size of your model)\nshowC('Hyperparameters defined')","metadata":{"papermill":{"duration":0.018663,"end_time":"2024-04-07T06:22:13.334973","exception":false,"start_time":"2024-04-07T06:22:13.316310","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:16:51.262337Z","iopub.execute_input":"2024-04-23T19:16:51.262695Z","iopub.status.idle":"2024-04-23T19:16:51.269296Z","shell.execute_reply.started":"2024-04-23T19:16:51.262665Z","shell.execute_reply":"2024-04-23T19:16:51.268223Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cell complete: Hyperparameters defined\n","output_type":"stream"}]},{"cell_type":"code","source":"class NeuralNetClassifier(nn.Module):\n    def __init__(self, r_size, v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(NeuralNetClassifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.hidden_layer1 = nn.Linear(r_size * v_size, hidden_size)\n        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n        self.hidden_layer3 = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout) #>> 0.2 seems to work OK\n        #self.softmax = nn.Softmax(dim=1)  # Softmax with dim=1 for class probabilities\n\n    def forward(self, x):\n        x = self.hidden_layer1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.hidden_layer2(x)\n        x = self.relu(x)\n        x = self.dropout(x) #>> dropout rate 0.1 may result in underfitting?\n        #x = self.softmax(x)  # Apply softmax after the output layer\n        x = self.hidden_layer3(x)\n        return x\n    \nclassifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1).to(device)\nshowC(f'{classifier} defined')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:16:51.270446Z","iopub.execute_input":"2024-04-23T19:16:51.270771Z","iopub.status.idle":"2024-04-23T19:16:51.540663Z","shell.execute_reply.started":"2024-04-23T19:16:51.270744Z","shell.execute_reply":"2024-04-23T19:16:51.539648Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cell complete: NeuralNetClassifier(\n  (hidden_layer1): Linear(in_features=30000, out_features=300, bias=True)\n  (hidden_layer2): Linear(in_features=300, out_features=300, bias=True)\n  (hidden_layer3): Linear(in_features=300, out_features=6, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n) defined\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nclass Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\n\nshowC(f'{Classifier} defined')\n'''","metadata":{"papermill":{"duration":0.026078,"end_time":"2024-04-07T06:22:13.371502","exception":false,"start_time":"2024-04-07T06:22:13.345424","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:16:51.542024Z","iopub.execute_input":"2024-04-23T19:16:51.542781Z","iopub.status.idle":"2024-04-23T19:16:51.550827Z","shell.execute_reply.started":"2024-04-23T19:16:51.542740Z","shell.execute_reply":"2024-04-23T19:16:51.549756Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"\\nclass Classifier(nn.Module):\\n    def __init__(self, r_size,v_size, num_classes):\\n        # r_size is the number of tokens in a review, 100.\\n        # v_size is the number of values in an embedding vector, 300.\\n        super(Classifier, self).__init__()\\n        \\n        # The input to fc will be a 2D tensor with with n rows and\\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\\n        # with n rows and num_classes columns.\\n        self.fc = nn.Linear(r_size * v_size, num_classes)\\n\\n    def forward(self, x1):\\n        # Pass input through the linear layer\\n        return self.fc(x1)\\n\\n# Create the classifier\\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\\n\\nshowC(f'{Classifier} defined')\\n\""},"metadata":{}}]},{"cell_type":"code","source":"class EarlyStopping():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n    Credit:\n    https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n    \"\"\"\n    def __init__(self, patience=5, min_delta=0):\n        \"\"\"\n        :param patience: how many epochs to wait before stopping when loss is\n               not improving\n        :param min_delta: minimum difference between new loss and old loss for\n               new loss to be considered as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            # reset counter if validation loss improves\n            self.counter = 0\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            #printd(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                printv(f'Early stopping: counter={self.counter}; patience={self.patience}')\n                self.early_stop = True\nearly_stopping = EarlyStopping(patience=20) #patience=30, min_delta=.01) \nshowC(f'{EarlyStopping} object defined')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T19:16:51.551966Z","iopub.execute_input":"2024-04-23T19:16:51.552301Z","iopub.status.idle":"2024-04-23T19:16:51.566323Z","shell.execute_reply.started":"2024-04-23T19:16:51.552262Z","shell.execute_reply":"2024-04-23T19:16:51.565242Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Cell complete: <class '__main__.EarlyStopping'> object defined\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss() # nn.CategoricalCrossentropy() #nn.Softmax() \n\n# Define SGD optimizer\n# Is Adam better? Didn't seem so based on 4/23 heuristics\noptimizer = optim.SGD(classifier.parameters(), lr=learning_rate)\n\nDEV = True\n# Training loop (adjust this to match your data and DataLoader)\nlosses = {} #<< track losses\nfor epoch in range(epochs):\n    for inputs, targets in train_loader :  # Assuming you have a DataLoader\n        # for batch_data in train_loader:  # Assuming you have a DataLoader\n        # inputs, targets = batch_data  # Assuming your DataLoader provides input data and targets    \n    \n        #printd(f'inputs shape: {inputs.shape}')\n        #printd(f'targets shape: {targets.shape}')\n        #printd(f'targets: {targets}')\n\n        optimizer.zero_grad()\n    \n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        inputs.to(device)\n        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n        #inputs = torch.reshape(inputs, (32,30000))\n        \n        #printd(f'Reshaped inputs: {inputs.shape}')\n        \n        outputs = classifier(inputs).to(device)\n        #printd(f'outputs shape {outputs.shape}')\n        \n        # output is a 32 x 6 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        loss = criterion(outputs.to(device), targets.to(device))\n        # print(f'loss.item: {loss.item()}')\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        \n       \n    \n    losses[loss.item()] = epoch+1\n    early_stopping(loss)    \n    if early_stopping.early_stop:\n       printv(f'Stopping early at epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n       break    \n        \n    if epoch % 50 == 0:\n        printv(f'Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n    \n \nif VERBOSE:\n    printv(f'Last loss: Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n    smallest_losses = sorted(list(losses.keys()))\n    printv('Smallest losses')\n    for idx in range(3):\n        l = smallest_losses[idx]\n        printv(f'    Loss: {l}, epoch = {losses[l]}')\nshowC(f'training complete')","metadata":{"papermill":{"duration":1344.87285,"end_time":"2024-04-07T06:44:38.254791","exception":false,"start_time":"2024-04-07T06:22:13.381941","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:16:51.567481Z","iopub.execute_input":"2024-04-23T19:16:51.568261Z","iopub.status.idle":"2024-04-23T19:17:54.041076Z","shell.execute_reply.started":"2024-04-23T19:16:51.568220Z","shell.execute_reply":"2024-04-23T19:17:54.040070Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"VERBOSE: Epoch [1/200] Loss: 1.5726490020751953\nVERBOSE: Epoch [51/200] Loss: 2.1762948036193848\nVERBOSE: Early stopping: counter=20; patience=20\nVERBOSE: Stopping early at epoch [99/200] Loss: 0.33466479182243347\nVERBOSE: Last loss: Epoch [99/200] Loss: 0.33466479182243347\nVERBOSE: Smallest losses\nVERBOSE:     Loss: 0.2760360538959503, epoch = 79\nVERBOSE:     Loss: 0.27930957078933716, epoch = 84\nVERBOSE:     Loss: 0.29202067852020264, epoch = 85\nCell complete: training complete\n","output_type":"stream"}]},{"cell_type":"code","source":"# Put model in evaluation mode\nclassifier.eval() \n\n# Tracking variables\npredictions = []\nactuals = []\n\n\n# Evaluate on validation set\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n\n        predictions.extend(predicted.tolist())\n        actuals.extend(targets.tolist())\n\n\n# Print predicted and actual values for all samples\n#>> print(\"Predicted | Actual\")\n#>>for pred, actual in zip(predictions, actuals):\n#>>    pass #printd(f\"{pred} | {actual}\")\n\n# Calculate validation accuracy\n#>> 4/12/24 Maybe it would help to see how close we came in each category?\n#>> For example, for category 5 predictions, show the actual results in each \n#>> category. And where's there's a large disrepancy, show  the reviews.\nr_by_category = [0,0,0,0,0]\nr = list('12345')\n\nfor idx in range(5):\n    r[idx] = r_by_category[:]\n       \nfor p,a in zip(predictions, actuals):\n    r[p-1][a-1] += 1 # Record the actual results for each category prediction\n\nnum_correct = 0\nfor idx in range (5):\n    printv(f'Categrory {idx+1} predictions actual results: ' +\\\n           f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n    num_correct += r[idx][idx]\n# num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \nval_accuracy = num_correct / len(predictions)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"papermill":{"duration":0.381905,"end_time":"2024-04-07T06:44:38.725263","exception":false,"start_time":"2024-04-07T06:44:38.343358","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-23T19:17:54.042322Z","iopub.execute_input":"2024-04-23T19:17:54.042878Z","iopub.status.idle":"2024-04-23T19:17:54.168057Z","shell.execute_reply.started":"2024-04-23T19:17:54.042851Z","shell.execute_reply":"2024-04-23T19:17:54.167052Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"VERBOSE: Categrory 1 predictions actual results: 1. 6; 2. 1; 3. 1; 4. 0; 5. 0\nVERBOSE: Categrory 2 predictions actual results: 1. 93; 2. 96; 3. 68; 4. 42; 5. 35\nVERBOSE: Categrory 3 predictions actual results: 1. 78; 2. 79; 3. 99; 4. 110; 5. 94\nVERBOSE: Categrory 4 predictions actual results: 1. 9; 2. 13; 3. 25; 4. 45; 5. 55\nVERBOSE: Categrory 5 predictions actual results: 1. 3; 2. 2; 3. 4; 4. 17; 5. 25\nValidation Accuracy: 0.27\n","output_type":"stream"}]}]}