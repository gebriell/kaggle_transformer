{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae36e3c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-29T09:09:11.741701Z",
     "iopub.status.busy": "2024-04-29T09:09:11.741335Z",
     "iopub.status.idle": "2024-04-29T09:09:35.938458Z",
     "shell.execute_reply": "2024-04-29T09:09:35.937577Z"
    },
    "papermill": {
     "duration": 24.205789,
     "end_time": "2024-04-29T09:09:35.940521",
     "exception": false,
     "start_time": "2024-04-29T09:09:11.734732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data file: /kaggle/input/80000/extracted_data.hdf5\n",
      "CUDA is available!\n",
      "GPU 0: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# For viewing and manipulating data\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors # >> alternative to gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Getting particular functions from these libraries \n",
    "from torch import Tensor\n",
    "from sklearn.utils import resample\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Using the NLTK to tokenize the text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "dataset_file_name = ''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames:\n",
    "        file_name = os.path.join(dirname, filename)\n",
    "        if file_name.endswith('hdf5'):\n",
    "            dataset_file_name = file_name\n",
    "        else:\n",
    "            print(f'Found unexpected file: {file_name}')\n",
    "                \n",
    "print(f'Preprocessed data file: {dataset_file_name}')\n",
    "\n",
    "# Checks if a CUDA enabled GPU is available and prints out its information\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    accelerator = True\n",
    "\n",
    "else:\n",
    "    accelerator = False\n",
    "    print(\"CUDA is not available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "VERBOSE = True\n",
    "def printv(text):\n",
    "    if VERBOSE: print('VERBOSE:', text)\n",
    "    return\n",
    "\n",
    "def showV(text):\n",
    "    '''unconditional verbose output'''\n",
    "    print('VERBOSE:', text)\n",
    "    return\n",
    "\n",
    "DEV = False\n",
    "def printd(text):\n",
    "    if DEV: print('DEV:', text)\n",
    "    return\n",
    "\n",
    "def showD(text):\n",
    "    '''unconditional DEV output'''\n",
    "    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n",
    "    return\n",
    "\n",
    "showCellCompletion = True  #<< 4/12/24 set default to True\n",
    "def showC(text):\n",
    "    if showCellCompletion:\n",
    "        print('Cell complete:', text)\n",
    "    return\n",
    "\n",
    "import subprocess\n",
    "showNv = True\n",
    "accelerator = True\n",
    "\n",
    "def printNv():\n",
    "    if not showNv or not accelerator: return\n",
    "    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(mem_usage.stdout.decode('utf-8'))\n",
    "\n",
    "showMemoryAllocation = True\n",
    "def printM():\n",
    "    if not showMemoryAllocation: return\n",
    "    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87a9a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:09:35.952041Z",
     "iopub.status.busy": "2024-04-29T09:09:35.951527Z",
     "iopub.status.idle": "2024-04-29T09:10:33.201987Z",
     "shell.execute_reply": "2024-04-29T09:10:33.201031Z"
    },
    "papermill": {
     "duration": 57.263393,
     "end_time": "2024-04-29T09:10:33.209152",
     "exception": false,
     "start_time": "2024-04-29T09:09:35.945759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80000, 100, 300])\n",
      "torch.Size([80000])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = '/kaggle/input/80000/extracted_data.hdf5'\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    # Access the datasets within the HDF5 file\n",
    "    text_reviews_dataset = hf['text_reviews']\n",
    "    ratings_dataset = hf['ratings']\n",
    "\n",
    "    # Convert the datasets to PyTorch tensors\n",
    "    text_reviews = torch.from_numpy(text_reviews_dataset[:])\n",
    "    ratings = torch.from_numpy(ratings_dataset[:])\n",
    "\n",
    "# Use the loaded tensors as needed\n",
    "print(text_reviews.shape)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1117efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:33.220697Z",
     "iopub.status.busy": "2024-04-29T09:10:33.220399Z",
     "iopub.status.idle": "2024-04-29T09:10:33.224194Z",
     "shell.execute_reply": "2024-04-29T09:10:33.223347Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.011637,
     "end_time": "2024-04-29T09:10:33.226088",
     "exception": false,
     "start_time": "2024-04-29T09:10:33.214451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(dataset_file_name, 'rb') as dataset_file:\n",
    "#     dataset = pickle.load(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f71939f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:33.237865Z",
     "iopub.status.busy": "2024-04-29T09:10:33.237194Z",
     "iopub.status.idle": "2024-04-29T09:10:45.603458Z",
     "shell.execute_reply": "2024-04-29T09:10:45.602289Z"
    },
    "papermill": {
     "duration": 12.374561,
     "end_time": "2024-04-29T09:10:45.605655",
     "exception": false,
     "start_time": "2024-04-29T09:10:33.231094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (reviews): torch.Size([80000, 100, 300])\n",
      "Shape of y (ratings): torch.Size([80000])\n",
      "Training Set:\n",
      "Number of ratings: 64000\n",
      "Number of reviews: 64000\n",
      "Number of reviews per rating:\n",
      "Rating 1: 12800 reviews\n",
      "Rating 2: 12800 reviews\n",
      "Rating 3: 12800 reviews\n",
      "Rating 4: 12800 reviews\n",
      "Rating 5: 12800 reviews\n",
      "\n",
      "Validation Set:\n",
      "Number of ratings: 16000\n",
      "Number of reviews: 16000\n",
      "Number of reviews per rating:\n",
      "Rating 1: 3200 reviews\n",
      "Rating 2: 3200 reviews\n",
      "Rating 3: 3200 reviews\n",
      "Rating 4: 3200 reviews\n",
      "Rating 5: 3200 reviews\n",
      "VERBOSE: The amount of data we have to train with is 64000 reviews\n",
      "VERBOSE: The amount of data we have to validate with is 16000 reviews\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(text_reviews, ratings)\n",
    "\n",
    "# Assuming your data is stored in 'dataset' as a PyTorch dataset object\n",
    "X = dataset.tensors[0]  # Assuming the reviews are stored at index 1 in the dataset tensors\n",
    "y = dataset.tensors[1]  # Assuming the ratings are stored at index 0 in the dataset tensors\n",
    "\n",
    "print(\"Shape of X (reviews):\", X.shape)\n",
    "print(\"Shape of y (ratings):\", y.shape)\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "y_train = []\n",
    "y_val = []\n",
    "\n",
    "# Perform stratified splitting for each rating class\n",
    "for rating in torch.unique(y):\n",
    "    X_rating = X[y == rating]\n",
    "    y_rating = y[y == rating]\n",
    "\n",
    "    X_train_rating, X_val_rating, y_train_rating, y_val_rating = train_test_split(\n",
    "        X_rating, y_rating, test_size = 0.2, random_state = 42, stratify = y_rating)\n",
    "\n",
    "    X_train.append(X_train_rating)\n",
    "    X_val.append(X_val_rating)\n",
    "    y_train.append(y_train_rating)\n",
    "    y_val.append(y_val_rating)\n",
    "\n",
    "# Combine the split data from all rating classes\n",
    "X_train = torch.cat(X_train)\n",
    "X_val = torch.cat(X_val)\n",
    "y_train = torch.cat(y_train)\n",
    "y_val = torch.cat(y_val)\n",
    "\n",
    "# Create new datasets using the split data\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(\"Number of ratings:\", len(y_train))\n",
    "print(\"Number of reviews:\", len(X_train))\n",
    "print(\"Number of reviews per rating:\")\n",
    "for rating in torch.unique(y_train):\n",
    "    count = torch.sum(y_train == rating).item()\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(\"Number of ratings:\", len(y_val))\n",
    "print(\"Number of reviews:\", len(X_val))\n",
    "print(\"Number of reviews per rating:\")\n",
    "for rating in torch.unique(y_val):\n",
    "    count = torch.sum(y_val == rating).item()\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "\n",
    "printv(f\"The amount of data we have to train with is {len(train_data)} reviews\") \n",
    "printv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n",
    "\n",
    "# DataLoader for training data\n",
    "train_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n",
    "\n",
    "# DataLoader for validation data\n",
    "val_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b25f3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:45.618925Z",
     "iopub.status.busy": "2024-04-29T09:10:45.618633Z",
     "iopub.status.idle": "2024-04-29T09:10:45.624403Z",
     "shell.execute_reply": "2024-04-29T09:10:45.623485Z"
    },
    "papermill": {
     "duration": 0.014701,
     "end_time": "2024-04-29T09:10:45.626648",
     "exception": false,
     "start_time": "2024-04-29T09:10:45.611947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell complete: Hyperparameters defined\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters for the model\n",
    "d_model = 300  # Should match the embedding dimension of your word embeddings\n",
    "seq_len = 100 #<< 4/13/24 100  # Maximum sequence length\n",
    "dropout = 0.1  # Adjust the dropout if needed\n",
    "\n",
    "num_layers = 10 # depth of our network\n",
    "input_size = d_model  # match the output dim of your ff_net\n",
    "num_classes = 5  # our ratings (1 - 5)\n",
    "hidden_size = 1024 # 2^n\n",
    "\n",
    "eps    = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\n",
    "epochs = 200 #<< 1000\n",
    "learning_rate = 0.001\n",
    "weight_decay  = 0.01\n",
    "\n",
    "showC('Hyperparameters defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e35d717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:45.639188Z",
     "iopub.status.busy": "2024-04-29T09:10:45.638897Z",
     "iopub.status.idle": "2024-04-29T09:10:45.647457Z",
     "shell.execute_reply": "2024-04-29T09:10:45.646517Z"
    },
    "papermill": {
     "duration": 0.017257,
     "end_time": "2024-04-29T09:10:45.649527",
     "exception": false,
     "start_time": "2024-04-29T09:10:45.632270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetClassifier(nn.Module):\n",
    "    def __init__(self, r_size, v_size, num_classes, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout):\n",
    "        super(NeuralNetClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(r_size * v_size, hidden_size))\n",
    "        self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c324098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:45.661989Z",
     "iopub.status.busy": "2024-04-29T09:10:45.661736Z",
     "iopub.status.idle": "2024-04-29T09:10:46.250238Z",
     "shell.execute_reply": "2024-04-29T09:10:46.249372Z"
    },
    "papermill": {
     "duration": 0.597008,
     "end_time": "2024-04-29T09:10:46.252328",
     "exception": false,
     "start_time": "2024-04-29T09:10:45.655320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetClassifier(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30000, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (11): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (13): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (15): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (17): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (19): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=1024, out_features=5, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = NeuralNetClassifier(seq_len, d_model, num_classes, hidden_size, num_layers, dropout)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b339c",
   "metadata": {
    "papermill": {
     "duration": 0.005588,
     "end_time": "2024-04-29T09:10:46.263779",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.258191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# neural net with rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654c3b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:46.276594Z",
     "iopub.status.busy": "2024-04-29T09:10:46.275943Z",
     "iopub.status.idle": "2024-04-29T09:10:46.281033Z",
     "shell.execute_reply": "2024-04-29T09:10:46.280192Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013578,
     "end_time": "2024-04-29T09:10:46.282892",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.269314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RecurrentNeuralNetClassifier(nn.Module):\n",
    "#     def __init__(self, r_size, v_size, num_classes, hidden_size = hidden_size, \n",
    "#                  num_layers = num_layers, dropout = dropout, rnn_hidden_size = 256, rnn_num_layers = 1):\n",
    "#         super(RecurrentNeuralNetClassifier, self).__init__()\n",
    "\n",
    "#         self.rnn = nn.RNN(r_size * v_size, rnn_hidden_size, rnn_num_layers, batch_first=True)\n",
    "        \n",
    "#         self.hidden_layers = nn.ModuleList()\n",
    "#         self.hidden_layers.append(nn.Linear(rnn_hidden_size, hidden_size))\n",
    "#         self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "#         for _ in range(num_layers - 1):\n",
    "#             self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "#             self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "#         self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "#         self.relu = nn.LeakyReLU()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Reshape the input to match the expected shape for RNN\n",
    "#         x = x.view(x.size(0), -1, x.size(-1))\n",
    "        \n",
    "#         # Pass the input through the RNN layer\n",
    "#         x, _ = self.rnn(x)\n",
    "        \n",
    "#         # Take the last output of the RNN\n",
    "#         x = x[:, -1, :]\n",
    "        \n",
    "#         for layer in self.hidden_layers:\n",
    "#             x = layer(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.dropout(x)\n",
    "        \n",
    "#         x = self.output_layer(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2729d60b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:46.295408Z",
     "iopub.status.busy": "2024-04-29T09:10:46.295139Z",
     "iopub.status.idle": "2024-04-29T09:10:46.298990Z",
     "shell.execute_reply": "2024-04-29T09:10:46.298105Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012316,
     "end_time": "2024-04-29T09:10:46.300962",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.288646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier = RecurrentNeuralNetClassifier(seq_len, d_model, num_classes, hidden_size, num_layers, dropout)\n",
    "# classifier = classifier.to(device)\n",
    "\n",
    "# print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62289a",
   "metadata": {
    "papermill": {
     "duration": 0.00542,
     "end_time": "2024-04-29T09:10:46.312144",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.306724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# early neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a7daba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:46.324893Z",
     "iopub.status.busy": "2024-04-29T09:10:46.324629Z",
     "iopub.status.idle": "2024-04-29T09:10:46.333322Z",
     "shell.execute_reply": "2024-04-29T09:10:46.332461Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017482,
     "end_time": "2024-04-29T09:10:46.335270",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.317788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass Classifier(nn.Module):\\n    def __init__(self, r_size,v_size, num_classes):\\n        # r_size is the number of tokens in a review, 100.\\n        # v_size is the number of values in an embedding vector, 300.\\n        super(Classifier, self).__init__()\\n        \\n        # The input to fc will be a 2D tensor with with n rows and\\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\\n        # with n rows and num_classes columns.\\n        self.fc = nn.Linear(r_size * v_size, num_classes)\\n\\n    def forward(self, x1):\\n        # Pass input through the linear layer\\n        return self.fc(x1)\\n\\n# Create the classifier\\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\\n\\nshowC(f'{Classifier} defined')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class NeuralNetClassifier(nn.Module):\n",
    "#     def __init__(self, r_size, v_size, num_classes):\n",
    "#         # r_size is the number of tokens in a review, 100.\n",
    "#         # v_size is the number of values in an embedding vector, 300.\n",
    "#         super(NeuralNetClassifier, self).__init__()\n",
    "        \n",
    "#         # The input to fc will be a 2D tensor with with n rows and\n",
    "#         # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n",
    "#         # with n rows and num_classes columns.\n",
    "#         self.hidden_layer1 = nn.Linear(r_size * v_size, hidden_size)\n",
    "#         self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.hidden_layer3 = nn.Linear(hidden_size, num_classes)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout) #>> 0.2 seems to work OK\n",
    "#         #self.softmax = nn.Softmax(dim=1)  # Softmax with dim=1 for class probabilities\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden_layer1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.hidden_layer2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x) #>> dropout rate 0.1 may result in underfitting?\n",
    "#         #x = self.softmax(x)  # Apply softmax after the output layer\n",
    "#         x = self.hidden_layer3(x)\n",
    "#         return x\n",
    "    \n",
    "# classifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1).to(device)\n",
    "# showC(f'{classifier} defined')\n",
    "\n",
    "'''\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, r_size,v_size, num_classes):\n",
    "        # r_size is the number of tokens in a review, 100.\n",
    "        # v_size is the number of values in an embedding vector, 300.\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # The input to fc will be a 2D tensor with with n rows and\n",
    "        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n",
    "        # with n rows and num_classes columns.\n",
    "        self.fc = nn.Linear(r_size * v_size, num_classes)\n",
    "\n",
    "    def forward(self, x1):\n",
    "        # Pass input through the linear layer\n",
    "        return self.fc(x1)\n",
    "\n",
    "# Create the classifier\n",
    "classifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\n",
    "\n",
    "showC(f'{Classifier} defined')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e425d",
   "metadata": {
    "papermill": {
     "duration": 0.005823,
     "end_time": "2024-04-29T09:10:46.346916",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.341093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# stoppage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e026025d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:46.361170Z",
     "iopub.status.busy": "2024-04-29T09:10:46.360542Z",
     "iopub.status.idle": "2024-04-29T09:10:46.365724Z",
     "shell.execute_reply": "2024-04-29T09:10:46.364801Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01494,
     "end_time": "2024-04-29T09:10:46.367658",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.352718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class EarlyStopping():\n",
    "#     \"\"\"\n",
    "#     Early stopping to stop the training when the loss does not improve after\n",
    "#     certain epochs.\n",
    "#     Credit:\n",
    "#     https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n",
    "#     \"\"\"\n",
    "#     def __init__(self, patience=5, min_delta=0):\n",
    "#         \"\"\"\n",
    "#         :param patience: how many epochs to wait before stopping when loss is\n",
    "#                not improving\n",
    "#         :param min_delta: minimum difference between new loss and old loss for\n",
    "#                new loss to be considered as an improvement\n",
    "#         \"\"\"\n",
    "#         self.patience = patience\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.best_loss = None\n",
    "#         self.early_stop = False\n",
    "        \n",
    "#     def __call__(self, val_loss):\n",
    "#         if self.best_loss == None:\n",
    "#             self.best_loss = val_loss\n",
    "#         elif self.best_loss - val_loss > self.min_delta:\n",
    "#             self.best_loss = val_loss\n",
    "#             # reset counter if validation loss improves\n",
    "#             self.counter = 0\n",
    "#         elif self.best_loss - val_loss < self.min_delta:\n",
    "#             self.counter += 1\n",
    "#             #printd(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "#             if self.counter >= self.patience:\n",
    "#                 printv(f'Early stopping: counter={self.counter}; patience={self.patience}')\n",
    "#                 self.early_stop = True\n",
    "# early_stopping = EarlyStopping(patience=20) #patience=30, min_delta=.01) \n",
    "# showC(f'{EarlyStopping} object defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e5433a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T09:10:46.380632Z",
     "iopub.status.busy": "2024-04-29T09:10:46.380374Z",
     "iopub.status.idle": "2024-04-29T10:36:31.195628Z",
     "shell.execute_reply": "2024-04-29T10:36:31.194608Z"
    },
    "papermill": {
     "duration": 5144.845461,
     "end_time": "2024-04-29T10:36:31.219037",
     "exception": false,
     "start_time": "2024-04-29T09:10:46.373576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBOSE: Epoch [1 / 200] Loss: 1.6648303270339966\n",
      "VERBOSE: Epoch [2 / 200] Loss: 1.7436070442199707\n",
      "VERBOSE: Epoch [3 / 200] Loss: 1.2785296440124512\n",
      "VERBOSE: Epoch [4 / 200] Loss: 1.335050344467163\n",
      "VERBOSE: Epoch [5 / 200] Loss: 1.1342482566833496\n",
      "VERBOSE: Epoch [6 / 200] Loss: 1.284279227256775\n",
      "VERBOSE: Epoch [7 / 200] Loss: 1.4356555938720703\n",
      "VERBOSE: Epoch [8 / 200] Loss: 0.6907355189323425\n",
      "VERBOSE: Epoch [9 / 200] Loss: 1.1115167140960693\n",
      "VERBOSE: Epoch [10 / 200] Loss: 0.6038281321525574\n",
      "VERBOSE: Epoch [11 / 200] Loss: 0.8765528798103333\n",
      "VERBOSE: Epoch [12 / 200] Loss: 0.4267658293247223\n",
      "VERBOSE: Epoch [13 / 200] Loss: 0.331423282623291\n",
      "VERBOSE: Epoch [14 / 200] Loss: 0.23514196276664734\n",
      "VERBOSE: Epoch [15 / 200] Loss: 0.735434889793396\n",
      "VERBOSE: Epoch [16 / 200] Loss: 0.29894667863845825\n",
      "VERBOSE: Epoch [17 / 200] Loss: 0.2077620029449463\n",
      "VERBOSE: Epoch [18 / 200] Loss: 0.3255316913127899\n",
      "VERBOSE: Epoch [19 / 200] Loss: 0.31575867533683777\n",
      "VERBOSE: Epoch [20 / 200] Loss: 0.1762191504240036\n",
      "VERBOSE: Epoch [21 / 200] Loss: 0.08940768986940384\n",
      "VERBOSE: Epoch [22 / 200] Loss: 0.0869622528553009\n",
      "VERBOSE: Epoch [23 / 200] Loss: 0.5222788453102112\n",
      "VERBOSE: Epoch [24 / 200] Loss: 0.18105067312717438\n",
      "VERBOSE: Epoch [25 / 200] Loss: 0.2905430793762207\n",
      "VERBOSE: Epoch [26 / 200] Loss: 0.32855504751205444\n",
      "VERBOSE: Epoch [27 / 200] Loss: 0.10779328644275665\n",
      "VERBOSE: Epoch [28 / 200] Loss: 0.21684953570365906\n",
      "VERBOSE: Epoch [29 / 200] Loss: 0.31847068667411804\n",
      "VERBOSE: Epoch [30 / 200] Loss: 0.059548888355493546\n",
      "VERBOSE: Epoch [31 / 200] Loss: 0.0782417580485344\n",
      "VERBOSE: Epoch [32 / 200] Loss: 0.007137454580515623\n",
      "VERBOSE: Epoch [33 / 200] Loss: 0.19622237980365753\n",
      "VERBOSE: Epoch [34 / 200] Loss: 0.02737554907798767\n",
      "VERBOSE: Epoch [35 / 200] Loss: 0.011220254935324192\n",
      "VERBOSE: Epoch [36 / 200] Loss: 0.15558849275112152\n",
      "VERBOSE: Epoch [37 / 200] Loss: 0.34395042061805725\n",
      "VERBOSE: Epoch [38 / 200] Loss: 0.29812368750572205\n",
      "VERBOSE: Epoch [39 / 200] Loss: 0.015545236878097057\n",
      "VERBOSE: Epoch [40 / 200] Loss: 0.05236726626753807\n",
      "VERBOSE: Epoch [41 / 200] Loss: 0.08418913185596466\n",
      "VERBOSE: Epoch [42 / 200] Loss: 0.1353018581867218\n",
      "VERBOSE: Epoch [43 / 200] Loss: 0.08987919241189957\n",
      "VERBOSE: Epoch [44 / 200] Loss: 0.22913525998592377\n",
      "VERBOSE: Epoch [45 / 200] Loss: 0.29948362708091736\n",
      "VERBOSE: Epoch [46 / 200] Loss: 0.11893785744905472\n",
      "VERBOSE: Epoch [47 / 200] Loss: 0.02069772779941559\n",
      "VERBOSE: Epoch [48 / 200] Loss: 0.018452562391757965\n",
      "VERBOSE: Epoch [49 / 200] Loss: 0.059996072202920914\n",
      "VERBOSE: Epoch [50 / 200] Loss: 0.04753774777054787\n",
      "VERBOSE: Epoch [51 / 200] Loss: 0.06950634717941284\n",
      "VERBOSE: Epoch [52 / 200] Loss: 0.02727200649678707\n",
      "VERBOSE: Epoch [53 / 200] Loss: 0.042799051851034164\n",
      "VERBOSE: Epoch [54 / 200] Loss: 0.015517426654696465\n",
      "VERBOSE: Epoch [55 / 200] Loss: 0.06336451321840286\n",
      "VERBOSE: Epoch [56 / 200] Loss: 0.0245360117405653\n",
      "VERBOSE: Epoch [57 / 200] Loss: 0.03192513436079025\n",
      "VERBOSE: Epoch [58 / 200] Loss: 0.0667278841137886\n",
      "VERBOSE: Epoch [59 / 200] Loss: 0.03137258067727089\n",
      "VERBOSE: Epoch [60 / 200] Loss: 0.03423924744129181\n",
      "VERBOSE: Epoch [61 / 200] Loss: 0.08073154836893082\n",
      "VERBOSE: Epoch [62 / 200] Loss: 0.0293105598539114\n",
      "VERBOSE: Epoch [63 / 200] Loss: 0.03167196735739708\n",
      "VERBOSE: Epoch [64 / 200] Loss: 0.08751710504293442\n",
      "VERBOSE: Epoch [65 / 200] Loss: 0.10803287476301193\n",
      "VERBOSE: Epoch [66 / 200] Loss: 0.10266841948032379\n",
      "VERBOSE: Epoch [67 / 200] Loss: 0.05167968198657036\n",
      "VERBOSE: Epoch [68 / 200] Loss: 0.02919071353971958\n",
      "VERBOSE: Epoch [69 / 200] Loss: 0.07177618145942688\n",
      "VERBOSE: Epoch [70 / 200] Loss: 0.01081493217498064\n",
      "VERBOSE: Epoch [71 / 200] Loss: 0.08873140811920166\n",
      "VERBOSE: Epoch [72 / 200] Loss: 0.011941668577492237\n",
      "VERBOSE: Epoch [73 / 200] Loss: 0.06005564332008362\n",
      "VERBOSE: Epoch [74 / 200] Loss: 0.010955991223454475\n",
      "VERBOSE: Epoch [75 / 200] Loss: 0.16169169545173645\n",
      "VERBOSE: Epoch [76 / 200] Loss: 0.07610941678285599\n",
      "VERBOSE: Epoch [77 / 200] Loss: 0.004167609382420778\n",
      "VERBOSE: Epoch [78 / 200] Loss: 0.015427233651280403\n",
      "VERBOSE: Epoch [79 / 200] Loss: 0.2875223755836487\n",
      "VERBOSE: Epoch [80 / 200] Loss: 0.032235823571681976\n",
      "VERBOSE: Epoch [81 / 200] Loss: 0.13256512582302094\n",
      "VERBOSE: Epoch [82 / 200] Loss: 0.16860727965831757\n",
      "VERBOSE: Epoch [83 / 200] Loss: 0.054724693298339844\n",
      "VERBOSE: Epoch [84 / 200] Loss: 0.026745004579424858\n",
      "VERBOSE: Epoch [85 / 200] Loss: 0.06514133512973785\n",
      "VERBOSE: Epoch [86 / 200] Loss: 0.017807602882385254\n",
      "VERBOSE: Epoch [87 / 200] Loss: 0.024431031197309494\n",
      "VERBOSE: Epoch [88 / 200] Loss: 0.02046441286802292\n",
      "VERBOSE: Epoch [89 / 200] Loss: 0.12756238877773285\n",
      "VERBOSE: Epoch [90 / 200] Loss: 0.002087182132527232\n",
      "VERBOSE: Epoch [91 / 200] Loss: 0.00914683099836111\n",
      "VERBOSE: Epoch [92 / 200] Loss: 0.05259143188595772\n",
      "VERBOSE: Epoch [93 / 200] Loss: 0.03079666569828987\n",
      "VERBOSE: Epoch [94 / 200] Loss: 0.028696313500404358\n",
      "VERBOSE: Epoch [95 / 200] Loss: 0.041284628212451935\n",
      "VERBOSE: Epoch [96 / 200] Loss: 0.007456684485077858\n",
      "VERBOSE: Epoch [97 / 200] Loss: 0.06800047308206558\n",
      "VERBOSE: Epoch [98 / 200] Loss: 0.3002777695655823\n",
      "VERBOSE: Epoch [99 / 200] Loss: 0.07118339091539383\n",
      "VERBOSE: Epoch [100 / 200] Loss: 0.08280277997255325\n",
      "VERBOSE: Epoch [101 / 200] Loss: 0.005145706702023745\n",
      "VERBOSE: Epoch [102 / 200] Loss: 0.09733878821134567\n",
      "VERBOSE: Epoch [103 / 200] Loss: 0.009307640604674816\n",
      "VERBOSE: Epoch [104 / 200] Loss: 0.12255434691905975\n",
      "VERBOSE: Epoch [105 / 200] Loss: 0.11724969744682312\n",
      "VERBOSE: Epoch [106 / 200] Loss: 0.06611190736293793\n",
      "VERBOSE: Epoch [107 / 200] Loss: 0.03405380994081497\n",
      "VERBOSE: Epoch [108 / 200] Loss: 0.006340071093291044\n",
      "VERBOSE: Epoch [109 / 200] Loss: 0.04251469671726227\n",
      "VERBOSE: Epoch [110 / 200] Loss: 0.07209526747465134\n",
      "VERBOSE: Epoch [111 / 200] Loss: 0.003196019446477294\n",
      "VERBOSE: Epoch [112 / 200] Loss: 0.031919367611408234\n",
      "VERBOSE: Epoch [113 / 200] Loss: 0.02793213725090027\n",
      "VERBOSE: Epoch [114 / 200] Loss: 0.05204153433442116\n",
      "VERBOSE: Epoch [115 / 200] Loss: 0.0035620802082121372\n",
      "VERBOSE: Epoch [116 / 200] Loss: 0.015160104259848595\n",
      "VERBOSE: Epoch [117 / 200] Loss: 0.05087849497795105\n",
      "VERBOSE: Epoch [118 / 200] Loss: 0.17744363844394684\n",
      "VERBOSE: Epoch [119 / 200] Loss: 0.07072954624891281\n",
      "VERBOSE: Epoch [120 / 200] Loss: 0.003328244900330901\n",
      "VERBOSE: Epoch [121 / 200] Loss: 0.02533417008817196\n",
      "VERBOSE: Epoch [122 / 200] Loss: 0.03774997964501381\n",
      "VERBOSE: Epoch [123 / 200] Loss: 0.0041717663407325745\n",
      "VERBOSE: Epoch [124 / 200] Loss: 0.39326322078704834\n",
      "VERBOSE: Epoch [125 / 200] Loss: 0.025253312662243843\n",
      "VERBOSE: Epoch [126 / 200] Loss: 0.05516018345952034\n",
      "VERBOSE: Epoch [127 / 200] Loss: 0.10066701471805573\n",
      "VERBOSE: Epoch [128 / 200] Loss: 0.017311127856373787\n",
      "VERBOSE: Epoch [129 / 200] Loss: 0.07672466337680817\n",
      "VERBOSE: Epoch [130 / 200] Loss: 0.1035243570804596\n",
      "VERBOSE: Epoch [131 / 200] Loss: 0.03519531711935997\n",
      "VERBOSE: Epoch [132 / 200] Loss: 0.021535325795412064\n",
      "VERBOSE: Epoch [133 / 200] Loss: 0.00394621305167675\n",
      "VERBOSE: Epoch [134 / 200] Loss: 0.06978479772806168\n",
      "VERBOSE: Epoch [135 / 200] Loss: 0.0968320295214653\n",
      "VERBOSE: Epoch [136 / 200] Loss: 0.10418831557035446\n",
      "VERBOSE: Epoch [137 / 200] Loss: 0.014916310086846352\n",
      "VERBOSE: Epoch [138 / 200] Loss: 0.041005752980709076\n",
      "VERBOSE: Epoch [139 / 200] Loss: 0.023174209520220757\n",
      "VERBOSE: Epoch [140 / 200] Loss: 0.05714552849531174\n",
      "VERBOSE: Epoch [141 / 200] Loss: 0.20334936678409576\n",
      "VERBOSE: Epoch [142 / 200] Loss: 0.011954816989600658\n",
      "VERBOSE: Epoch [143 / 200] Loss: 0.16800622642040253\n",
      "VERBOSE: Epoch [144 / 200] Loss: 0.10237227380275726\n",
      "VERBOSE: Epoch [145 / 200] Loss: 0.08101597428321838\n",
      "VERBOSE: Epoch [146 / 200] Loss: 0.01286977156996727\n",
      "VERBOSE: Epoch [147 / 200] Loss: 0.10428652912378311\n",
      "VERBOSE: Epoch [148 / 200] Loss: 0.046418603509664536\n",
      "VERBOSE: Epoch [149 / 200] Loss: 0.1715669333934784\n",
      "VERBOSE: Epoch [150 / 200] Loss: 0.006447069346904755\n",
      "VERBOSE: Epoch [151 / 200] Loss: 0.007467174436897039\n",
      "VERBOSE: Epoch [152 / 200] Loss: 0.011410157196223736\n",
      "VERBOSE: Epoch [153 / 200] Loss: 0.10830757021903992\n",
      "VERBOSE: Epoch [154 / 200] Loss: 0.015920165926218033\n",
      "VERBOSE: Epoch [155 / 200] Loss: 0.16860632598400116\n",
      "VERBOSE: Epoch [156 / 200] Loss: 0.013677815906703472\n",
      "VERBOSE: Epoch [157 / 200] Loss: 0.12234301120042801\n",
      "VERBOSE: Epoch [158 / 200] Loss: 0.08418383449316025\n",
      "VERBOSE: Epoch [159 / 200] Loss: 0.049379944801330566\n",
      "VERBOSE: Epoch [160 / 200] Loss: 0.09859872609376907\n",
      "VERBOSE: Epoch [161 / 200] Loss: 0.05589943379163742\n",
      "VERBOSE: Epoch [162 / 200] Loss: 0.10925315320491791\n",
      "VERBOSE: Epoch [163 / 200] Loss: 0.13417507708072662\n",
      "VERBOSE: Epoch [164 / 200] Loss: 0.1444491446018219\n",
      "VERBOSE: Epoch [165 / 200] Loss: 0.0590045228600502\n",
      "VERBOSE: Epoch [166 / 200] Loss: 0.010865816846489906\n",
      "VERBOSE: Epoch [167 / 200] Loss: 0.023572277277708054\n",
      "VERBOSE: Epoch [168 / 200] Loss: 0.017869850620627403\n",
      "VERBOSE: Epoch [169 / 200] Loss: 0.022012662142515182\n",
      "VERBOSE: Epoch [170 / 200] Loss: 0.04300684481859207\n",
      "VERBOSE: Epoch [171 / 200] Loss: 0.11846112459897995\n",
      "VERBOSE: Epoch [172 / 200] Loss: 0.036839984357357025\n",
      "VERBOSE: Epoch [173 / 200] Loss: 0.0426742322742939\n",
      "VERBOSE: Epoch [174 / 200] Loss: 0.027973167598247528\n",
      "VERBOSE: Epoch [175 / 200] Loss: 0.0352279394865036\n",
      "VERBOSE: Epoch [176 / 200] Loss: 0.014815321192145348\n",
      "VERBOSE: Epoch [177 / 200] Loss: 0.10939362645149231\n",
      "VERBOSE: Epoch [178 / 200] Loss: 0.011148538440465927\n",
      "VERBOSE: Epoch [179 / 200] Loss: 0.03799557313323021\n",
      "VERBOSE: Epoch [180 / 200] Loss: 0.01257642824202776\n",
      "VERBOSE: Epoch [181 / 200] Loss: 0.24728652834892273\n",
      "VERBOSE: Epoch [182 / 200] Loss: 0.011469860561192036\n",
      "VERBOSE: Epoch [183 / 200] Loss: 0.005740319844335318\n",
      "VERBOSE: Epoch [184 / 200] Loss: 0.03116576559841633\n",
      "VERBOSE: Epoch [185 / 200] Loss: 0.03516928851604462\n",
      "VERBOSE: Epoch [186 / 200] Loss: 0.024953601881861687\n",
      "VERBOSE: Epoch [187 / 200] Loss: 0.09146855026483536\n",
      "VERBOSE: Epoch [188 / 200] Loss: 0.011088031344115734\n",
      "VERBOSE: Epoch [189 / 200] Loss: 0.015384282916784286\n",
      "VERBOSE: Epoch [190 / 200] Loss: 0.013605864718556404\n",
      "VERBOSE: Epoch [191 / 200] Loss: 0.015962908044457436\n",
      "VERBOSE: Epoch [192 / 200] Loss: 0.018147706985473633\n",
      "VERBOSE: Epoch [193 / 200] Loss: 0.14185082912445068\n",
      "VERBOSE: Epoch [194 / 200] Loss: 0.04539535567164421\n",
      "VERBOSE: Epoch [195 / 200] Loss: 0.04850593954324722\n",
      "VERBOSE: Epoch [196 / 200] Loss: 0.04412057250738144\n",
      "VERBOSE: Epoch [197 / 200] Loss: 0.3739381730556488\n",
      "VERBOSE: Epoch [198 / 200] Loss: 0.05539558082818985\n",
      "VERBOSE: Epoch [199 / 200] Loss: 0.17065390944480896\n",
      "VERBOSE: Epoch [200 / 200] Loss: 0.05817534402012825\n",
      "VERBOSE: Last loss: Epoch [200 / 200] Loss: 0.05817534402012825\n",
      "VERBOSE: Smallest losses\n",
      "VERBOSE:     Loss: 0.002087182132527232, epoch = 90\n",
      "VERBOSE:     Loss: 0.003196019446477294, epoch = 111\n",
      "VERBOSE:     Loss: 0.003328244900330901, epoch = 120\n",
      "Cell complete: training complete\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "# Is Adam better? Didn't seem so based on 4/23 heuristics\n",
    "# optimizer = optim.SGD(classifier.parameters(), lr = learning_rate)\n",
    "optimizer = optim.AdamW(classifier.parameters(), \n",
    "                        lr = learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "DEV = True\n",
    "\n",
    "# Training loop\n",
    "losses = {} #<< track losses\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_loader : \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # keep nn.linear happy by combining the last two dimensions of inputs.\n",
    "        inputs.to(device)\n",
    "        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n",
    "        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n",
    "\n",
    "        outputs = classifier(inputs).to(device)\n",
    "\n",
    "        # output is a 32 x 6 tensor of floats,\n",
    "        # targets will be a 32 x 1 tensor of ints\n",
    "        # loss = criterion(outputs.to(device), targets.to(device))\n",
    "        loss = criterion(outputs.to(device), targets.to(device).long())\n",
    "        loss.backward(retain_graph = True)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    losses[loss.item()] = epoch + 1\n",
    "#    early_stopping(loss)\n",
    "\n",
    "#     if early_stopping.early_stop:\n",
    "#         printv(f'Stopping early at epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n",
    "#         break    \n",
    "\n",
    "    # if epoch % 50 == 0:\n",
    "    printv(f'Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n",
    "\n",
    "if VERBOSE:\n",
    "    printv(f'Last loss: Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n",
    "    smallest_losses = sorted(list(losses.keys()))\n",
    "    printv('Smallest losses')\n",
    "    for idx in range(3):\n",
    "        l = smallest_losses[idx]\n",
    "        printv(f'    Loss: {l}, epoch = {losses[l]}')\n",
    "showC(f'training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab04e777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T10:36:31.264282Z",
     "iopub.status.busy": "2024-04-29T10:36:31.263654Z",
     "iopub.status.idle": "2024-04-29T10:36:33.286606Z",
     "shell.execute_reply": "2024-04-29T10:36:33.285628Z"
    },
    "papermill": {
     "duration": 2.048411,
     "end_time": "2024-04-29T10:36:33.288888",
     "exception": false,
     "start_time": "2024-04-29T10:36:31.240477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5069\n",
      "Validation Precision: 0.5129\n",
      "Validation Recall: 0.5069\n",
      "Validation F1-score: 0.5060\n",
      "Confusion Matrix:\n",
      "[[1857  653  178  221  291]\n",
      " [ 552 1729  282  323  314]\n",
      " [ 377  552 1278  571  422]\n",
      " [ 223  369  314 1498  796]\n",
      " [ 205  246  171  830 1748]]\n",
      "The model may have high bias (underfitting). Consider increasing model complexity.\n"
     ]
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "classifier.eval() \n",
    "\n",
    "# Tracking variables\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# Evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n",
    "        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        predictions.extend(predicted.tolist())\n",
    "        actuals.extend(targets.tolist())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(actuals, predictions)\n",
    "precision = precision_score(actuals, predictions, average='weighted')\n",
    "recall = recall_score(actuals, predictions, average='weighted')\n",
    "f1 = f1_score(actuals, predictions, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation F1-score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# # Analyze predictions by category\n",
    "# num_categories = len(cm)\n",
    "# for idx in range(num_categories):\n",
    "#     print(f\"Category {idx+1} predictions actual results:\")\n",
    "#     for j in range(num_categories):\n",
    "#         print(f\"{j+1}. {cm[idx][j]}\")\n",
    "\n",
    "# Assess bias and variance\n",
    "if accuracy < 0.7:  # Adjust the threshold as per your requirements\n",
    "    print(\"The model may have high bias (underfitting). Consider increasing model complexity.\")\n",
    "elif accuracy > 0.95:  # Adjust the threshold as per your requirements\n",
    "    print(\"The model may have high variance (overfitting). Consider regularization techniques.\")\n",
    "else:\n",
    "    print(\"The model seems to have a good balance between bias and variance.\")\n",
    "    \n",
    "# r_by_category = [0,0,0,0,0]\n",
    "# r = list('12345')\n",
    "\n",
    "# for idx in range(5):\n",
    "#     r[idx] = r_by_category[:]\n",
    "\n",
    "# for p,a in zip(predictions, actuals):\n",
    "#     r[p-1][a-1] += 1 # Record the actual results for each category prediction\n",
    "\n",
    "# num_correct = 0\n",
    "# for idx in range (5):\n",
    "#     printv(f'Categrory {idx+1} predictions actual results: ' +\\\n",
    "#            f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n",
    "#     num_correct += r[idx][idx]\n",
    "\n",
    "# # num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \n",
    "# val_accuracy = num_correct / len(predictions)\n",
    "# print(f'Validation Accuracy: {val_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e675a14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-29T10:36:33.334918Z",
     "iopub.status.busy": "2024-04-29T10:36:33.334033Z",
     "iopub.status.idle": "2024-04-29T10:36:33.340916Z",
     "shell.execute_reply": "2024-04-29T10:36:33.338751Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.03209,
     "end_time": "2024-04-29T10:36:33.342902",
     "exception": false,
     "start_time": "2024-04-29T10:36:33.310812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Put model in evaluation mode\n",
    "# classifier.eval() \n",
    "\n",
    "# # Tracking variables\n",
    "# predictions = []\n",
    "# actuals = []\n",
    "\n",
    "# # Evaluate on validation set\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in val_loader:\n",
    "#         inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n",
    "\n",
    "#         outputs = classifier(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         predictions.extend(predicted.tolist())\n",
    "#         actuals.extend(targets.tolist())\n",
    "\n",
    "# # Print predicted and actual values for all samples\n",
    "# #>> print(\"Predicted | Actual\")\n",
    "# #>>for pred, actual in zip(predictions, actuals):\n",
    "# #>>    pass #printd(f\"{pred} | {actual}\")\n",
    "\n",
    "# # Calculate validation accuracy\n",
    "# #>> 4/12/24 Maybe it would help to see how close we came in each category?\n",
    "# #>> For example, for category 5 predictions, show the actual results in each \n",
    "# #>> category. And where's there's a large disrepancy, show  the reviews.\n",
    "# r_by_category = [0,0,0,0,0]\n",
    "# r = list('12345')\n",
    "\n",
    "# for idx in range(5):\n",
    "#     r[idx] = r_by_category[:]\n",
    "\n",
    "# for p,a in zip(predictions, actuals):\n",
    "#     r[p-1][a-1] += 1 # Record the actual results for each category prediction\n",
    "\n",
    "# num_correct = 0\n",
    "# for idx in range (5):\n",
    "#     printv(f'Categrory {idx+1} predictions actual results: ' +\\\n",
    "#            f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n",
    "#     num_correct += r[idx][idx]\n",
    "\n",
    "# # num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \n",
    "# val_accuracy = num_correct / len(predictions)\n",
    "# print(f'Validation Accuracy: {val_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4901680,
     "sourceId": 8259057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5246.978589,
   "end_time": "2024-04-29T10:36:35.993984",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-29T09:09:09.015395",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
