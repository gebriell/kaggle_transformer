{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8182344,"sourceType":"datasetVersion","datasetId":4844516}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2372.211165,"end_time":"2024-04-07T06:44:42.316126","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T06:05:10.104961","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndataset_file_name = ''\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('dataset'):\n            dataset_file_name = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Preprocessed data file: {dataset_file_name}')      \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = False\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n    return\n\nshowCellCompletion = True  #<< 4/12/24 set default to True\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":18.682261,"end_time":"2024-04-07T06:05:31.518053","exception":false,"start_time":"2024-04-07T06:05:12.835792","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n    accelerator = True\n\nelse:\n    accelerator = False\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n    print(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(dataset_file_name, 'rb') as dataset_file:\n    dataset = pickle.load(dataset_file)\n    \ntrain_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\n\n# Random split\ntrain_data, val_data = random_split(dataset, [train_len, val_len])\n\nprintv(f\"The amount of data we have to train with is {len(train_data)} reviews\") \nprintv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"papermill":{"duration":0.025623,"end_time":"2024-04-07T06:22:13.305100","exception":false,"start_time":"2024-04-07T06:22:13.279477","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperParameters for the model\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = 100 #<< 4/13/24 100  # Maximum sequence length\ndropout = 0.1  # Adjust the dropout if needed\n\nnum_layers = 10 # depth of our network\ninput_size = d_model  # match the output dim of your ff_net\nnum_classes = 5  # our ratings (1 - 5)\nhidden_size = 512 # 1024 or d_model\n\neps    = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\nepochs = 200 #<< 1000\nlearning_rate = 0.001\nweight_decay  = 0.01\n\nshowC('Hyperparameters defined')","metadata":{"papermill":{"duration":0.018663,"end_time":"2024-04-07T06:22:13.334973","exception":false,"start_time":"2024-04-07T06:22:13.316310","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeuralNetClassifier(nn.Module):\n    def __init__(self, r_size, v_size, num_classes, hidden_size=512, num_layers=4, dropout=0.2):\n        super(NeuralNetClassifier, self).__init__()\n        \n        self.hidden_layers = nn.ModuleList()\n        self.hidden_layers.append(nn.Linear(r_size * v_size, hidden_size))\n        self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n        for _ in range(num_layers - 1):\n            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n        self.output_layer = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.LeakyReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = layer(x)\n            x = self.relu(x)\n            x = self.dropout(x)\n        \n        x = self.output_layer(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1, hidden_size, num_layers, dropout)\nclassifier = classifier.to(device)\n\nprint(classifier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class RecurrentNeuralNetClassifier(nn.Module):\n#     def __init__(self, r_size, v_size, num_classes, hidden_size=512, num_layers=4, dropout=0.2, rnn_hidden_size=256, rnn_num_layers=1):\n#         super(NeuralNetClassifier, self).__init__()\n\n#         self.rnn = nn.RNN(v_size, rnn_hidden_size, rnn_num_layers, batch_first=True)\n        \n#         self.hidden_layers = nn.ModuleList()\n#         self.hidden_layers.append(nn.Linear(r_size * rnn_hidden_size, hidden_size))\n#         self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n#         for _ in range(num_layers - 1):\n#             self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n#             self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n        \n#         self.output_layer = nn.Linear(hidden_size, num_classes)\n#         self.relu = nn.LeakyReLU()\n#         self.dropout = nn.Dropout(dropout)\n        \n#     def forward(self, x):\n#         # Reshape the input to match the expected shape for RNN\n#         x = x.view(x.size(0), -1, x.size(-1))\n        \n#         # Pass the input through the RNN layer\n#         x, _ = self.rnn(x)\n        \n#         # Take the last output of the RNN\n#         x = x[:, -1, :]\n        \n#         # Flatten the output\n#         x = x.view(x.size(0), -1)\n        \n#         for layer in self.hidden_layers:\n#             x = layer(x)\n#             x = self.relu(x)\n#             x = self.dropout(x)\n        \n#         x = self.output_layer(x)\n#         return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rnn_classifier = RecurrentNeuralNetClassifier(seq_len, d_model, num_classes + 1, hidden_size, num_layers, dropout)\n# rnn_classifier = classifier.to(device)\n\n# print(rnn_classifier)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class NeuralNetClassifier(nn.Module):\n#     def __init__(self, r_size, v_size, num_classes):\n#         # r_size is the number of tokens in a review, 100.\n#         # v_size is the number of values in an embedding vector, 300.\n#         super(NeuralNetClassifier, self).__init__()\n        \n#         # The input to fc will be a 2D tensor with with n rows and\n#         # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n#         # with n rows and num_classes columns.\n#         self.hidden_layer1 = nn.Linear(r_size * v_size, hidden_size)\n#         self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n#         self.hidden_layer3 = nn.Linear(hidden_size, num_classes)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(dropout) #>> 0.2 seems to work OK\n#         #self.softmax = nn.Softmax(dim=1)  # Softmax with dim=1 for class probabilities\n\n#     def forward(self, x):\n#         x = self.hidden_layer1(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         x = self.hidden_layer2(x)\n#         x = self.relu(x)\n#         x = self.dropout(x) #>> dropout rate 0.1 may result in underfitting?\n#         #x = self.softmax(x)  # Apply softmax after the output layer\n#         x = self.hidden_layer3(x)\n#         return x\n    \n# classifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1).to(device)\n# showC(f'{classifier} defined')\n\n'''\nclass Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\n\nshowC(f'{Classifier} defined')\n'''","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class EarlyStopping():\n#     \"\"\"\n#     Early stopping to stop the training when the loss does not improve after\n#     certain epochs.\n#     Credit:\n#     https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n#     \"\"\"\n#     def __init__(self, patience=5, min_delta=0):\n#         \"\"\"\n#         :param patience: how many epochs to wait before stopping when loss is\n#                not improving\n#         :param min_delta: minimum difference between new loss and old loss for\n#                new loss to be considered as an improvement\n#         \"\"\"\n#         self.patience = patience\n#         self.min_delta = min_delta\n#         self.counter = 0\n#         self.best_loss = None\n#         self.early_stop = False\n        \n#     def __call__(self, val_loss):\n#         if self.best_loss == None:\n#             self.best_loss = val_loss\n#         elif self.best_loss - val_loss > self.min_delta:\n#             self.best_loss = val_loss\n#             # reset counter if validation loss improves\n#             self.counter = 0\n#         elif self.best_loss - val_loss < self.min_delta:\n#             self.counter += 1\n#             #printd(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n#             if self.counter >= self.patience:\n#                 printv(f'Early stopping: counter={self.counter}; patience={self.patience}')\n#                 self.early_stop = True\n# early_stopping = EarlyStopping(patience=20) #patience=30, min_delta=.01) \n# showC(f'{EarlyStopping} object defined')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\n# Is Adam better? Didn't seem so based on 4/23 heuristics\n# optimizer = optim.SGD(classifier.parameters(), lr = learning_rate)\noptimizer = optim.AdamW(classifier.parameters(), \n                        lr = learning_rate, weight_decay = weight_decay)\n\nDEV = True\n\n# Training loop\nlosses = {} #<< track losses\nfor epoch in range(epochs):\n    for inputs, targets in train_loader : \n\n        optimizer.zero_grad()\n\n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        inputs.to(device)\n        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n\n        outputs = classifier(inputs).to(device)\n\n        # output is a 32 x 6 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        loss = criterion(outputs.to(device), targets.to(device))\n        loss.backward(retain_graph = True)\n\n        optimizer.step()\n\n    losses[loss.item()] = epoch + 1\n#    early_stopping(loss)\n\n#     if early_stopping.early_stop:\n#         printv(f'Stopping early at epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n#         break    \n\n    if epoch % 50 == 0:\n        printv(f'Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n\nif VERBOSE:\n    printv(f'Last loss: Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n    smallest_losses = sorted(list(losses.keys()))\n    printv('Smallest losses')\n    for idx in range(3):\n        l = smallest_losses[idx]\n        printv(f'    Loss: {l}, epoch = {losses[l]}')\nshowC(f'training complete')","metadata":{"papermill":{"duration":1344.87285,"end_time":"2024-04-07T06:44:38.254791","exception":false,"start_time":"2024-04-07T06:22:13.381941","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put model in evaluation mode\nclassifier.eval() \n\n# Tracking variables\npredictions = []\nactuals = []\n\n# Evaluate on validation set\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n\n        predictions.extend(predicted.tolist())\n        actuals.extend(targets.tolist())\n\n# Print predicted and actual values for all samples\n#>> print(\"Predicted | Actual\")\n#>>for pred, actual in zip(predictions, actuals):\n#>>    pass #printd(f\"{pred} | {actual}\")\n\n# Calculate validation accuracy\n#>> 4/12/24 Maybe it would help to see how close we came in each category?\n#>> For example, for category 5 predictions, show the actual results in each \n#>> category. And where's there's a large disrepancy, show  the reviews.\nr_by_category = [0,0,0,0,0]\nr = list('12345')\n\nfor idx in range(5):\n    r[idx] = r_by_category[:]\n\nfor p,a in zip(predictions, actuals):\n    r[p-1][a-1] += 1 # Record the actual results for each category prediction\n\nnum_correct = 0\nfor idx in range (5):\n    printv(f'Categrory {idx+1} predictions actual results: ' +\\\n           f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n    num_correct += r[idx][idx]\n\n# num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \nval_accuracy = num_correct / len(predictions)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"papermill":{"duration":0.381905,"end_time":"2024-04-07T06:44:38.725263","exception":false,"start_time":"2024-04-07T06:44:38.343358","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}