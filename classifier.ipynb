{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8182344,"sourceType":"datasetVersion","datasetId":4844516},{"sourceId":8232114,"sourceType":"datasetVersion","datasetId":4882065},{"sourceId":8233334,"sourceType":"datasetVersion","datasetId":4882927}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":2372.211165,"end_time":"2024-04-07T06:44:42.316126","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T06:05:10.104961","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport re\nimport math\nimport string\nimport nltk\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndata_filename = ''\ndict_filename = ''\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('dataset-2000-200'):\n            data_filename = file_name\n        elif file_name.endswith('dict-2000-200'):\n            dict_filename = file_name\n        else:\n            print(f'Unidentified file: {file_name}')\n                \nprint(f'Preprocessed data file: {data_filename}; dictionary file: {dict_filename}')      \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = False\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n    return\n\nshowCellCompletion = True  #<< 4/12/24 set default to True\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":18.682261,"end_time":"2024-04-07T06:05:31.518053","exception":false,"start_time":"2024-04-07T06:05:12.835792","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-26T06:28:48.251787Z","iopub.execute_input":"2024-04-26T06:28:48.252130Z","iopub.status.idle":"2024-04-26T06:29:12.762580Z","shell.execute_reply.started":"2024-04-26T06:28:48.252101Z","shell.execute_reply":"2024-04-26T06:29:12.761661Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Unidentified file: /kaggle/input/preprocessed-data-1000-100/preprocessed_dataset-1000-100\nUnidentified file: /kaggle/input/preprocessed-data-1000-100/preprocessed_dict-1000-100\nUnidentified file: /kaggle/input/preprocessed-dataset/preprocessed_dataset\nUnidentified file: /kaggle/input/preprocessed-dataset/preprocessed_dataset.json\nPreprocessed data file: /kaggle/input/preprocessed-2000-per-category-200-max-review-size/preprocessed_dataset-2000-200; dictionary file: /kaggle/input/preprocessed-2000-per-category-200-max-review-size/preprocessed_dict-2000-200\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n    accelerator = True\n\nelse:\n    accelerator = False\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n    print(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:29:26.171301Z","iopub.execute_input":"2024-04-26T06:29:26.171903Z","iopub.status.idle":"2024-04-26T06:29:26.181209Z","shell.execute_reply.started":"2024-04-26T06:29:26.171874Z","shell.execute_reply":"2024-04-26T06:29:26.180199Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CUDA is available!\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nwith open(data_filename, 'rb') as dataset_file:\n    dataset = pickle.load(dataset_file)\n    \nwith open(dict_filename, 'rb') as dict_file:\n    data_params = pickle.load(dict_file)\n \nprint(f'Dataset description: {data_params[\"description\"]}')\nmax_sequence_length  = data_params[\"max_sequence_length\"]\n    \ntrain_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\n\n# Random split\ntrain_data, val_data = random_split(dataset, [train_len, val_len])\n\nprintv(f\"The amount of data we have to train with is {len(train_data)} reviews\") \nprintv(f\"The amount of data we have to validate with is {len(val_data)} reviews\")\n#print(f\"The amount of data we have to validate with is on {train_data.device}\")\n#print(f\"The amount of data we have to validate with is on {val_data.device}\")\n\n# DataLoader for training data\ntrain_loader = DataLoader(train_data, batch_size = 32, shuffle = True)  # Use shuffle for training\n\n# DataLoader for validation data\nval_loader = DataLoader(val_data, batch_size = 32, shuffle = False)  # No need to shuffle for validation","metadata":{"papermill":{"duration":0.025623,"end_time":"2024-04-07T06:22:13.305100","exception":false,"start_time":"2024-04-07T06:22:13.279477","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-26T06:29:32.437646Z","iopub.execute_input":"2024-04-26T06:29:32.438319Z","iopub.status.idle":"2024-04-26T06:29:47.822789Z","shell.execute_reply.started":"2024-04-26T06:29:32.438285Z","shell.execute_reply":"2024-04-26T06:29:47.821863Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Dataset description: 2000 reviews per category; 200 max review size; removes punctuation, symbols, and numeric strings\nVERBOSE: The amount of data we have to train with is 8000 reviews\nVERBOSE: The amount of data we have to validate with is 2000 reviews\n","output_type":"stream"}]},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of your word embeddings\nseq_len = max_sequence_length #<<100 #<< 4/13/24 100  # Maximum sequence length\ndropout = 0.1  # Adjust the dropout if needed\n\neps     = 1e-6 # epsilon value to prevent the standard deviation from becoming zero\nnum_classes = 5  # Replace with your number of classes\nepochs = 200 #<< 1000\nlearning_rate = 0.01\nnum_layers = 6\nhidden_size = d_model\n\ninput_size = d_model  # Adjust this based on the output size of your feed-forward network\n# input_size = len(train_data[0])  # Adjust based on your input size (should match the output size of your model)\nshowC('Hyperparameters defined')","metadata":{"papermill":{"duration":0.018663,"end_time":"2024-04-07T06:22:13.334973","exception":false,"start_time":"2024-04-07T06:22:13.316310","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-26T06:29:57.527290Z","iopub.execute_input":"2024-04-26T06:29:57.527662Z","iopub.status.idle":"2024-04-26T06:29:57.534184Z","shell.execute_reply.started":"2024-04-26T06:29:57.527634Z","shell.execute_reply":"2024-04-26T06:29:57.533148Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cell complete: Hyperparameters defined\n","output_type":"stream"}]},{"cell_type":"code","source":"class NeuralNetClassifier(nn.Module):\n    def __init__(self, r_size, v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(NeuralNetClassifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.hidden_layer1 = nn.Linear(r_size * v_size, hidden_size)\n        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n        self.hidden_layer3 = nn.Linear(hidden_size, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout) #>> 0.2 seems to work OK\n        #self.softmax = nn.Softmax(dim=1)  # Softmax with dim=1 for class probabilities\n\n    def forward(self, x):\n        x = self.hidden_layer1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.hidden_layer2(x)\n        x = self.relu(x)\n        x = self.dropout(x) #>> dropout rate 0.1 may result in underfitting?\n        #x = self.softmax(x)  # Apply softmax after the output layer\n        x = self.hidden_layer3(x)\n        return x\n    \nclassifier = NeuralNetClassifier(seq_len, d_model, num_classes + 1).to(device)\nshowC(f'{classifier} defined')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:30:04.513397Z","iopub.execute_input":"2024-04-26T06:30:04.513756Z","iopub.status.idle":"2024-04-26T06:30:04.730197Z","shell.execute_reply.started":"2024-04-26T06:30:04.513728Z","shell.execute_reply":"2024-04-26T06:30:04.729268Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cell complete: NeuralNetClassifier(\n  (hidden_layer1): Linear(in_features=60000, out_features=300, bias=True)\n  (hidden_layer2): Linear(in_features=300, out_features=300, bias=True)\n  (hidden_layer3): Linear(in_features=300, out_features=6, bias=True)\n  (relu): ReLU()\n  (dropout): Dropout(p=0.1, inplace=False)\n) defined\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nclass Classifier(nn.Module):\n    def __init__(self, r_size,v_size, num_classes):\n        # r_size is the number of tokens in a review, 100.\n        # v_size is the number of values in an embedding vector, 300.\n        super(Classifier, self).__init__()\n        \n        # The input to fc will be a 2D tensor with with n rows and\n        # r_size * v_size columns, where n >= 1; and the output will be a 2D tensor\n        # with n rows and num_classes columns.\n        self.fc = nn.Linear(r_size * v_size, num_classes)\n\n    def forward(self, x1):\n        # Pass input through the linear layer\n        return self.fc(x1)\n\n# Create the classifier\nclassifier = Classifier(seq_len, d_model, num_classes + 1).to(device)\n\nshowC(f'{Classifier} defined')\n'''","metadata":{"papermill":{"duration":0.026078,"end_time":"2024-04-07T06:22:13.371502","exception":false,"start_time":"2024-04-07T06:22:13.345424","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n    Credit:\n    https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/\n    \"\"\"\n    def __init__(self, patience=5, min_delta=0):\n        \"\"\"\n        :param patience: how many epochs to wait before stopping when loss is\n               not improving\n        :param min_delta: minimum difference between new loss and old loss for\n               new loss to be considered as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            # reset counter if validation loss improves\n            self.counter = 0\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            #printd(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                printv(f'Early stopping: counter={self.counter}; patience={self.patience}')\n                self.early_stop = True\nearly_stopping = EarlyStopping(patience=16) #patience=30, min_delta=.01) \nshowC(f'{EarlyStopping} object defined')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:30:14.747564Z","iopub.execute_input":"2024-04-26T06:30:14.747947Z","iopub.status.idle":"2024-04-26T06:30:14.757837Z","shell.execute_reply.started":"2024-04-26T06:30:14.747917Z","shell.execute_reply":"2024-04-26T06:30:14.756855Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Cell complete: <class '__main__.EarlyStopping'> object defined\n","output_type":"stream"}]},{"cell_type":"code","source":"def checkpoint(model, filename):\n    torch.save(model.state_dict(), filename)\n    \ndef restore_from_checkpoint(model, filename):\n    model.load_state_dict(torch.load(filename))\n\nbest_epoch = -1\nlowest_loss = 100\n\ndef set_best(model,loss, epoch,filename): \n    global best_epoch\n    global lowest_loss\n    if loss < lowest_loss:\n        lowest_loss = loss\n        best_epoch = epoch\n        checkpoint(model,filename)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T06:30:21.747601Z","iopub.execute_input":"2024-04-26T06:30:21.747987Z","iopub.status.idle":"2024-04-26T06:30:21.754422Z","shell.execute_reply.started":"2024-04-26T06:30:21.747956Z","shell.execute_reply":"2024-04-26T06:30:21.753252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define Cross-Entropy loss\ncriterion = nn.CrossEntropyLoss() # nn.CategoricalCrossentropy() #nn.Softmax() \n\n# Define SGD optimizer\n# Is Adam better? Didn't seem so based on 4/23 heuristics\noptimizer = optim.SGD(classifier.parameters(), lr=learning_rate)\n\nDEV = True\n# Training loop (adjust this to match your data and DataLoader)\nlosses = {} #<< track losses\nfor epoch in range(epochs):\n    for inputs, targets in train_loader :  # Assuming you have a DataLoader\n        # for batch_data in train_loader:  # Assuming you have a DataLoader\n        # inputs, targets = batch_data  # Assuming your DataLoader provides input data and targets    \n    \n        #printd(f'inputs shape: {inputs.shape}')\n        #printd(f'targets shape: {targets.shape}')\n        #printd(f'targets: {targets}')\n\n        optimizer.zero_grad()\n    \n        # keep nn.linear happy by combining the last two dimensions of inputs.\n        inputs.to(device)\n        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n        #inputs = torch.reshape(inputs, (32,30000))\n        \n        #printd(f'Reshaped inputs: {inputs.shape}')\n        \n        outputs = classifier(inputs).to(device)\n        #printd(f'outputs shape {outputs.shape}')\n        \n        # output is a 32 x 6 tensor of floats,\n        # targets will be a 32 x 1 tensor of ints\n        loss = criterion(outputs.to(device), targets.to(device))\n        # print(f'loss.item: {loss.item()}')\n        loss.backward(retain_graph=True)\n        optimizer.step()\n        \n    set_best(classifier,loss.item(), epoch,'best_model.pth')\n    losses[loss.item()] = epoch+1\n    early_stopping(loss)    \n    if early_stopping.early_stop:\n       printv(f'Stopping early at epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n       break    \n        \n\n    if epoch % 50 == 0:\n        printv(f'Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n        \nrestore_from_checkpoint(classifier, 'best_model.pth')\nprint(f'Using classifer trained at epoch {best_epoch}, loss = {lowest_loss} ')    \n\nif VERBOSE:\n    printv(f'Last loss: Epoch [{epoch+1}/{epochs}] Loss: {loss.item()}')\n    smallest_losses = sorted(list(losses.keys()))\n    printv('Smallest losses')\n    for idx in range(5):\n        l = smallest_losses[idx]\n        printv(f'    Loss: {l}, epoch = {losses[l]}')\nshowC(f'training complete')","metadata":{"papermill":{"duration":1344.87285,"end_time":"2024-04-07T06:44:38.254791","exception":false,"start_time":"2024-04-07T06:22:13.381941","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-26T06:30:30.017315Z","iopub.execute_input":"2024-04-26T06:30:30.018159Z","iopub.status.idle":"2024-04-26T06:31:25.992819Z","shell.execute_reply.started":"2024-04-26T06:30:30.018126Z","shell.execute_reply":"2024-04-26T06:31:25.991859Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"VERBOSE: Epoch [1/200] Loss: 1.6922454833984375\nVERBOSE: Epoch [51/200] Loss: 0.009921971708536148\nVERBOSE: Epoch [101/200] Loss: 0.002677323529496789\nVERBOSE: Early stopping: counter=16; patience=16\nVERBOSE: Stopping early at epoch [103/200] Loss: 0.003515572752803564\nUsing classifer trained at epoch 86, loss = 0.001787966233678162 \nVERBOSE: Last loss: Epoch [103/200] Loss: 0.003515572752803564\nVERBOSE: Smallest losses\nVERBOSE:     Loss: 0.001787966233678162, epoch = 87\nVERBOSE:     Loss: 0.0018257200717926025, epoch = 102\nVERBOSE:     Loss: 0.0018786011496558785, epoch = 94\nVERBOSE:     Loss: 0.002018197439610958, epoch = 95\nVERBOSE:     Loss: 0.0021247644908726215, epoch = 71\nCell complete: training complete\n","output_type":"stream"}]},{"cell_type":"code","source":"# Put model in evaluation mode\nclassifier.eval() \n\n# Tracking variables\npredictions = []\nactuals = []\n\n\n# Evaluate on validation set\nwith torch.no_grad():\n    for inputs, targets in val_loader:\n        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n\n        predictions.extend(predicted.tolist())\n        actuals.extend(targets.tolist())\n\n\n# Print predicted and actual values for all samples\n#>> print(\"Predicted | Actual\")\n#>>for pred, actual in zip(predictions, actuals):\n#>>    pass #printd(f\"{pred} | {actual}\")\n\n# Calculate validation accuracy\n#>> 4/12/24 Maybe it would help to see how close we came in each category?\n#>> For example, for category 5 predictions, show the actual results in each \n#>> category. And where's there's a large disrepancy, show  the reviews.\nr_by_category = [0,0,0,0,0]\nr = list('12345')\n\nfor idx in range(5):\n    r[idx] = r_by_category[:]\n       \nfor p,a in zip(predictions, actuals):\n    r[p-1][a-1] += 1 # Record the actual results for each category prediction\n\nDEV = False\nnum_correct = 0\nfor idx in range (5):\n    printv(f'Categrory {idx+1} predictions actual results: ' +\\\n           f'1. {r[idx][0]}; 2. {r[idx][1]}; 3. {r[idx][2]}; 4. {r[idx][3]}; 5. {r[idx][4]}')\n    total_category = r[idx][0] + r[idx][1] + r[idx][2] + r[idx][3] + r[idx][4]\n    printd(f'{total_category} in category {idx+1}')\n\n    num_correct += r[idx][idx]\n# num_correct = sum([p == a for p, a in zip(predictions, actuals)]) \nval_accuracy = num_correct / len(predictions)\nprint(f'Validation Accuracy: {val_accuracy:.2f}')","metadata":{"papermill":{"duration":0.381905,"end_time":"2024-04-07T06:44:38.725263","exception":false,"start_time":"2024-04-07T06:44:38.343358","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-26T06:31:51.046299Z","iopub.execute_input":"2024-04-26T06:31:51.046923Z","iopub.status.idle":"2024-04-26T06:31:51.179465Z","shell.execute_reply.started":"2024-04-26T06:31:51.046879Z","shell.execute_reply":"2024-04-26T06:31:51.178496Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"VERBOSE: Categrory 1 predictions actual results: 1. 177; 2. 94; 3. 60; 4. 39; 5. 51\nVERBOSE: Categrory 2 predictions actual results: 1. 87; 2. 129; 3. 75; 4. 32; 5. 31\nVERBOSE: Categrory 3 predictions actual results: 1. 64; 2. 72; 3. 116; 4. 94; 5. 58\nVERBOSE: Categrory 4 predictions actual results: 1. 29; 2. 60; 3. 98; 4. 144; 5. 103\nVERBOSE: Categrory 5 predictions actual results: 1. 39; 2. 37; 3. 40; 4. 87; 5. 184\nValidation Accuracy: 0.38\n","output_type":"stream"}]}]}