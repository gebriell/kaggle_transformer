{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":2415872,"sourceType":"datasetVersion","datasetId":1461623}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\n\nimport re\nimport math\nimport torch\nimport spacy\nimport string\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries \nfrom scipy import stats\nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n\n# Using the NLTK to tokenize the text\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n\nelse:\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nreviews_file = ''\nw2v_file = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('.csv'): \n            reviews_file = file_name\n        elif file_name.endswith('.bin') or ('.gz'):\n            w2v_file = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n                \nprint(f'Amazon reviews file: {reviews_file}')\nprint(f'Google news word to vec file: {w2v_file}')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = False\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('VERBOSE:', text)\n    return\n\nshowCellCompletion = False\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = False\naccelerator = False\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip3 install manim\n\nfrom manim import *\n\nclass AttentionScoresPlot(Scene):\n    def construct(self):\n        scores = [0.2, 0.4, 0.6, 0.8, 0.5]\n        labels = ['Score 1', 'Score 2', 'Score 3', 'Score 4', 'Score 5']\n        \n        chart = BarChart(values=scores, bar_names=labels, y_range=[0, 1, 0.2], y_length=5, x_length=10)\n        self.play(Create(chart))\n        self.wait()\n        \n!manim -qm AttentionScoresPlot\n\nfrom IPython.display import Video\nVideo('media/videos/scene/480p15/AttentionScoresPlot.mp4', embed=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data from CSV\n#>>  3/13 needed to change the path as below\n# path ='/kaggle/input/Reviews.csv'#\"/kaggle/input/amazon-product-reviews/Reviews.csv\"\ndata = pd.read_csv(reviews_file) # Use pandas to analyze data\nshowD('Amazon reviews loaded into Panda')\n\n# print number of rows in our ratings column\nprintv(f'Number of reviews: {len(data[\"Score\"])}')\nprintv(f'Column names -\\n {data.columns}\\n') \nprintv(f'First five rows -\\n{data.head()}')\n\n# Check for empty entries or missing data in each column\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        print(f\"Column '{column}' has empty entries or missing data.\")\n    else:\n        print(f\"Column '{column}' has no empty entries or missing data.\")\n\n# Get count of ratings \nrating_counts = data['Score'].value_counts()\n\n# Sort counts by index ascending\nrating_counts = rating_counts.sort_index()  \n\n# Create bar plot\nax = rating_counts.plot(kind = 'bar')\n\nax.set_title(\"Ratings Distribution\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Occurrences\")\n\n# Fix x-axis tick labels\nax.set_xticklabels(ax.get_xticklabels(), rotation = 0) \n\nfor rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show() #<< show the rating in each of the 5 categories\n\nbalanced_data_size = 100 #25000<< number of reviews in each rating category, tailored for CPU capacity\n\n# Specify the column for sorting and balancing\nsort_column = 'Score'  # This is one the rating column\n\n# Sort the data by the rating values\nsorted_data = data.sort_values(by = sort_column)\n\n# Create a balanced dataset with 25,000 samples from each class\n#balanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n=25000))\n\n#>> DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. \n#>> This behavior is deprecated, and in a future version of pandas the grouping \n#>> columns will be excluded from the operation. \n#>> Either pass `include_groups=False` to exclude the groupings or \n#>> explicitly select the grouping columns after groupby to silence this warning.\n#\nbalanced_data = sorted_data.groupby(sort_column).apply(lambda x: x.sample(n = balanced_data_size))\n\n#>> Does this mean to reset the row numbers?? ##Columns Numbers\nbalanced_data.reset_index(drop = True, inplace = True)\n\nprintv(f\"The number of reviews equally distributed across all ratings is {len(balanced_data['Score'])}\")\n\n# Get count of ratings\nrating_counts = balanced_data['Score'].value_counts()\n\n# Create bar plot\nax = rating_counts.plot(kind='bar')\n\nax.set_title(\"Ratings Distribution After Balancing\")\nax.set_xlabel(\"Rating\")\nax.set_ylabel(\"Number of Samples\")\n\n# Fix x-axis ticks  \nax.set_xticklabels(ax.get_xticklabels(), rotation = 0)\n\n# Print number of reviews per rating\n\nif DEV:\n    for rating, count in rating_counts.items():\n        print(f\"{count:,} samples from balanced data with rating {rating}\\n\")\n\nplt.show()\n\n# Load the English tokenizer model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Compile the regular expressions\nhtml_tags = re.compile(\"<.*?>\")\npunct_and_symbols = re.compile(r'[^\\w\\s\\']+')\n\n# custom stop words to remove\ncustom_stop_words = ['a', 'of']\n\ndef tokenizer(text, nlp):\n    \"\"\"\n    Tokenizes a text string and removes stop words.\n    \n    text (str): The text string to tokenize.\n    nlp: The Spacy language model.\n\n    Returns:\n        list: The tokenized text string.\n    \"\"\"\n    text = text.lower()  # Convert text to lowercase\n    text = html_tags.sub(\"\", text)  # Remove HTML tags\n    text = punct_and_symbols.sub(\"\", text)  # Remove punctuation and symbols\n\n    # Tokenize the text\n    doc = nlp(text)\n\n    # Remove stop words and punctuation tokens\n    tokens = [token.text for token in doc if not token.is_punct \n              and token.text not in custom_stop_words]\n\n    return tokens\n\n# Assuming balanced_data is a pandas DataFrame and 'Text' is a column containing the reviews\n# Apply the function to the Text column and store it in a new column\nbalanced_data['CleanedReview'] = balanced_data['Text'].apply(lambda x: tokenizer(x, nlp))\n\n# Show that cell has finished executing\nshowC(f'{tokenizer} defined, and then used to create CleanedReview column')\n\n# Store the Rating column\nrating = balanced_data['Score']  \n\n# Store the CleanedReview column\ntokenized_review = balanced_data['CleanedReview']\nshowD(f'specify the columns that will be used to train the classifier')\n\nprint(balanced_data['Text'][0],'\\n')\nprint(tokenized_review[0], '\\n')\nprint(len(tokenized_review[0]), '\\n')\nprint(tokenized_review.shape)\n\ndef analyze_review_tokens(reviews):\n    token_counts = [len(review) for review in reviews]\n\n    mean = np.mean(token_counts)\n    median = np.median(token_counts)\n    mode = stats.mode(token_counts)\n\n    print(f\"Mean: {mean:.2f}\")\n    print(f\"Median: {median:.2f}\")\n    print(f\"Mode: {mode}\")\n\n    plt.figure(figsize=(8, 6))\n    plt.hist(token_counts, bins=20, edgecolor='black')\n    plt.xlabel('Number of Tokens')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Token Counts per Review')\n    plt.grid(True)\n    plt.show()\n\nanalyze_review_tokens(tokenized_review)\n\n# >> 3/29/24 Took less than a minute to run without the accelerator.\n# Load Word2Vec model\n#w2v = api.load('word2vec-google-news-300')\nw2v = KeyedVectors.load_word2vec_format (w2v_file, binary=True)\n\n# Define the aimum sequence length (adjust as needed)\n#>> Will increasing max_sequence_length impact performance?\nmax_sequence_length = 100\n\nshowD(f'{w2v} can map words onto vectors with 300 dimensions')\n\n\n#>> 3/29/24 Completed in less than 30 seconds using 1347 as max_sequence_length \n\n# Assume we have a list of tokenized review called tokenized_review\n# Each element in tokenized_review is a list of tokens for a single review\n\nlengths = []\nfor review_tokens in tokenized_review:\n    lengths.append(len(review_tokens))\n\nlengths = sorted(lengths)\nlengths = lengths[-1:0:-1]\nshowD(f'Lengths of 100 longest reviews: {lengths[0:100]}')        \n\nmax_sequence_length = 100 #<< 3/29/24 1347 was the longest review length in sample\n\n# Initialize an empty tensor for padded reviews on the GPU\npadded_reviews = torch.zeros((len(tokenized_review), max_sequence_length, 300))\n\nout_words = {}\nwords_in = 0\nwords_out = 0\n\n# padding and embedding conversion loop\nfor i, review_tokens in enumerate(tokenized_review):\n    review_length = min(len(review_tokens), max_sequence_length)\n    for j in range(review_length):\n        word = review_tokens[j]\n        if word in w2v:\n            words_in += 1\n            # Use Word2Vec vector if available\n            padded_reviews[i, j, :] = torch.tensor(w2v[word])\n        else:\n            words_out += 1\n            out_words[word] = out_words.get(word, 0) + 1\n\nprintv(f'{words_in} words found with vector representations, {words_out} without')\nprintv(f'Number of unique words without vector representations: {len(out_words)}')\nprintd('Sample of words without vector representations')\nkwords = list(out_words.keys())\nfor idx in range(0,len(kwords)-1,100):\n    kword = kwords[idx]\n    printd(f'{kword} - {out_words[kword]}')\n# Apply max pooling to aggregate embeddings along the sequence dimension\n# review_embeddings = torch.max(padded_reviews, dim=1)[0]\n\n# Now,review_embeddings contains the aggregated Word2Vec \n# embeddings for each review on the GPU\n\nshowC(f\"Created zero-padded, standard length reviews\")\n\nprint(padded_reviews[1])\n\n#>> padded_reviews are 100 x 300 tensors, zero padded if necessary\n#>> to get the standard lenth\ntext_embeddings_tensors = padded_reviews.to(device)\n\n# Rating labels\nrating_labels_tensors = torch.tensor(rating.values).to(device)\n\n# Dataset\ndataset = TensorDataset(text_embeddings_tensors, rating_labels_tensors)\nshowC(f'{dataset} defined')\n\n# shape of the tensors\nprint(text_embeddings_tensors.shape)\nprint(rating_labels_tensors.shape)\n\n# location of the tensors\nprint(text_embeddings_tensors.device)\nprint(rating_labels_tensors.device)\n\nprint(dataset)\n\n# DataLoader for training data\ndata_loader = DataLoader(dataset, batch_size = 32, shuffle = True)  # Use shuffle for training\n\nprintM() # print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\nprintNv() #!nvidia-smi\nprint('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\nif accelerator and showNv:\n    print(torch.cuda.memory_summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{}},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of our word embeddings\nseq_len = 100  # Maximum sequence length\ninput_size = d_model  # based on the output size of our feed-forward network\n\nnum_layers = 1 # 4 # Number of encoder layers\nh       = 1 # 10   # number of attention head\nd_ffn   = 2048 # dimension of the feedforward layer\n\ndropout = 0.0 # 0.1  # we can adjust the dropout if needed\neps     = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\nlearning_rate = 0.01\n\n\"\"\"\nTo ensure compatibility, it's important to choose the \nnumber of attention heads (h) such that d_model is \nevenly divisible by h in the multi-head attention \nmodule's self.d_k. This allows for a clean distribution \nof the model dimensionality across the attention heads.\n\"\"\"\n\n# d_model / attn.h = 300 / 10 = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch's Scaled Dot Product Attention","metadata":{}},{"cell_type":"code","source":"# FROM TORCH.NN.FUNCTIONAL.SCALED_DOT_PRODUCT_ATTENTION DOCS\n\n\n# Efficient implementation equivalent to the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n    # Efficient implementation equivalent to the following:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @ value","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear projection matrices (takes in 300 dimesions, outputs 300 dimensions)\nW_k = nn.Linear(300, 300).to(device)\nW_q = nn.Linear(300, 300).to(device)\nW_v = nn.Linear(300, 300).to(device)\n\n# Compute key, query, and value tensors\nkey_tensor   = W_k(text_embeddings_tensors) # shape: (batch_size, max_len, emb)\nquery_tensor = W_q(text_embeddings_tensors) # shape: (batch_size, max_len, emb)\nvalue_tensor = W_v(text_embeddings_tensors) # shape: (batch_size, max_len, emb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attn_weights, attn_output = scaled_dot_product_attention(\n    query_tensor, key_tensor, value_tensor)\n\nprint(key_tensor[0])\nprint(query_tensor[0])\nprint(value_tensor[0])\n\nprint(text_embeddings_tensors[0])\nprint(attn_weights[0])\nprint(attn_output[0])\n\n\nprint(\"Attention Weights (shape:\", attn_weights.shape, \")\")\nprint(\"Attention Scores (shape:\", attn_output.shape, \")\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT 4 Turbo\n    \nYou can use hooks to retrieve the attention weights from PyTorch's `F.scaled_dot_product_attention` function before they are multiplied by the value tensor, but it requires a slightly different approach since `F.scaled_dot_product_attention` is a function, not a module. PyTorch hooks can only be directly attached to instances of `nn.Module`. To work around this, you can encapsulate the `F.scaled_dot_product_attention` function within a custom module and then attach a hook to this module.\n\nHere’s how you can do it:\n\n### Step 1: Create a Custom Module\nFirst, you need to create a custom module that wraps the `F.scaled_dot_product_attention` function.\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass CustomScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, attn_mask=None):\n        # Compute the raw attention scores\n        attn_output_weights = torch.matmul(query, key.transpose(-2, -1))\n        attn_output_weights = attn_output_weights / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n        \n        if attn_mask is not None:\n            attn_output_weights += attn_mask\n        \n        # Apply softmax to get the probabilities\n        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n\n        # This is where you can use a hook to capture attn_output_weights before they are used to weight the values\n        attn_output = torch.matmul(attn_output_weights, value)\n\n        return attn_output, attn_output_weights\n```\n\n### Step 2: Define a Hook Function\nNext, define a hook function that captures the attention weights.\n\n```python\ndef attention_weights_hook(module, input, output):\n    # output[1] contains the attention weights\n    print(\"Attention Weights:\", output[1].detach())\n```\n\n### Step 3: Instantiate the Custom Module and Register the Hook\nCreate an instance of your custom module and register the hook.\n\n```python\n# Create an instance of the custom attention module\nattention_module = CustomScaledDotProductAttention()\n\n# Register the hook\nhook = attention_module.register_forward_hook(attention_weights_hook)\n\n# Example tensors for query, key, and value\nquery = torch.rand(10, 32, 64)  # (sequence_length, batch_size, d_model)\nkey = torch.rand(10, 32, 64)\nvalue = torch.rand(10, 32, 64)\n\n# Optionally, define an attention mask\nattn_mask = None  # Example: torch.zeros(10, 10)\n\n# Forward pass through the custom module\noutput = attention_module(query, key, value, attn_mask)\n```\n\n### Step 4: Use and Remove the Hook\nAfter using the hook to capture the attention weights during the forward pass, remember to remove it to avoid memory leaks.\n\n```python\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()\n```\n\n### Conclusion\nBy wrapping the `F.scaled_dot_product_attention` function in a custom module, you maintain the flexibility of using hooks to capture intermediate data like the attention weights. This method leverages the modular nature of PyTorch and allows for detailed inspection and manipulation of the data flowing through neural networks. This can be particularly useful for research, debugging, and enhancing model interpretability.","metadata":{}},{"cell_type":"code","source":"class CustomScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, query, key, value, attn_mask=None):\n        # Compute the raw attention scores\n        attn_output_weights = torch.matmul(query, key.transpose(-2, -1))\n        attn_output_weights = attn_output_weights / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n        \n        if attn_mask is not None:\n            attn_output_weights += attn_mask\n        \n        # Apply softmax to get the probabilities\n        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n\n        # This is where you can use a hook to capture attn_output_weights before they are used to weight the values\n        attn_output = torch.matmul(attn_output_weights, value)\n\n        return attn_output, attn_output_weights\n\ndef attention_weights_hook(module, input, output):\n    # output[1] contains the attention weights\n    print(\"Attention Weights:\", output[1].detach())\n\n# Create an instance of the custom attention module\nattention_module = CustomScaledDotProductAttention()\n\n# Register the hook\nhook = attention_module.register_forward_hook(attention_weights_hook)\n\n# Example tensors for query, key, and value\nquery = torch.rand(10, 32, 64)  # (sequence_length, batch_size, d_model)\nkey = torch.rand(10, 32, 64)\nvalue = torch.rand(10, 32, 64)\n\n# Optionally, define an attention mask\nattn_mask = None  # Example: torch.zeros(10, 10)\n\n# Forward pass through the custom module\noutput = attention_module(query, key, value, attn_mask)\n\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT 4\n\nYou cannot use hooks to directly retrieve the attention weights from `torch.nn.functional.scaled_dot_product_attention` because it is a functional call and not a module. Hooks can only be registered on `nn.Module` instances, where you have a `forward` method that defines the computation.\n\nHowever, if you wish to capture the attention weights from this function, you would typically need to write a custom wrapper module around the `scaled_dot_product_attention` call and then register a hook on that. Here's how you can do it:\n\n### Step 1: Create a Custom Module\nFirst, create a custom module that wraps the `scaled_dot_product_attention` function. This allows you to register hooks on this module.\n\n```python\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass ScaledDotProductAttention(nn.Module):\n    def forward(self, query, key, value, mask=None, dropout=None):\n        # Call the F.scaled_dot_product_attention function\n        attn_output, attn_output_weights = F.scaled_dot_product_attention(query, key, value, mask, dropout)\n\n        # Instead of returning the output, save the attn_output_weights as a member variable\n        self.attn_output_weights = attn_output_weights\n\n        # Return the attention output (which is the result of attention weights on value tensor)\n        return attn_output\n\n# Now you can instantiate this module and register a hook on it\nattention_module = ScaledDotProductAttention()\n```\n\n### Step 2: Register a Hook\nAfter creating the custom module, you can register a hook that will allow you to access the attention weights.\n\n```python\ndef get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # Save the attention weights, which are stored as a member variable in the module\n        attention_scores_list.append(module.attn_output_weights.detach())\n    return hook\n\n# List to store the attention weights\nattention_scores_list = []\n\n# Register the hook\nhook = attention_module.register_forward_hook(get_attention_hook(attention_scores_list))\n```\n\n### Step 3: Use the Custom Module in Your Forward Pass\nNow you can use the custom module in your forward pass, and the hook will capture the attention weights.\n\n```python\n# Example tensors for query, key, and value\nquery = torch.rand(10, 32, 512)\nkey = torch.rand(10, 32, 512)\nvalue = torch.rand(10, 32, 512)\n\n# Perform a forward pass\noutput = attention_module(query, key, value)\n\n# Check the captured attention weights\nprint(attention_scores_list)\n\n# Clean up: remove the hook\nhook.remove()\n```\n\nIn this example, after the forward pass, the `attention_scores_list` will contain the attention weights that were used before being multiplied to the `value` tensor. The hook captures these weights via the member variable `attn_output_weights` that we added to the `ScaledDotProductAttention` module.\n\nKeep in mind that this approach requires you to replace direct calls to `torch.nn.functional.scaled_dot_product_attention` with an instance of your custom module. This allows you to leverage hooks and capture the raw attention weights for further analysis or visualization.","metadata":{}},{"cell_type":"code","source":"class ScaledDotProductAttention(nn.Module):\n    def forward(self, query, key, value, mask=None, dropout=None):\n        # Call the F.scaled_dot_product_attention function\n        attn_output, attn_output_weights = F.scaled_dot_product_attention(query, key, value, mask, dropout)\n\n        # Instead of returning the output, save the attn_output_weights as a member variable\n        self.attn_output_weights = attn_output_weights\n\n        # Return the attention output (which is the result of attention weights on value tensor)\n        return attn_output\n\ndef get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # Save the attention weights, which are stored as a member variable in the module\n        attention_scores_list.append(module.attn_output_weights.detach())\n    return hook\n\n# List to store the attention weights\nattention_scores_list = []\n\n# Register the hook\nhook = attention_module.register_forward_hook(get_attention_hook(attention_scores_list))\n\n### Step 3: Use the Custom Module in Your Forward Pass\n\n# Example tensors for query, key, and value\nquery = torch.rand(10, 32, 512)\nkey = torch.rand(10, 32, 512)\nvalue = torch.rand(10, 32, 512)\n\n# Perform a forward pass\noutput = attention_module(query, key, value)\n\n# Check the captured attention weights\nprint(attention_scores_list)\n\n# Clean up: remove the hook\nhook.remove()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our Scaled Dot Product Attention","metadata":{}},{"cell_type":"markdown","source":"Mute our tensor generators and use tensors from above implementation","metadata":{}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout = dropout):\n        super().__init__()\n        \n        # Number of attention heads\n        self.h = h\n        \n        # Dimensionality of each attention head\n        self.d_k = d_model // h\n        \n        # Linear layers for query, key, value, and output projections\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p = dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask = None, dropout = None):\n        # Compute the dimensionality of each attention head\n        d_k = query.size(-1)\n        \n        # Compute the attention scores using the dot product between query and key\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        # Apply the mask to the attention scores (if provided)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Apply softmax to obtain the attention probabilities\n        p_attn = torch.softmax(scores, dim = -1)\n        \n        # Apply dropout to the attention probabilities (if specified)\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n        \n        # Compute the weighted sum of values using the attention probabilities\n        # Return the attended values and attention probabilities\n        return torch.matmul(p_attn, value), p_attn\n\n    def forward(self, query, key, value, mask = None):\n        # Get the number of batches\n        nbatches = query.size(0)\n        \n        # Project and reshape the query, key, and value for multi-head attention\n        # The projections are done using the linear layers defined in __init__\n        query, key, value = [\n            l(attn_output).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for l, attn_output in zip(self.linears, (query, key, value))\n        ]\n        \n        # Perform scaled dot-product attention on the projected query, key, and value\n        attn_output, attn_scores = self.scaled_dot_product_attention(query, key, value, mask = mask, dropout = self.dropout)\n        \n        # Reshape the attended output and concatenate the attention heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        \n        # Apply a final linear projection to the concatenated output\n        attn_output = self.linears[-1](attn_output)\n\n        # Return the attention output\n        return attn_output, attn_scores, query, key, value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"our_mha = MultiHeadedAttention(h, d_model, dropout).to(device)\n\n# parameters are the 3 copies of the input embedding tensor\nour_attn_output, query, key, value, our_attn_scores = our_mha(\n    text_embeddings_tensors, text_embeddings_tensors, \n    text_embeddings_tensors)\n\nprint(our_attn_output.shape)\nprint(our_attn_output[0])\n\nprint(our_attn_scores.shape)\nprint(our_attn_scores[0])\n\nprint(query[0])\nprint(key[0])\nprint(value[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison","metadata":{}},{"cell_type":"code","source":"def cosine_similarity(tensor1, tensor2):\n    tensor1_norm = tensor1 / tensor1.norm(dim=-1, keepdim=True)\n    tensor2_norm = tensor2 / tensor2.norm(dim=-1, keepdim=True)\n    return (tensor1_norm * tensor2_norm).sum(dim=-1)\n\n# cosine, mse, and correlation\nsimilarity = cosine_similarity(attn_output, our_attn_output)\nmse = torch.mean((attn_output - our_attn_output) ** 2)\ncorrelation = torch.corrcoef(torch.stack((attn_output.view(-1), our_attn_output.view(-1))))\n\nprint(\"Cosine Similarity:\", similarity)\nprint(\"Mean Squared Error:\", mse)\nprint(\"Correlation Coefficient:\", correlation[0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Comparing the attention outputs of our scaled dot product function and PyTorch's function is almost similar to comparing the attention outputs of different heads in a multi-head attention system. The differences arise from factors such as random initialization, implementation variations, and numerical precision, but the fundamental principles and functionality of the attention mechanism remain consistent.*\n\n*Random Initialization: The weights of the linear layers used for query, key, and value projections are randomly initialized in both functions. This random initialization leads to different starting points and can result in different attention patterns.*\n\n*Implementation Differences: Although both functions aim to implement the scaled dot product attention mechanism, there might be slight differences in their implementations, such as the order of operations, the use of specific PyTorch functions, or the handling of edge cases. These differences can contribute to variations in the attention outputs.*\n\n*Numerical Precision: The attention outputs can be sensitive to numerical precision, especially when dealing with large input sequences or high-dimensional representations. Differences in numerical precision between our function and PyTorch's function can lead to slight variations in the attention outputs.*","metadata":{}},{"cell_type":"markdown","source":"# Pytorch's Transformer Encoder\n","metadata":{}},{"cell_type":"markdown","source":"Look into d_ffn and set it to emb_size (300)\nPytorch Hooks","metadata":{}},{"cell_type":"code","source":"pytorch_encoder_layer = nn.TransformerEncoderLayer(d_model, h, d_ffn, dropout).to(device)\n\npytorch_encoded_batches = []  # List to store encoded batches\npytorch_y_batches = []        # List to store corresponding y batches\ni = 0\n\nwith torch.no_grad():\n    for x_batch, y_batch in data_loader:\n        print(f'Size of batch: {x_batch.shape}')\n        i += 1\n        print(i)\n        print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n        \n        # Move the batch to the device\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        output_features = pytorch_encoder_layer(x_batch)\n        \n        # Append the encoded batch to the list\n        pytorch_encoded_batches.append(output_features)\n        pytorch_y_batches.append(y_batch)\n        \n        print(f'Current Size of Reviews: {len(pytorch_encoded_batches)} tensors')\n        print(f'Current Size of Ratings: {len(pytorch_y_batches)} tensors')\n\n# Concatenate all the encoded batches into a single tensor\npytorch_encoded_batch = torch.cat(pytorch_encoded_batches, dim = 0)\n\n# Concatenate all the corresponding y batches into a single tensor\npytorch_y_batch = torch.cat(pytorch_y_batches, dim = 0)\n\nprint(total_encoded_batch.attention_weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT 4 Turbo\n\nYou cannot directly return the weights from `nn.TransformerEncoderLayer` without modifying the source code. The `nn.TransformerEncoderLayer` in PyTorch does not provide an out-of-the-box option to return the attention weights along with the output. The attention scores are computed internally within the multi-head attention mechanism, and only the output of the attention layer is returned to the user.\n\nHowever, there are a few alternative approaches you can take to inspect the attention weights:\n\n1. **Use Hooks**: You can register a forward hook on the multi-head attention submodule within the `nn.TransformerEncoderLayer`. This will let you access the intermediate values, including the attention weights, without modifying the source code. Here's how you could implement this:\n\n```python\nimport torch\n\ndef get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # output is a tuple in the form (output, attention_weights)\n        attention_scores_list.append(output[1].detach())\n    return hook\n\n# Example Usage:\nmodel = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# Initialize a list to hold the attention scores\nattention_scores_list = []\n\n# Register the hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Forward pass with input\ninput_tensor = torch.rand(10, 32, 512)  # (sequence_length, batch_size, d_model)\noutput = model(input_tensor)\n\n# Now attention_scores_list contains the attention weights\nprint(attention_scores_list)\n\n# Don't forget to remove the hook when you're done to prevent memory leaks\nhook.remove()\n```\n\n2. **Modify the Source Code**: If you're okay with modifying the source code, you can subclass `nn.TransformerEncoderLayer` and override the `forward` method to include the output of the attention scores.\n\n3. **Use a Custom Transformer Implementation**: There are custom implementations of the Transformer model that allow you to return the attention weights directly. For example, the Hugging Face Transformers library provides access to attention weights in models like BERT or GPT-2 when you pass `output_attentions=True` to the model.\n\nIf you simply want to inspect the weights and not necessarily return them within the same function, hooks are a great solution. However, if you require the weights to be returned for further processing, you may need to opt for the second or third approach.\n\nI understand. I already have a custom transformer implementation and was looking to compare the weights with those of nn.TransformerEncoderLayer. Can you expand more on the hooks approach?\n\nCertainly! Using hooks in PyTorch is a powerful way to access intermediate data in your models, such as weights and outputs from specific layers during the forward pass, without altering the original source code of those layers. When you want to compare the attention weights of your custom transformer implementation with those of the `nn.TransformerEncoderLayer`, using hooks can be particularly useful.\n\n### Detailed Explanation of Hooks\n\nHooks in PyTorch can be of two types: **forward hooks** and **backward hooks**. Forward hooks are used to grab information during the forward pass, which is what you'll need for accessing attention weights.\n\nHere’s an expanded explanation on using forward hooks with `nn.TransformerEncoderLayer`:\n\n1. **Defining the Hook Function**: The hook function you define will be called every time the specified layer has completed its forward pass. For the attention mechanism in `TransformerEncoderLayer`, the hook function needs to capture the second element of the output tuple, which contains the attention weights.\n\n2. **Registering the Hook**: You attach the hook to the specific submodule (in this case, the multi-head attention submodule of the encoder layer, `self_attn`). The hook will then store or process the attention weights during each forward pass.\n\n3. **Storing the Outputs**: In the example provided, the hook function appends the attention weights to a list. This list can be used later for comparison or analysis.\n\n4. **Managing the Hook**: It’s important to remove the hook after you're done with it to prevent memory leaks, especially if you're evaluating or training for multiple iterations.\n\n### Expanded Example with Explanations\n\nHere’s an expanded version of the earlier example with more detailed comments:\n\n```python\nimport torch\nimport torch.nn as nn\n\ndef get_attention_hook(attention_scores_list):\n    \"\"\"Hook function to capture attention weights.\"\"\"\n    def hook(module, input, output):\n        # Attention weights are stored in output[1], as the output is a tuple (output, attention_weights)\n        attention_scores_list.append(output[1].detach())  # Use detach() to avoid saving gradients\n    return hook\n\n# Initialize Transformer Encoder Layer\nmodel = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# List to store attention weights\nattention_scores_list = []\n\n# Register hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Create a random tensor to simulate input data (sequence_length, batch_size, d_model)\ninput_tensor = torch.rand(10, 32, 512)\n\n# Perform a forward pass (this will trigger the hook and capture the attention weights)\noutput = model(input_tensor)\n\n# Output the captured attention weights\nprint(attention_scores_list)\n\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()\n\n```\n\n### Using the Captured Data\n\nOnce you have the attention weights stored in `attention_scores_list`, you can process them as needed. For instance, you can:\n\n- **Visualize the Attention Patterns**: Use plotting libraries like Matplotlib to visualize how the attention is distributed across input tokens.\n- **Compare Weights**: Perform statistical comparisons or compute similarity measures between the attention weights from your custom transformer and the `nn.TransformerEncoderLayer`.\n- **Debug and Analyze Model Behavior**: Understanding attention patterns can help debug model behavior or improve model interpretability, especially on specific input examples.\n\nThis approach provides a non-intrusive method to extract and analyze the internals of PyTorch models, which can be invaluable for advanced machine learning research and development.\n\nCan I also use hooks to retrieve the attention weights from Pytorch's F.scaled_dot_product_attention before they are multiplied to the value tensor?","metadata":{}},{"cell_type":"code","source":"def get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # output is a tuple in the form (output, attention_weights)\n        attention_scores_list.append(output[1].detach())\n    return hook\n\n# Example Usage:\nmodel = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# Initialize a list to hold the attention scores\nattention_scores_list = []\n\n# Register the hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Forward pass with input\ninput_tensor = torch.rand(10, 32, 512)  # (sequence_length, batch_size, d_model)\noutput = model(input_tensor)\n\n# Now attention_scores_list contains the attention weights\nprint(attention_scores_list)\n\n# Don't forget to remove the hook when you're done to prevent memory leaks\nhook.remove()\n\n\n\n\n\ndef get_attention_hook(attention_scores_list):\n    \"\"\"Hook function to capture attention weights.\"\"\"\n    def hook(module, input, output):\n        # Attention weights are stored in output[1], as the output is a tuple (output, attention_weights)\n        attention_scores_list.append(output[1].detach())  # Use detach() to avoid saving gradients\n    return hook\n\n# Initialize Transformer Encoder Layer\nmodel = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# List to store attention weights\nattention_scores_list = []\n\n# Register hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Create a random tensor to simulate input data (sequence_length, batch_size, d_model)\ninput_tensor = torch.rand(10, 32, 512)\n\n# Perform a forward pass (this will trigger the hook and capture the attention weights)\noutput = model(input_tensor)\n\n# Output the captured attention weights\nprint(attention_scores_list)\n\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT 4\n\nYou cannot directly return the weights from `nn.TransformerEncoderLayer` without modifying the source code. The `nn.TransformerEncoderLayer` in PyTorch does not provide an out-of-the-box option to return the attention weights along with the output. The attention scores are computed internally within the multi-head attention mechanism, and only the output of the attention layer is returned to the user.\n\nHowever, there are a few alternative approaches you can take to inspect the attention weights:\n\n1. **Use Hooks**: You can register a forward hook on the multi-head attention submodule within the `nn.TransformerEncoderLayer`. This will let you access the intermediate values, including the attention weights, without modifying the source code. Here's how you could implement this:\n\n```python\nimport torch\n\ndef get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # output is a tuple in the form (output, attention_weights)\n        attention_scores_list.append(output[1].detach())\n    return hook\n\n# Example Usage:\nmodel = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# Initialize a list to hold the attention scores\nattention_scores_list = []\n\n# Register the hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Forward pass with input\ninput_tensor = torch.rand(10, 32, 512)  # (sequence_length, batch_size, d_model)\noutput = model(input_tensor)\n\n# Now attention_scores_list contains the attention weights\nprint(attention_scores_list)\n\n# Don't forget to remove the hook when you're done to prevent memory leaks\nhook.remove()\n```\n\n2. **Modify the Source Code**: If you're okay with modifying the source code, you can subclass `nn.TransformerEncoderLayer` and override the `forward` method to include the output of the attention scores.\n\n3. **Use a Custom Transformer Implementation**: There are custom implementations of the Transformer model that allow you to return the attention weights directly. For example, the Hugging Face Transformers library provides access to attention weights in models like BERT or GPT-2 when you pass `output_attentions=True` to the model.\n\nIf you simply want to inspect the weights and not necessarily return them within the same function, hooks are a great solution. However, if you require the weights to be returned for further processing, you may need to opt for the second or third approach.\n\nI understand. I already have a custom transformer implementation and was looking to compare the weights with those of nn.TransformerEncoderLayer. Can you expand more on the hooks approach?\n\nCertainly! Using hooks in PyTorch is a powerful way to access intermediate data in your models, such as weights and outputs from specific layers during the forward pass, without altering the original source code of those layers. When you want to compare the attention weights of your custom transformer implementation with those of the `nn.TransformerEncoderLayer`, using hooks can be particularly useful.\n\n### Detailed Explanation of Hooks\n\nHooks in PyTorch can be of two types: **forward hooks** and **backward hooks**. Forward hooks are used to grab information during the forward pass, which is what you'll need for accessing attention weights.\n\nHere’s an expanded explanation on using forward hooks with `nn.TransformerEncoderLayer`:\n\n1. **Defining the Hook Function**: The hook function you define will be called every time the specified layer has completed its forward pass. For the attention mechanism in `TransformerEncoderLayer`, the hook function needs to capture the second element of the output tuple, which contains the attention weights.\n\n2. **Registering the Hook**: You attach the hook to the specific submodule (in this case, the multi-head attention submodule of the encoder layer, `self_attn`). The hook will then store or process the attention weights during each forward pass.\n\n3. **Storing the Outputs**: In the example provided, the hook function appends the attention weights to a list. This list can be used later for comparison or analysis.\n\n4. **Managing the Hook**: It’s important to remove the hook after you're done with it to prevent memory leaks, especially if you're evaluating or training for multiple iterations.\n\n### Expanded Example with Explanations\n\nHere’s an expanded version of the earlier example with more detailed comments:\n\n```python\nimport torch\nimport torch.nn as nn\n\ndef get_attention_hook(attention_scores_list):\n    \"\"\"Hook function to capture attention weights.\"\"\"\n    def hook(module, input, output):\n        # Attention weights are stored in output[1], as the output is a tuple (output, attention_weights)\n        attention_scores_list.append(output[1].detach())  # Use detach() to avoid saving gradients\n    return hook\n\n# Initialize Transformer Encoder Layer\nmodel = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# List to store attention weights\nattention_scores_list = []\n\n# Register hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Create a random tensor to simulate input data (sequence_length, batch_size, d_model)\ninput_tensor = torch.rand(10, 32, 512)\n\n# Perform a forward pass (this will trigger the hook and capture the attention weights)\noutput = model(input_tensor)\n\n# Output the captured attention weights\nprint(attention_scores_list)\n\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()\n\n```\n\n### Using the Captured Data\n\nOnce you have the attention weights stored in `attention_scores_list`, you can process them as needed. For instance, you can:\n\n- **Visualize the Attention Patterns**: Use plotting libraries like Matplotlib to visualize how the attention is distributed across input tokens.\n- **Compare Weights**: Perform statistical comparisons or compute similarity measures between the attention weights from your custom transformer and the `nn.TransformerEncoderLayer`.\n- **Debug and Analyze Model Behavior**: Understanding attention patterns can help debug model behavior or improve model interpretability, especially on specific input examples.\n\nThis approach provides a non-intrusive method to extract and analyze the internals of PyTorch models, which can be invaluable for advanced machine learning research and development.\n\nCan I also use hooks to retrieve the attention weights from Pytorch's F.scaled_dot_product_attention before they are multiplied to the value tensor?","metadata":{}},{"cell_type":"code","source":"def get_attention_hook(attention_scores_list):\n    def hook(module, input, output):\n        # output is a tuple in the form (output, attention_weights)\n        attention_scores_list.append(output[1].detach())\n    return hook\n\n# Example Usage:\nmodel = torch.nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# Initialize a list to hold the attention scores\nattention_scores_list = []\n\n# Register the hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Forward pass with input\ninput_tensor = torch.rand(10, 32, 512)  # (sequence_length, batch_size, d_model)\noutput = model(input_tensor)\n\n# Now attention_scores_list contains the attention weights\nprint(attention_scores_list)\n\n# Don't forget to remove the hook when you're done to prevent memory leaks\nhook.remove()\n\n\n\n\ndef get_attention_hook(attention_scores_list):\n    \"\"\"Hook function to capture attention weights.\"\"\"\n    def hook(module, input, output):\n        # Attention weights are stored in output[1], as the output is a tuple (output, attention_weights)\n        attention_scores_list.append(output[1].detach())  # Use detach() to avoid saving gradients\n    return hook\n\n# Initialize Transformer Encoder Layer\nmodel = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n\n# List to store attention weights\nattention_scores_list = []\n\n# Register hook to the multi-head attention submodule\nhook = model.self_attn.register_forward_hook(get_attention_hook(attention_scores_list))\n\n# Create a random tensor to simulate input data (sequence_length, batch_size, d_model)\ninput_tensor = torch.rand(10, 32, 512)\n\n# Perform a forward pass (this will trigger the hook and capture the attention weights)\noutput = model(input_tensor)\n\n# Output the captured attention weights\nprint(attention_scores_list)\n\n# Cleanup: remove the hook to prevent memory leaks\nhook.remove()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our Transformer Encoder","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \n    def __init__(self, d_model: int, dropout: float, seq_len: int):\n        \"\"\"\n        Initialize the PositionalEncoding module.\n\n        Args:\n            d_model (int): The dimensionality of the model (embedding size).\n            dropout (float): The dropout rate to be applied.\n            seq_len (int): The maximum sequence length.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(p = dropout)\n\n        # Create a tensor of shape (seq_len, 1) representing the positions\n        position = torch.arange(seq_len).unsqueeze(1)\n        \n        # Compute the denominator term for the sinusoidal positional encoding\n        # The denominator is 10000^(2i/d_model), where i is the position index\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        \n        # Create a tensor of shape (seq_len, 1, d_model) to store the positional encodings\n        pe = torch.zeros(seq_len, 1, d_model)\n        \n        # Compute the sinusoidal positional encodings for even dimensions\n        # pe[:, 0, 0::2] selects the even dimensions (0, 2, 4, ...) of the positional encoding tensor\n        # position * div_term computes the angle for each position and even dimension\n        # torch.sin applies the sine function element-wise to compute the positional encoding values\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        \n        # Compute the sinusoidal positional encodings for odd dimensions\n        # pe[:, 0, 1::2] selects the odd dimensions (1, 3, 5, ...) of the positional encoding tensor\n        # torch.cos applies the cosine function element-wise to compute the positional encoding values\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        \n        # Register the positional encoding tensor as a buffer in the module\n        # This ensures that the positional encoding tensor is saved and loaded with the module\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Perform the forward pass of the PositionalEncoding module.\n\n        Args:\n            x (Tensor): The input tensor of shape (seq_len, batch_size, embedding_dim).\n\n        Returns:\n            Tensor: The input tensor with positional encodings added.\n        \"\"\"\n        # Add positional encodings to the input tensor\n        # x.size(0) returns the sequence length dimension of the input tensor\n        # self.pe[:x.size(0)] selects the positional encodings corresponding to the sequence length\n        x = x + self.pe[:x.size(0)]\n        \n        # Apply dropout to the tensor with positional encodings\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=dropout):\n        super().__init__()\n        \n        # Number of attention heads\n        self.h = h\n        \n        # Dimensionality of each attention head\n        self.d_k = d_model // h\n        \n        # Linear layers for query, key, value, and output projections\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p = dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask = None, dropout = None):\n        # Compute the dimensionality of each attention head\n        d_k = query.size(-1)\n        \n        # Compute the attention scores using the dot product between query and key\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        # Apply the mask to the attention scores (if provided)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Apply softmax to obtain the attention probabilities\n        p_attn = torch.softmax(scores, dim = -1)\n        \n        # Apply dropout to the attention probabilities (if specified)\n        if dropout is not None:\n            p_attn = dropout(p_attn)\n        \n        # Compute the weighted sum of values using the attention probabilities\n        # Return the attended values and attention probabilities\n        return torch.matmul(p_attn, value), p_attn\n\n    def forward(self, query, key, value, mask = None):\n        # Get the number of batches\n        nbatches = query.size(0)\n        \n        # Project and reshape the query, key, and value for multi-head attention\n        # The projections are done using the linear layers defined in __init__\n        query, key, value = [\n            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n        \n        # Perform scaled dot-product attention on the projected query, key, and value\n        x = self.scaled_dot_product_attention(query, key, value, mask = mask, dropout = self.dropout)\n        \n        # Reshape the attended output and concatenate the attention heads\n        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n        \n        # Apply a final linear projection to the concatenated output\n        x = self.linears[-1](x)\n\n        # Return the attention output\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\"\n    Construct a layernorm module (See citation for details).\n    \n    Layer normalization is a technique to normalize the activations of a layer.\n    It helps stabilize the training process and can lead to faster convergence.\n    \n    This implementation follows the original paper:\n    \"Layer Normalization\" by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton\n    https://arxiv.org/abs/1607.06450\n    \"\"\"\n    \n    def __init__(self, features, eps = eps):\n        \"\"\"\n        Initialize the LayerNorm module.\n        \n        Args:\n            features (int): The number of features (channels) in the input tensor.\n            eps (float): A small value added to the variance for numerical stability.\n                         Default is 1e-6.\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        \n        # Create learnable parameters for scaling and shifting\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        \n        self.eps = eps\n    \n    def forward(self, x):\n        \"\"\"\n        Perform layer normalization on the input tensor.\n        \n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, ..., features).\n        \n        Returns:\n            torch.Tensor: Normalized tensor of the same shape as the input.\n        \"\"\"\n        # Compute the mean across the last dimension (features)\n        mean = x.mean(-1, keepdim=True)\n        \n        # Compute the standard deviation across the last dimension (features)\n        std = x.std(-1, keepdim=True)\n        \n        # Normalize the input tensor\n        x_normalized = (x - mean) / (std + self.eps)\n        \n        # Scale and shift the normalized tensor\n        out = self.a_2 * x_normalized + self.b_2\n        \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Initializes the PositionwiseFeedForward module.\n\n        Args:\n            d_model (int): The dimension of the input embeddings.\n            d_ffn (int): The dimension of the hidden layer in the feed-forward network.\n            dropout (float): The probability of dropout occurring.\n        \"\"\"\n        super().__init__()\n        \n        # Linear layer that maps from the input dimension (d_model) to the hidden dimension (d_ffn)\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        \n        # Linear layer that maps from the hidden dimension (d_ffn) back to the input dimension (d_model)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        \n        # Dropout layer with the specified dropout probability\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Performs the forward pass of the PositionwiseFeedForward module.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model),\n                              representing the output from the attention mechanism.\n\n        Returns:\n            torch.Tensor: The output tensor of shape (batch_size, seq_length, d_model),\n                          representing the expanded-and-contracted representation.\n        \"\"\"\n        \n        # Apply the first linear transformation (w_1) to the input tensor (x)\n        # This maps the input from the embedding dimension (d_model) to the hidden dimension (d_ffn)\n        hidden = self.w_1(x)\n        \n        # Apply the ReLU activation function to the hidden representation\n        activated = torch.relu(hidden)\n        \n        # Apply dropout to the activated hidden representation\n        dropped = self.dropout(activated)\n        \n        # Apply the second linear transformation (w_2) to the dropped representation\n        # This maps the hidden dimension (d_ffn) back to the embedding dimension (d_model)\n        output = self.w_2(dropped)\n        \n        # Return the final output tensor\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize an encoder layer.\n\n        Args:\n            d_model (int): The dimension of the input and output of the layer.\n            num_heads (int): The number of attention heads.\n            d_ffn (int): The dimension of the feedforward network.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadedAttention(num_heads, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.sublayer = nn.ModuleList([LayerNorm(d_model) for _ in range(2)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the encoder layer.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the encoder layer.\n        \"\"\"\n        sublayer_output, attn = self.self_attn(x, x, x, mask)\n        x = x + self.dropout(sublayer_output)  # Apply dropout to the sublayer output before adding it to the input\n        x = self.sublayer[0](x)  # Apply normalization after the residual connection\n        sublayer_output = self.feed_forward(x)\n        x = x + self.dropout(sublayer_output)  # Again, apply dropout before the residual connection\n        x = self.sublayer[1](x)  # Apply normalization after the residual connection\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StackedEncoder(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, d_ffn, dropout):\n        \"\"\"\n        Initialize a stacked encoder.\n\n        Args:\n            num_layers (int): The number of encoder layers.\n            d_model (int): The dimension of the input and output of each layer.\n            num_heads (int): The number of attention heads in each layer.\n            d_ffn (int): The dimension of the feedforward network in each layer.\n            dropout (float): The dropout probability.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ffn, dropout) for _ in range(num_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        \"\"\"\n        Perform the forward pass of the stacked encoder.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n            mask (torch.Tensor): The attention mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n\n        Returns:\n            torch.Tensor: The output tensor of the stacked encoder.\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)  # Apply normalization to the output of the last layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_encoder = StackedEncoder(num_layers, d_model, \n                                 h, d_ffn, dropout).to(device)\n\n# encoder information\nprint('Below is the complete encoder: \\n')\nprint(stacked_encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_encoded_batches = []  # List to store encoded batches\ntotal_y_batches = []        # List to store corresponding y batches\n\ni = 0\n\nwith torch.no_grad():\n    for x_batch, y_batch in data_loader:\n        print(f'Size of batch: {x_batch.shape}')\n        i += 1\n        print(i)\n        print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n        \n        # Move the batch to the device\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        # Encode the batch using the stacked_encoder\n        encoded_batch = stacked_encoder(x_batch, mask = None)\n        \n        # Append the encoded batch to the list\n        total_encoded_batches.append(encoded_batch)\n        total_y_batches.append(y_batch)\n        \n        print(f'Current Size of Reviews: {len(total_encoded_batches)} tensors')\n        print(f'Current Size of Ratings: {len(total_y_batches)} tensors')\n\n# Concatenate all the encoded batches into a single tensor\ntotal_encoded_batch = torch.cat(total_encoded_batches, dim = 0)\n\n# Concatenate all the corresponding y batches into a single tensor\ntotal_y_batch = torch.cat(total_y_batches, dim = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the tensors\nprint(total_encoded_batch.shape)\nprint(total_y_batch.shape)\nprint('\\n')\n\n# size in MB\nprint(f'total_encoded_batch in bytes: { total_encoded_batch.nelement() * total_encoded_batch.element_size() }')\nprint(f'total_y_batch in bytes: { total_y_batch.nelement() * total_y_batch.element_size() }')\nprint('\\n')\n\n# position of the tensors\nprint(f\"The Encoded batch is on: {total_encoded_batch.device}\")\nprint(f\"The Y batch is on: {total_y_batch.device}\")\nprint('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training approaches like masked-language modeling next sentence prediction, permutative language modeling, contrastive learning\n\n1. Visualization: You can visualize the input and output tensors to see if there are any noticeable patterns or differences. Techniques like heatmaps, scatter plots, or dimensionality reduction methods (e.g., t-SNE, PCA) can help you visualize high-dimensional data in a more interpretable way.\n\n2. Comparing input and output: You can directly compare the input and output tensors element-wise to see how the values have changed. This can give you an idea of the magnitude and direction of the transformations applied by the encoder.\n\n3. Analyzing attention weights: If our transformer encoder uses attention mechanisms, you can examine the attention weights to understand which parts of the input the model is focusing on. Higher attention weights indicate that the model is paying more attention to those specific positions or features.\n\n4. Probing the learned representations: You can train a separate model (e.g., a classifier or regressor) on top of the encoded representations to see if they capture meaningful information for a downstream task. If the model performs well, it suggests that the encoder has learned useful representations.\n\n5. Ablation studies: You can systematically remove or modify certain components of the transformer encoder (e.g., attention layers, normalization) and observe how the output changes. This can help you understand the role and impact of each component on the encoding process.\n\n6. Analyzing the distribution of values: You can compute statistics like mean, variance, and range of the input and output tensors to see how the distribution of values has changed. This can provide insights into the overall effect of the encoder on the data.\n\n7. Gradient-based methods: If you have access to the gradients of the encoder, you can use techniques like saliency maps or gradient-based attribution methods to identify which input features have the most influence on the output.\n\nKeep in mind that interpreting the behavior of deep learning models can be challenging, especially for complex architectures like transformers. It often requires a combination of different analysis techniques and domain knowledge to gain meaningful insights.\n\nRemember to normalize or scale the input and output tensors appropriately before visualization or comparison, as the raw floating-point values may have different scales and ranges.","metadata":{}}]}