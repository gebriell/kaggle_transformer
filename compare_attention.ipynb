{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gebrielgidey/compare-attention-ipynb?scriptVersionId=185949079\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#>> updated 4/1\n# For viewing and manipulating data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Importing the necessary libraries\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors # >> alternative to gensim.downloader\nimport matplotlib.pyplot as plt\n\n# Getting particular functions from these libraries\nfrom scipy import stats\nfrom torch import Tensor\nfrom sklearn.utils import resample\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom collections import Counter\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nreviews_file = ''\nw2v_file = ''\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        if file_name.endswith('.csv'): \n            reviews_file = file_name\n        elif file_name.endswith('.bin') or ('.gz'):\n            w2v_file = file_name\n        else:\n            print(f'Found unexpected file: {file_name}')\n            \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nVERBOSE = True\ndef printv(text):\n    if VERBOSE: print('VERBOSE:', text)\n    return\n\ndef showV(text):\n    '''unconditional verbose output'''\n    print('VERBOSE:', text)\n    return\n\nDEV = True\ndef printd(text):\n    if DEV: print('DEV:', text)\n    return\n\ndef showD(text):\n    '''unconditional DEV output'''\n    print('VERBOSE:', text)\n    return\n\nshowCellCompletion = False\ndef showC(text):\n    if showCellCompletion:\n        print('Cell complete:', text)\n    return\n\nimport subprocess\nshowNv = True\naccelerator = True\n\ndef printNv():\n    if not showNv or not accelerator: return\n    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n    print(mem_usage.stdout.decode('utf-8'))\n\nshowMemoryAllocation = True\ndef printM():\n    if not showMemoryAllocation: return\n    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-28T17:23:23.737655Z","iopub.execute_input":"2024-06-28T17:23:23.738454Z","iopub.status.idle":"2024-06-28T17:23:38.457125Z","shell.execute_reply.started":"2024-06-28T17:23:23.738422Z","shell.execute_reply":"2024-06-28T17:23:38.456127Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Checks if a CUDA enabled GPU is available and prints out its information\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        \n    device = torch.device(\"cuda:0\")\n\nelse:\n    print(\"CUDA is not available.\")\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:23:42.677505Z","iopub.execute_input":"2024-06-28T17:23:42.677862Z","iopub.status.idle":"2024-06-28T17:23:42.683766Z","shell.execute_reply.started":"2024-06-28T17:23:42.677834Z","shell.execute_reply":"2024-06-28T17:23:42.682788Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CUDA is available!\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# import h5py\n# file_path = '/kaggle/input/145000/145000.h5'\n# with h5py.File(file_path, 'r') as hf:\n#   # Access the datasets within the HDF5 file\n#   text_reviews_dataset = hf['text_embeddings']\n#   ratings_dataset = hf['rating_labels']\n#   masks_dataset = hf['mask']\n\n#   # Convert the datasets to PyTorch tensors\n#   text_reviews = torch.from_numpy(text_reviews_dataset[:])\n#   ratings = torch.from_numpy(ratings_dataset[:])\n#   masks = torch.from_numpy(masks_dataset[:])\n\n# # check loaded tensors\n\n# print(text_reviews.shape)\n# print(ratings.shape)\n# print(masks.shape)\n\n# class ReviewDataset(Dataset):\n#   def __init__(self, text_reviews, ratings, masks):\n#     self.text_reviews = text_reviews\n#     self.ratings = ratings\n#     self.masks = masks\n\n#   def __getitem__(self, index):\n#     review = self.text_reviews[index]\n#     rating = self.ratings[index]\n#     mask = self.masks[index]\n\n\n#     return review, rating, mask\n\n# print(text_reviews.shape)\n# print(ratings.shape)\n# print(masks.shape)\n\n# # Create the dataset\n# dataset = ReviewDataset(text_reviews, ratings, masks)\n\n# # Create data loaaders\n# data_loader = DataLoader(dataset, batch_size = 32, shuffle = False)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiation","metadata":{}},{"cell_type":"code","source":"text_embeddings_tensors = torch.rand(1000, 100, 300).to(device)\n\n# Linear projection matrices (takes in 300 dimesions, outputs 300 dimensions)\nW_k = nn.Linear(300, 300).to(device)\nW_q = nn.Linear(300, 300).to(device)\nW_v = nn.Linear(300, 300).to(device)\n\n# Compute key, query, and value tensors\nkey_tensor   = W_k(text_embeddings_tensors) # shape: (batch_size, max_len, emb)\nquery_tensor = W_q(text_embeddings_tensors) # shape: (batch_size, max_len, emb)\nvalue_tensor = W_v(text_embeddings_tensors) # shape: (batch_size, max_len, emb)\n\nprint(text_embeddings_tensors[0])\nprint(key_tensor[0])\nprint(query_tensor[0])\nprint(value_tensor[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:23:44.83854Z","iopub.execute_input":"2024-06-28T17:23:44.838899Z","iopub.status.idle":"2024-06-28T17:23:45.735395Z","shell.execute_reply.started":"2024-06-28T17:23:44.838869Z","shell.execute_reply":"2024-06-28T17:23:45.734324Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor([[0.5121, 0.3123, 0.1569,  ..., 0.3187, 0.2253, 0.9384],\n        [0.6458, 0.8690, 0.7288,  ..., 0.1773, 0.5933, 0.5932],\n        [0.8236, 0.8649, 0.6571,  ..., 0.1761, 0.2031, 0.9777],\n        ...,\n        [0.4672, 0.2094, 0.0991,  ..., 0.7459, 0.8336, 0.9345],\n        [0.7825, 0.9908, 0.7685,  ..., 0.7729, 0.5302, 0.3785],\n        [0.1011, 0.3027, 0.8615,  ..., 0.2218, 0.2238, 0.0121]],\n       device='cuda:0')\ntensor([[ 2.4702e-02,  4.6389e-01, -8.7308e-02,  ...,  4.1977e-01,\n          2.4056e-01,  4.2406e-01],\n        [ 1.1506e-01,  5.1160e-01, -3.0963e-02,  ...,  2.4979e-01,\n          3.2743e-01,  2.3399e-02],\n        [ 2.8117e-01,  5.9181e-01,  9.6312e-05,  ...,  2.4495e-01,\n          7.2934e-02,  3.0909e-01],\n        ...,\n        [ 8.8383e-03,  5.3434e-01,  2.7508e-03,  ...,  3.1964e-01,\n          1.8849e-01,  1.7927e-01],\n        [-2.1820e-01,  1.5770e-01,  2.6884e-02,  ...,  2.0175e-01,\n          4.4826e-01,  2.6815e-01],\n        [-9.2963e-02,  2.4232e-01,  9.9712e-02,  ...,  2.5156e-01,\n          1.7206e-01,  5.5466e-02]], device='cuda:0',\n       grad_fn=<SelectBackward0>)\ntensor([[-0.0688, -0.3995, -0.2857,  ..., -0.2803,  0.0279, -0.1708],\n        [-0.3859, -0.4854, -0.3111,  ..., -0.3284,  0.0885, -0.1785],\n        [-0.3218,  0.0145, -0.1388,  ..., -0.2895, -0.0198,  0.0045],\n        ...,\n        [-0.2958, -0.1216, -0.3634,  ..., -0.2844, -0.1988,  0.1725],\n        [-0.2796, -0.2140, -0.3021,  ..., -0.0112, -0.3553,  0.0927],\n        [-0.5177, -0.2675, -0.1507,  ..., -0.2645, -0.0361, -0.0057]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 0.1448,  0.0221,  0.2794,  ..., -0.1451,  0.0331, -0.0738],\n        [ 0.1953,  0.0855,  0.1607,  ..., -0.3904, -0.0510,  0.0319],\n        [ 0.0681,  0.0293,  0.2969,  ..., -0.1504,  0.2406, -0.2569],\n        ...,\n        [ 0.3574, -0.0888,  0.3465,  ..., -0.3903,  0.1693, -0.0565],\n        [ 0.2218,  0.0508,  0.0186,  ..., -0.4991, -0.0037,  0.1052],\n        [ 0.2120,  0.2295,  0.4527,  ..., -0.3136, -0.0075,  0.0322]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyperparams","metadata":{}},{"cell_type":"code","source":"# HyperParameters for the module\nd_model = 300  # Should match the embedding dimension of our word embeddings\nseq_len = 100  # Maximum sequence length\ninput_size = d_model  # based on the output size of our feed-forward network\n\nnum_layers = 1 # 4 # Number of encoder layers\nh       = 1 # 10   # number of attention head\nd_ffn   = 2048 # dimension of the feedforward layer\n\ndropout = 0.0 # 0.1  # we can adjust the dropout if needed\neps     = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\nlearning_rate = 0.01\n\n\"\"\"\nTo ensure compatibility, it's important to choose the \nnumber of attention heads (h) such that d_model is \nevenly divisible by h in the multi-head attention \nmodule's self.d_k. This allows for a clean distribution \nof the model dimensionality across the attention heads.\n\"\"\"\n\n# d_model / attn.h = 300 / 10 = 30","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:23:50.782431Z","iopub.execute_input":"2024-06-28T17:23:50.782764Z","iopub.status.idle":"2024-06-28T17:23:50.791442Z","shell.execute_reply.started":"2024-06-28T17:23:50.782739Z","shell.execute_reply":"2024-06-28T17:23:50.790572Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"\\nTo ensure compatibility, it's important to choose the \\nnumber of attention heads (h) such that d_model is \\nevenly divisible by h in the multi-head attention \\nmodule's self.d_k. This allows for a clean distribution \\nof the model dimensionality across the attention heads.\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"#### Our Scaled Dot Product Attention \n#### Our Multi-Head Attention\n\n#### -------------------------------------------------------\n\n#### Pytorch's Scaled Dot Product Attention\n#### Pytorch's Multi-Head Attention\n\n#### -------------------------------------------------------\n\n#### Our Transformer Encoder\n#### Pytorch's Transformer Encoder\n#### Pytorch's Transformer Encoder (Custom)","metadata":{}},{"cell_type":"markdown","source":"# Our Scaled Dot Product Attention \n# Our Multi-Head Attention\n\n### Mute our tensor generators and use tensors from above implementation\n\n*In this modified version, we've removed the linear layers for projecting the input embeddings to query, key, and value tensors. Instead, the forward method now accepts pre-computed query, key, and value tensors as input.*","metadata":{}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h: int, d_model: int, dropout: float = 0.0):\n        super().__init__()\n\n        self.d_model = d_model\n        self.h = h\n        self.d_k = d_model // h\n\n        self.query_linear = nn.Linear(d_model, d_model)\n        self.key_linear = nn.Linear(d_model, d_model)\n        self.value_linear = nn.Linear(d_model, d_model)\n        self.output_linear = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n        \"\"\"\n        Compute the scaled dot-product attention.\n\n        Args:\n            query: Tensor of shape [batch_size, h, seq_len, d_k].\n            key: Tensor of shape [batch_size, h, seq_len, d_k].\n            value: Tensor of shape [batch_size, h, seq_len, d_k].\n            mask: Optional mask tensor of shape [batch_size, 1, seq_len, seq_len].\n            dropout: Optional dropout layer.\n\n        Returns:\n            attention_output: Tensor of shape [batch_size, h, seq_len, d_k].\n            attention_weights: Tensor of shape [batch_size, h, seq_len, seq_len].\n        \"\"\"\n        d_k = query.size(-1)\n        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            # Ensure mask has the same shape as attention_scores\n            mask = mask.unsqueeze(1)  # Add dimension for heads first\n            # Then expand to match attention_scores\n            mask = mask.expand(-1, query.size(1), -1, -1)\n            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n\n        if dropout is not None:\n            attention_weights = dropout(attention_weights)\n\n        attention_output = torch.matmul(attention_weights, value)\n\n        return attention_output, attention_weights\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Compute the multi-headed attention.\n\n        Args:\n            query: Tensor of shape [batch_size, seq_len, d_model].\n            key: Tensor of shape [batch_size, seq_len, d_model].\n            value: Tensor of shape [batch_size, seq_len, d_model].\n            mask: Optional mask tensor of shape [batch_size, seq_len, seq_len].\n\n        Returns:\n            output: Tensor of shape [batch_size, seq_len, d_model].\n        \"\"\"\n        batch_size = query.size(0)\n\n#         query = self.query_linear(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n#         key = self.key_linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n#         value = self.value_linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)  # Add a dimension for h\n\n        # attention_output, _ = self.scaled_dot_product_attention(query, key, value, mask, self.dropout)\n        attention_output, attention_weights = self.scaled_dot_product_attention(query, key, value, mask, self.dropout) \n\n        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        output = self.output_linear(attention_output)\n\n        #return output\n        return attention_weights, output # Return both values\n    \n# ours\nour_mha = MultiHeadedAttention(h, d_model, dropout).to(device)\n\n# Use pre-computed query, key, and value tensors\nour_attn_weights, our_attn_output = our_mha(query_tensor, key_tensor, value_tensor)\n\nprint(our_attn_weights[0])\nprint(our_attn_output[0])\n\nprint(\"Attention Weights (shape:\", our_attn_weights.shape, \")\")\nprint(\"Attention Scores (shape:\", our_attn_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:23:54.675288Z","iopub.execute_input":"2024-06-28T17:23:54.67563Z","iopub.status.idle":"2024-06-28T17:23:54.748366Z","shell.execute_reply.started":"2024-06-28T17:23:54.675604Z","shell.execute_reply":"2024-06-28T17:23:54.747432Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[0.0103, 0.0095, 0.0098,  ..., 0.0097, 0.0109, 0.0096],\n        [0.0101, 0.0094, 0.0104,  ..., 0.0101, 0.0103, 0.0098],\n        [0.0099, 0.0094, 0.0107,  ..., 0.0098, 0.0100, 0.0092],\n        ...,\n        [0.0107, 0.0099, 0.0104,  ..., 0.0097, 0.0097, 0.0098],\n        [0.0101, 0.0094, 0.0097,  ..., 0.0098, 0.0102, 0.0093],\n        [0.0109, 0.0095, 0.0099,  ..., 0.0106, 0.0097, 0.0094]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 0.1800,  0.1482, -0.0772,  ...,  0.1646, -0.0510,  0.1309],\n        [ 0.1610,  0.0924, -0.1067,  ...,  0.0653, -0.1533,  0.1574],\n        [-0.0397,  0.0012,  0.0203,  ..., -0.0285, -0.0270, -0.0785],\n        ...,\n        [ 0.0859,  0.0469, -0.0650,  ...,  0.0068, -0.1386,  0.0817],\n        [-0.1329, -0.0972, -0.2016,  ..., -0.0973,  0.1515, -0.1282],\n        [-0.1302, -0.0847,  0.0308,  ..., -0.1535, -0.0661, -0.1399]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\nAttention Weights (shape: torch.Size([1000, 100, 100]) )\nAttention Scores (shape: torch.Size([1000, 100, 300]) )\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pytorch's Scaled Dot Product Attention","metadata":{}},{"cell_type":"code","source":"# FROM TORCH.NN.FUNCTIONAL.SCALED_DOT_PRODUCT_ATTENTION DOCS\n\n# Efficient implementation equivalent to the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype, device = device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight, attn_weight @ value\n\n# pytorch\nattn_weights, attn_output = scaled_dot_product_attention(\n    query_tensor, key_tensor, value_tensor)\n\nprint(attn_weights[0])\nprint(attn_output[0])\n\nprint(\"Attention Weights (shape:\", attn_weights.shape, \")\")\nprint(\"Attention Scores (shape:\", attn_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:23:58.175633Z","iopub.execute_input":"2024-06-28T17:23:58.175984Z","iopub.status.idle":"2024-06-28T17:23:58.227121Z","shell.execute_reply.started":"2024-06-28T17:23:58.175957Z","shell.execute_reply":"2024-06-28T17:23:58.226132Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[0.0103, 0.0095, 0.0098,  ..., 0.0097, 0.0109, 0.0096],\n        [0.0101, 0.0094, 0.0104,  ..., 0.0101, 0.0103, 0.0098],\n        [0.0099, 0.0094, 0.0107,  ..., 0.0098, 0.0100, 0.0092],\n        ...,\n        [0.0107, 0.0099, 0.0104,  ..., 0.0097, 0.0097, 0.0098],\n        [0.0101, 0.0094, 0.0097,  ..., 0.0098, 0.0102, 0.0093],\n        [0.0109, 0.0095, 0.0099,  ..., 0.0106, 0.0097, 0.0094]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 0.2117,  0.0447,  0.2545,  ..., -0.2009,  0.0257, -0.0692],\n        [ 0.2114,  0.0458,  0.2538,  ..., -0.2007,  0.0253, -0.0685],\n        [ 0.2109,  0.0454,  0.2545,  ..., -0.2003,  0.0248, -0.0696],\n        ...,\n        [ 0.2103,  0.0452,  0.2550,  ..., -0.1998,  0.0247, -0.0687],\n        [ 0.2112,  0.0457,  0.2545,  ..., -0.2005,  0.0258, -0.0689],\n        [ 0.2112,  0.0450,  0.2542,  ..., -0.2001,  0.0251, -0.0685]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\nAttention Weights (shape: torch.Size([1000, 100, 100]) )\nAttention Scores (shape: torch.Size([1000, 100, 300]) )\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Pytorch's Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"multihead_attn = nn.MultiheadAttention(d_model, h).to(device)\nmulti_attn_output, multi_attn_weights = multihead_attn(\n    query_tensor, key_tensor, value_tensor)\n\nprint(multi_attn_weights[0])\nprint(multi_attn_output[0])\n\nprint(\"Attention Weights (shape:\", multi_attn_weights.shape, \")\")\nprint(\"Attention Scores (shape:\", multi_attn_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:24:01.564978Z","iopub.execute_input":"2024-06-28T17:24:01.565345Z","iopub.status.idle":"2024-06-28T17:24:01.620746Z","shell.execute_reply.started":"2024-06-28T17:24:01.565316Z","shell.execute_reply":"2024-06-28T17:24:01.619763Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0009, 0.0010],\n        ...,\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 0.0044, -0.0584,  0.2606,  ..., -0.1229, -0.0265, -0.0580],\n        [ 0.0013, -0.0609,  0.2588,  ..., -0.1209, -0.0279, -0.0578],\n        [ 0.0051, -0.0573,  0.2582,  ..., -0.1231, -0.0282, -0.0585],\n        ...,\n        [ 0.0067, -0.0647,  0.2595,  ..., -0.1183, -0.0284, -0.0589],\n        [ 0.0035, -0.0604,  0.2568,  ..., -0.1192, -0.0294, -0.0575],\n        [ 0.0053, -0.0592,  0.2605,  ..., -0.1220, -0.0273, -0.0556]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\nAttention Weights (shape: torch.Size([100, 1000, 1000]) )\nAttention Scores (shape: torch.Size([1000, 100, 300]) )\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Our Transformer Encoder","metadata":{}},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h: int, d_model: int, dropout: float = 0.0):\n        super().__init__()\n\n        self.d_model = d_model\n        self.h = h\n        self.d_k = d_model // h\n\n        self.query_linear = nn.Linear(d_model, d_model)\n        self.key_linear = nn.Linear(d_model, d_model)\n        self.value_linear = nn.Linear(d_model, d_model)\n        self.output_linear = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    @staticmethod\n    def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n        \"\"\"\n        Compute the scaled dot-product attention.\n\n        Args:\n            query: Tensor of shape [batch_size, h, seq_len, d_k].\n            key: Tensor of shape [batch_size, h, seq_len, d_k].\n            value: Tensor of shape [batch_size, h, seq_len, d_k].\n            mask: Optional mask tensor of shape [batch_size, 1, seq_len, seq_len].\n            dropout: Optional dropout layer.\n\n        Returns:\n            attention_output: Tensor of shape [batch_size, h, seq_len, d_k].\n            attention_weights: Tensor of shape [batch_size, h, seq_len, seq_len].\n        \"\"\"\n        d_k = query.size(-1)\n        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n        if mask is not None:\n            # Ensure mask has the same shape as attention_scores\n            mask = mask.unsqueeze(1)  # Add dimension for heads first\n            # Then expand to match attention_scores\n            mask = mask.expand(-1, query.size(1), -1, -1)\n            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n\n        if dropout is not None:\n            attention_weights = dropout(attention_weights)\n\n        attention_output = torch.matmul(attention_weights, value)\n\n        return attention_output, attention_weights\n\n    def forward(self, query, key, value, mask=None):\n        \"\"\"\n        Compute the multi-headed attention.\n\n        Args:\n            query: Tensor of shape [batch_size, seq_len, d_model].\n            key: Tensor of shape [batch_size, seq_len, d_model].\n            value: Tensor of shape [batch_size, seq_len, d_model].\n            mask: Optional mask tensor of shape [batch_size, seq_len, seq_len].\n\n        Returns:\n            output: Tensor of shape [batch_size, seq_len, d_model].\n        \"\"\"\n        batch_size = query.size(0)\n\n#         query = self.query_linear(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n#         key = self.key_linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n#         value = self.value_linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)  # Add a dimension for h\n\n        # attention_output, _ = self.scaled_dot_product_attention(query, key, value, mask, self.dropout)\n        attention_output, attention_weights = self.scaled_dot_product_attention(query, key, value, mask, self.dropout) \n\n        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        output = self.output_linear(attention_output)\n\n        #return output\n        return attention_weights, output # Return both values\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features: int, eps: float = 1e-6):\n        \"\"\"\n        Construct a layernorm module.\n\n        Args:\n            features: Number of input features.\n            eps: Small value added to the denominator for numerical stability.\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        \"\"\"\n        Apply layer normalization.\n\n        Args:\n            x: Input tensor of shape [batch_size, seq_len, features].\n\n        Returns:\n            Normalized tensor of the same shape as the input.\n        \"\"\"\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Construct a position-wise feed-forward network.\n\n        Args:\n            d_model: Dimension of the input embeddings.\n            d_ffn: Dimension of the feed-forward network.\n            dropout: Probability of dropout.\n        \"\"\"\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ffn)\n        self.w_2 = nn.Linear(d_ffn, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Apply the position-wise feed-forward network.\n\n        Args:\n            x: Input tensor of shape [batch_size, seq_len, d_model].\n\n        Returns:\n            Output tensor of shape [batch_size, seq_len, d_model].\n        \"\"\"\n        return self.w_2(self.dropout(torch.relu(self.w_1(x))))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, h: int, d_ffn: int, dropout: float):\n        \"\"\"\n        Construct an encoder layer.\n\n        Args:\n            d_model: Dimension of the input embeddings.\n            h: Number of attention heads.\n            d_ffn: Dimension of the feed-forward network.\n            dropout: Probability of dropout.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadedAttention(h, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.sublayer = nn.ModuleList([LayerNorm(d_model) for _ in range(2)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Apply the encoder layer.\n\n        Args:\n            x: Input tensor of shape [batch_size, seq_len, d_model].\n            mask: Attention mask tensor of shape [batch_size, seq_len, seq_len].\n\n        Returns:\n            Output tensor of shape [batch_size, seq_len, d_model].\n        \"\"\"\n        sublayer_output = self.self_attn(x, x, x, mask)\n        x = x + self.dropout(sublayer_output)\n        x = self.sublayer[0](x)\n        sublayer_output = self.feed_forward(x)\n        x = x + self.dropout(sublayer_output)\n        x = self.sublayer[1](x)\n        return x\n\nclass StackedEncoder(nn.Module):\n    def __init__(self, num_layers: int, d_model: int, h: int, d_ffn: int, dropout: float):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, h, d_ffn, dropout) for _ in range(num_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n\n    def forward(self, key, query, value, mask=None):\n        \"\"\"\n        Apply the stacked encoder.\n\n        Args:\n            key: Input tensor of shape [batch_size, seq_len, d_model].\n            query: Input tensor of shape [batch_size, seq_len, d_model].\n            value: Input tensor of shape [batch_size, seq_len, d_model].\n            mask: Attention mask tensor of shape [batch_size, seq_len, seq_len].\n\n        Returns:\n            tuple: (attention_weights, output)\n                attention_weights: Tensor of shape [batch_size, num_layers, h, seq_len, seq_len].\n                output: Tensor of shape [batch_size, seq_len, d_model].\n        \"\"\"\n        attention_weights = []\n        x = query  # Start with query as the input\n\n        for layer in self.layers:\n            layer_weights, x = layer(key, query, value, mask)\n            attention_weights.append(layer_weights)\n\n        x = self.norm(x)\n        attention_weights = torch.stack(attention_weights, dim=1)\n        \n        return attention_weights, x\n\n# Update the EncoderLayer to return attention weights\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model: int, h: int, d_ffn: int, dropout: float):\n        super().__init__()\n        self.self_attn = MultiHeadedAttention(h, d_model, dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ffn, dropout)\n        self.sublayer = nn.ModuleList([LayerNorm(d_model) for _ in range(2)])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, key, query, value, mask=None):\n        attn_weights, sublayer_output = self.self_attn(query, key, value, mask)\n        x = query + self.dropout(sublayer_output)\n        x = self.sublayer[0](x)\n        sublayer_output = self.feed_forward(x)\n        x = x + self.dropout(sublayer_output)\n        x = self.sublayer[1](x)\n        return attn_weights, x\n\n# Usage\nstacked_encoder = StackedEncoder(num_layers, d_model, h, d_ffn, dropout).to(device)\n\n# When using the encoder\nour_encoder_weights, our_encoder_output = stacked_encoder(key_tensor, query_tensor, value_tensor, mask=None)\n\nprint(our_encoder_weights[0])\nprint(our_encoder_output[0])\n\nprint(\"Attention Weights (shape:\", our_encoder_weights.shape, \")\")\nprint(\"Encoder Output (shape:\", our_encoder_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:24:04.370583Z","iopub.execute_input":"2024-06-28T17:24:04.370905Z","iopub.status.idle":"2024-06-28T17:24:04.488969Z","shell.execute_reply.started":"2024-06-28T17:24:04.37088Z","shell.execute_reply":"2024-06-28T17:24:04.488006Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([[[0.0103, 0.0095, 0.0098,  ..., 0.0097, 0.0109, 0.0096],\n         [0.0101, 0.0094, 0.0104,  ..., 0.0101, 0.0103, 0.0098],\n         [0.0099, 0.0094, 0.0107,  ..., 0.0098, 0.0100, 0.0092],\n         ...,\n         [0.0107, 0.0099, 0.0104,  ..., 0.0097, 0.0097, 0.0098],\n         [0.0101, 0.0094, 0.0097,  ..., 0.0098, 0.0102, 0.0093],\n         [0.0109, 0.0095, 0.0099,  ..., 0.0106, 0.0097, 0.0094]]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 0.0590, -0.7327, -0.9108,  ..., -0.8204, -0.1390, -0.5511],\n        [-0.4166, -1.2057, -1.3338,  ..., -0.7905, -0.3517, -0.2234],\n        [-1.0198,  0.3016, -1.0658,  ..., -1.0742, -0.1294,  0.0723],\n        ...,\n        [-0.1843, -0.0950, -1.4184,  ..., -0.6220, -1.1983,  0.6643],\n        [-0.7517, -0.9233, -0.7616,  ...,  0.0027,  0.0032,  0.5945],\n        [-1.3442, -0.7566, -1.2242,  ..., -0.5650, -0.0825,  0.2474]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\nAttention Weights (shape: torch.Size([1000, 1, 100, 100]) )\nEncoder Output (shape: torch.Size([1000, 100, 300]) )\n","output_type":"stream"}]},{"cell_type":"markdown","source":"238","metadata":{}},{"cell_type":"code","source":"encoder_layer = nn.TransformerEncoderLayer(d_model, h, d_ffn, dropout).to(device)\ntransformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(device)\n\npytorch_encoder_weights, pytorch_encoder_output = transformer_encoder(key_tensor, query_tensor, value_tensor, mask=None)\n\nprint(pytorch_encoder_weights[0])\nprint(pytorch_encoder_output[0])\n\nprint(\"Attention Weights (shape:\", pytorch_encoder_weights.shape, \")\")\nprint(\"Encoder Output (shape:\", pytorch_encoder_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:24:17.056062Z","iopub.execute_input":"2024-06-28T17:24:17.056453Z","iopub.status.idle":"2024-06-28T17:24:17.643849Z","shell.execute_reply.started":"2024-06-28T17:24:17.056425Z","shell.execute_reply":"2024-06-28T17:24:17.642615Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(d_model, h, d_ffn, dropout)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m transformer_encoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoder(encoder_layer, num_layers\u001b[38;5;241m=\u001b[39mnum_layers)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m pytorch_encoder_weights, pytorch_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(pytorch_encoder_weights[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(pytorch_encoder_output[\u001b[38;5;241m0\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: TransformerEncoder.forward() got multiple values for argument 'mask'"],"ename":"TypeError","evalue":"TransformerEncoder.forward() got multiple values for argument 'mask'","output_type":"error"}]},{"cell_type":"markdown","source":"# Pytorch's Transformer Encoder (Custom)","metadata":{}},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n        super(TransformerEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, query, key, value, src_mask=None):\n        \"\"\"\n        Pass the input through self-attention, feed-forward, and layer normalization.\n\n        Args:\n          src: Tensor - The input sequence (batch_size, seq_len, d_model).\n          src_mask: Tensor (optional) - Mask for the source sequence (batch_size, 1, 1, seq_len).\n\n        Returns:\n          Tuple[Tensor, Tensor] - The processed output (batch_size, seq_len, d_model) \n                                  and self-attention weights (batch_size, nheads, seq_len, seq_len).\n        \"\"\"\n        # Get self-attention weights along with the output\n        src, attn_weights = self.self_attn(query, key, value, attn_mask=src_mask)\n        src = src + src  # Add & Residual Connection\n\n        src = self.norm1(src)  # LayerNorm before self-attention\n        src = self.dropout(src)  # Dropout after self-attention\n\n        src = self.linear2(self.dropout(self.linear1(src)))\n        src = src + src  # Add & Residual Connection after dropout\n        src = self.norm2(src)  # LayerNorm before feed-forward\n\n        return src, attn_weights\n    \ncustom_pytorch_transformer = TransformerEncoderLayer(d_model, h, d_ffn, dropout).to(device)\ncustom_pytorch_output, custom_pytorch_output_weights = custom_pytorch_transformer(query_tensor, key_tensor, value_tensor)\n\nprint(custom_pytorch_output_weights[0])\nprint(custom_pytorch_output[0])\n\nprint(\"Attention Weights (shape:\", custom_pytorch_output_weights.shape, \")\")\nprint(\"Attention Scores (shape:\", custom_pytorch_output.shape, \")\")","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:24:29.023329Z","iopub.execute_input":"2024-06-28T17:24:29.023723Z","iopub.status.idle":"2024-06-28T17:24:29.140759Z","shell.execute_reply.started":"2024-06-28T17:24:29.02369Z","shell.execute_reply":"2024-06-28T17:24:29.139675Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        ...,\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0009]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[ 1.5555,  1.4374,  1.5129,  ...,  0.5679, -2.8997,  1.0194],\n        [ 1.5881,  1.4268,  1.5469,  ...,  0.5640, -2.8892,  1.0321],\n        [ 1.6105,  1.4418,  1.5330,  ...,  0.6094, -2.9029,  1.0125],\n        ...,\n        [ 1.5670,  1.4322,  1.5513,  ...,  0.5765, -2.8570,  1.0492],\n        [ 1.5707,  1.4129,  1.5393,  ...,  0.5673, -2.8711,  1.0418],\n        [ 1.5739,  1.4145,  1.5430,  ...,  0.6067, -2.8735,  1.0201]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\nAttention Weights (shape: torch.Size([100, 1000, 1000]) )\nAttention Scores (shape: torch.Size([1000, 100, 300]) )\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Comparison","metadata":{}},{"cell_type":"code","source":"# Our Scaled Dot Product Attention / Our Multi-Head Attention\nprint(our_attn_weights[0])\nprint(our_attn_output[0])\n\n# Pytorch's Scaled Dot Product Attention\nprint(attn_weights[0])\nprint(attn_output[0])\n\n# Pytorch's Multi-Head Attention\nprint(multi_attn_weights[0])\nprint(multi_attn_output[0])\n\n# Our Transformer Encoder\nprint(our_encoder_weights[0])\nprint(our_encoder_output[0])\n\n# Pytorch's Transformer Encoder\n# print(pytorch_encoder_weights[0])\n# print(pytorch_encoder_output[0])\n\n# Pytorch's Transformer Encoder (Custom)\nprint(custom_pytorch_output_weights[0])\nprint(custom_pytorch_output[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T01:23:07.119315Z","iopub.execute_input":"2024-06-28T01:23:07.120152Z","iopub.status.idle":"2024-06-28T01:23:07.138625Z","shell.execute_reply.started":"2024-06-28T01:23:07.12011Z","shell.execute_reply":"2024-06-28T01:23:07.137544Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"tensor([[0.0104, 0.0101, 0.0099,  ..., 0.0088, 0.0103, 0.0100],\n        [0.0105, 0.0102, 0.0097,  ..., 0.0092, 0.0109, 0.0099],\n        [0.0103, 0.0104, 0.0091,  ..., 0.0093, 0.0110, 0.0102],\n        ...,\n        [0.0102, 0.0101, 0.0087,  ..., 0.0094, 0.0104, 0.0099],\n        [0.0102, 0.0100, 0.0095,  ..., 0.0087, 0.0107, 0.0100],\n        [0.0108, 0.0101, 0.0093,  ..., 0.0090, 0.0102, 0.0104]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[0.0104, 0.0101, 0.0099,  ..., 0.0088, 0.0103, 0.0100],\n        [0.0105, 0.0102, 0.0097,  ..., 0.0092, 0.0109, 0.0099],\n        [0.0103, 0.0104, 0.0091,  ..., 0.0093, 0.0110, 0.0102],\n        ...,\n        [0.0102, 0.0101, 0.0087,  ..., 0.0094, 0.0104, 0.0099],\n        [0.0102, 0.0100, 0.0095,  ..., 0.0087, 0.0107, 0.0100],\n        [0.0108, 0.0101, 0.0093,  ..., 0.0090, 0.0102, 0.0104]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        ...,\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[[0.0104, 0.0101, 0.0099,  ..., 0.0088, 0.0103, 0.0100],\n         [0.0105, 0.0102, 0.0097,  ..., 0.0092, 0.0109, 0.0099],\n         [0.0103, 0.0104, 0.0091,  ..., 0.0093, 0.0110, 0.0102],\n         ...,\n         [0.0102, 0.0101, 0.0087,  ..., 0.0094, 0.0104, 0.0099],\n         [0.0102, 0.0100, 0.0095,  ..., 0.0087, 0.0107, 0.0100],\n         [0.0108, 0.0101, 0.0093,  ..., 0.0090, 0.0102, 0.0104]]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\ntensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0011,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        ...,\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n       device='cuda:0', grad_fn=<SelectBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"def cosine_similarity(tensor1, tensor2):\n    tensor1_norm = tensor1 / tensor1.norm(dim=-1, keepdim=True)\n    tensor2_norm = tensor2 / tensor2.norm(dim=-1, keepdim=True)\n    return (tensor1_norm * tensor2_norm).sum(dim=-1)\n\n# cosine, mse, and correlation\nsimilarity = cosine_similarity(custom_pytorch_output_weights, our_encoder_weights)\nmse = torch.mean((custom_pytorch_output_weights - our_encoder_weights) ** 2)\ncorrelation = torch.corrcoef(torch.stack((custom_pytorch_output_weights.view(-1), our_encoder_weights.view(-1))))\n\nprint(\"Cosine Similarity:\", similarity)\nprint(\"Mean Squared Error:\", mse)\nprint(\"Correlation Coefficient:\", correlation[0, 1])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T01:43:48.594556Z","iopub.execute_input":"2024-06-28T01:43:48.594906Z","iopub.status.idle":"2024-06-28T01:43:48.660045Z","shell.execute_reply.started":"2024-06-28T01:43:48.594879Z","shell.execute_reply":"2024-06-28T01:43:48.65889Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tensor1_norm \u001b[38;5;241m*\u001b[39m tensor2_norm)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cosine, mse, and correlation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_pytorch_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mour_encoder_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m mse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((custom_pytorch_output_weights \u001b[38;5;241m-\u001b[39m our_encoder_weights) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m correlation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcorrcoef(torch\u001b[38;5;241m.\u001b[39mstack((custom_pytorch_output_weights\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), our_encoder_weights\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n","Cell \u001b[0;32mIn[24], line 4\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(tensor1, tensor2)\u001b[0m\n\u001b[1;32m      2\u001b[0m tensor1_norm \u001b[38;5;241m=\u001b[39m tensor1 \u001b[38;5;241m/\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m tensor2_norm \u001b[38;5;241m=\u001b[39m tensor2 \u001b[38;5;241m/\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtensor1_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor2_norm\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (100) at non-singleton dimension 3"],"ename":"RuntimeError","evalue":"The size of tensor a (1000) must match the size of tensor b (100) at non-singleton dimension 3","output_type":"error"}]},{"cell_type":"code","source":"# def compare_similarities(attn_weights, multi_attn_output_weights):\n#     # Cosine similarity with multihead attention weights\n#     multihead_similarity = cosine_similarity(our_attn_weights, multi_attn_output_weights)\n#     # MSE with attn_weights and multihead attention weights\n#     mse_attn_multihead = torch.mean((our_attn_weights - multi_attn_output_weights) ** 2)\n#     # Correlation with attn_weights and our_attn_weights (reshape for corrcoef)\n#     correlation = torch.corrcoef(torch.stack((our_attn_weights.view(-1), multi_attn_output_weights.view(-1))))\n\n#     print(\"Cosine Similarity (Attn vs Multihead):\", multihead_similarity)\n#     print(\"Mean Squared Error (Attn vs Multihead):\", mse_attn_multihead)\n#     print(\"Correlation Coefficient:\", correlation[0, 1])\n\n# # Usage example\n# compare_similarities(attn_weights, multi_attn_output_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize","metadata":{}},{"cell_type":"code","source":"# pytorch\n\ndef plot_attention_map(attention_weights, labels = None, figsize = (8, 8), dpi = 100):\n    \"\"\"\n    Plots an attention map given the attention weights and optional labels.\n\n    Args:\n        attention_weights (torch.Tensor): A square matrix of attention weights.\n        labels (list, optional): A list of labels for the tokens. Defaults to None.\n        figsize (tuple, optional): The figure size in inches. Defaults to (8, 8).\n        dpi (int, optional): The resolution of the figure in dots per inch. Defaults to 100.\n\n    Returns:\n        None\n    \"\"\"\n    fig, ax = plt.subplots(figsize = figsize, dpi = dpi)\n    # remove detach\n    im = ax.imshow(attention_weights.cpu().detach().numpy(), cmap = 'viridis')\n\n    if labels:\n        step = max(len(labels) // 10, 1)  # Adjust tick frequency based on the number of labels\n        ax.set_xticks(range(0, len(labels), step))\n        ax.set_yticks(range(0, len(labels), step))\n        ax.set_xticklabels(labels[::step], rotation = 90)\n        ax.set_yticklabels(labels[::step])\n\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Query')\n    ax.set_title('Attention Map')\n \n    cbar = ax.figure.colorbar(im, ax = ax)\n    cbar.ax.set_ylabel('Attention Weight', rotation = -90, va = 'bottom')\n\n    fig.tight_layout()\n    plt.show()\n\n# Replace with your actual token labels\nlabels = ['Token {}'.format(i) for i in range(seq_len)]\nplot_attention_map(attn_weights[0], labels)\n\n# ours\n\ndef plot_attention_map(attention_weights, labels = None, figsize = (8, 8), dpi = 100):\n    \"\"\"\n    Plots an attention map given the attention weights and optional labels.\n\n    Args:\n        attention_weights (torch.Tensor): A square matrix of attention weights.\n        labels (list, optional): A list of labels for the tokens. Defaults to None.\n        figsize (tuple, optional): The figure size in inches. Defaults to (8, 8).\n        dpi (int, optional): The resolution of the figure in dots per inch. Defaults to 100.\n\n    Returns:\n        None\n    \"\"\"\n    fig, ax = plt.subplots(figsize = figsize, dpi = dpi)\n    # remove detach\n    im = ax.imshow(attention_weights.cpu().detach().numpy(), cmap = 'viridis')\n\n    if labels:\n        step = max(len(labels) // 10, 1)  # Adjust tick frequency based on the number of labels\n        ax.set_xticks(range(0, len(labels), step))\n        ax.set_yticks(range(0, len(labels), step))\n        ax.set_xticklabels(labels[::step], rotation = 90)\n        ax.set_yticklabels(labels[::step])\n\n    ax.set_xlabel('Key')\n    ax.set_ylabel('Query')\n    ax.set_title('Attention Map')\n \n    cbar = ax.figure.colorbar(im, ax = ax)\n    cbar.ax.set_ylabel('Attention Weight', rotation = -90, va = 'bottom')\n\n    fig.tight_layout()\n    plt.show()\n\n# Replace with your actual token labels\nlabels = ['Token {}'.format(i) for i in range(seq_len)]\nplot_attention_map(our_attn_weights[0], labels)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Comparing the attention outputs of our scaled dot product function and PyTorch's function is almost similar to comparing the attention outputs of different heads in a multi-head attention system. The differences arise from factors such as random initialization, implementation variations, and numerical precision, but the fundamental principles and functionality of the attention mechanism remain consistent.*\n\n*Random Initialization: The weights of the linear layers used for query, key, and value projections are randomly initialized in both functions. This random initialization leads to different starting points and can result in different attention patterns.*\n\n*Implementation Differences: Although both functions aim to implement the scaled dot product attention mechanism, there might be slight differences in their implementations, such as the order of operations, the use of specific PyTorch functions, or the handling of edge cases. These differences can contribute to variations in the attention outputs.*\n\n*Numerical Precision: The attention outputs can be sensitive to numerical precision, especially when dealing with large input sequences or high-dimensional representations. Differences in numerical precision between our function and PyTorch's function can lead to slight variations in the attention outputs.*","metadata":{}}]}