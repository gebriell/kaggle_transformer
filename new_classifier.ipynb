{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eac9339",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-30T16:47:55.635782Z",
     "iopub.status.busy": "2024-04-30T16:47:55.635448Z",
     "iopub.status.idle": "2024-04-30T16:48:19.942895Z",
     "shell.execute_reply": "2024-04-30T16:48:19.942007Z"
    },
    "papermill": {
     "duration": 24.314136,
     "end_time": "2024-04-30T16:48:19.944941",
     "exception": false,
     "start_time": "2024-04-30T16:47:55.630805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data file: /kaggle/input/140000/review_data.hdf5\n",
      "CUDA is available!\n",
      "GPU 0: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# For viewing and manipulating data\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors # >> alternative to gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Getting particular functions from these libraries \n",
    "from torch import Tensor\n",
    "from sklearn.utils import resample\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Using the NLTK to tokenize the text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "dataset_file_name = ''\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames:\n",
    "        file_name = os.path.join(dirname, filename)\n",
    "        if file_name.endswith('hdf5'):\n",
    "            dataset_file_name = file_name\n",
    "        else:\n",
    "            print(f'Found unexpected file: {file_name}')\n",
    "                \n",
    "print(f'Preprocessed data file: {dataset_file_name}')\n",
    "\n",
    "# Checks if a CUDA enabled GPU is available and prints out its information\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    accelerator = True\n",
    "\n",
    "else:\n",
    "    accelerator = False\n",
    "    print(\"CUDA is not available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "VERBOSE = True\n",
    "def printv(text):\n",
    "    if VERBOSE: print('VERBOSE:', text)\n",
    "    return\n",
    "\n",
    "def showV(text):\n",
    "    '''unconditional verbose output'''\n",
    "    print('VERBOSE:', text)\n",
    "    return\n",
    "\n",
    "DEV = False\n",
    "def printd(text):\n",
    "    if DEV: print('DEV:', text)\n",
    "    return\n",
    "\n",
    "def showD(text):\n",
    "    '''unconditional DEV output'''\n",
    "    print('DEV:', text)  #<< 4/12/24 changed \"VERBOSE\" to \"DEV\"\n",
    "    return\n",
    "\n",
    "showCellCompletion = True  #<< 4/12/24 set default to True\n",
    "def showC(text):\n",
    "    if showCellCompletion:\n",
    "        print('Cell complete:', text)\n",
    "    return\n",
    "\n",
    "import subprocess\n",
    "showNv = True\n",
    "accelerator = True\n",
    "\n",
    "def printNv():\n",
    "    if not showNv or not accelerator: return\n",
    "    mem_usage = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "    print(mem_usage.stdout.decode('utf-8'))\n",
    "\n",
    "showMemoryAllocation = True\n",
    "def printM():\n",
    "    if not showMemoryAllocation: return\n",
    "    print(f\"Total allocated memory: {torch.cuda.memory_allocated()} bytes\")\n",
    "    \n",
    "import h5py\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa1d1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:48:19.952316Z",
     "iopub.status.busy": "2024-04-30T16:48:19.951556Z",
     "iopub.status.idle": "2024-04-30T16:51:11.997914Z",
     "shell.execute_reply": "2024-04-30T16:51:11.996927Z"
    },
    "papermill": {
     "duration": 172.05498,
     "end_time": "2024-04-30T16:51:12.002959",
     "exception": false,
     "start_time": "2024-04-30T16:48:19.947979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140000, 100, 300])\n",
      "torch.Size([140000])\n"
     ]
    }
   ],
   "source": [
    "file_path = '/kaggle/input/140000/review_data.hdf5'\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    # Access the datasets within the HDF5 file\n",
    "    text_reviews_dataset = hf['text_reviews']\n",
    "    ratings_dataset = hf['ratings']\n",
    "\n",
    "    # Convert the datasets to PyTorch tensors\n",
    "    text_reviews = torch.from_numpy(text_reviews_dataset[:])\n",
    "    ratings = torch.from_numpy(ratings_dataset[:])\n",
    "\n",
    "# check loaded tensors\n",
    "print(text_reviews.shape)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e590195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:51:12.011644Z",
     "iopub.status.busy": "2024-04-30T16:51:12.011365Z",
     "iopub.status.idle": "2024-04-30T16:51:12.091454Z",
     "shell.execute_reply": "2024-04-30T16:51:12.090291Z"
    },
    "papermill": {
     "duration": 0.086026,
     "end_time": "2024-04-30T16:51:12.093381",
     "exception": false,
     "start_time": "2024-04-30T16:51:12.007355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112000\n",
      "28000\n"
     ]
    }
   ],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, text_reviews, ratings):\n",
    "        self.text_reviews = text_reviews\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.text_reviews[index]\n",
    "        rating = self.ratings[index]\n",
    "        return review, rating\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ReviewDataset(text_reviews, ratings)\n",
    "\n",
    "# Perform stratified splitting on indices\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "train_indices, val_indices = next(sss.split(range(len(dataset)), dataset.ratings))\n",
    "\n",
    "# Create subsets using the indices\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "print (len(train_indices))\n",
    "print (len(val_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489364c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:51:12.100648Z",
     "iopub.status.busy": "2024-04-30T16:51:12.100347Z",
     "iopub.status.idle": "2024-04-30T16:51:13.083965Z",
     "shell.execute_reply": "2024-04-30T16:51:13.082967Z"
    },
    "papermill": {
     "duration": 0.989772,
     "end_time": "2024-04-30T16:51:13.086228",
     "exception": false,
     "start_time": "2024-04-30T16:51:12.096456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Number of reviews: 112000\n",
      "Number of reviews per rating:\n",
      "Rating 1: 22400 reviews\n",
      "Rating 2: 22400 reviews\n",
      "Rating 3: 22400 reviews\n",
      "Rating 4: 22400 reviews\n",
      "Rating 5: 22400 reviews\n",
      "\n",
      "Validation Set:\n",
      "Number of reviews: 28000\n",
      "Number of reviews per rating:\n",
      "Rating 1: 5600 reviews\n",
      "Rating 2: 5600 reviews\n",
      "Rating 3: 5600 reviews\n",
      "Rating 4: 5600 reviews\n",
      "Rating 5: 5600 reviews\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\")\n",
    "print(\"Number of reviews:\", len(train_dataset))\n",
    "print(\"Number of reviews per rating:\")\n",
    "train_ratings = [rating.item() for _, rating in train_dataset]\n",
    "train_rating_counts = Counter(train_ratings)\n",
    "for rating, count in sorted(train_rating_counts.items()):\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(\"Number of reviews:\", len(val_dataset))\n",
    "print(\"Number of reviews per rating:\")\n",
    "val_ratings = [rating.item() for _, rating in val_dataset]\n",
    "val_rating_counts = Counter(val_ratings)\n",
    "for rating, count in sorted(val_rating_counts.items()):\n",
    "    print(f\"Rating {rating}: {count} reviews\")\n",
    "    \n",
    "assert len(train_dataset) == 112000, f\"Expected 112000 training reviews, got {len(train_dataset)}\"\n",
    "assert len(val_dataset) == 28000, f\"Expected 28000 validation reviews, got {len(val_dataset)}\"\n",
    "\n",
    "for rating in sorted(train_rating_counts.keys()):\n",
    "    train_count = train_rating_counts[rating]\n",
    "    val_count = val_rating_counts[rating]\n",
    "    assert train_count == val_count * 4, f\"Rating {rating}: Expected 4 times more reviews in training set than validation set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e2aeb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:51:13.094346Z",
     "iopub.status.busy": "2024-04-30T16:51:13.094042Z",
     "iopub.status.idle": "2024-04-30T16:51:15.003809Z",
     "shell.execute_reply": "2024-04-30T16:51:15.002870Z"
    },
    "papermill": {
     "duration": 1.916472,
     "end_time": "2024-04-30T16:51:15.006141",
     "exception": false,
     "start_time": "2024-04-30T16:51:13.089669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell complete: Hyperparameters defined\n",
      "NeuralNetClassifier(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30000, out_features=2048, bias=True)\n",
      "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (11): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (15): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (17): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (19): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (21): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (23): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (25): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (27): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (28): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (29): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (31): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (33): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (34): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (35): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (37): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (39): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (40): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (41): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (43): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (44): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (45): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (46): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (47): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (48): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (49): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=2048, out_features=5, bias=True)\n",
      "  (relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters for the model\n",
    "d_model = 300  # Should match the embedding dimension of your word embeddings\n",
    "seq_len = 100 #<< 4/13/24 100  # Maximum sequence length\n",
    "dropout = 0.1  # Adjust the dropout if needed\n",
    "\n",
    "num_layers = 25 # depth of our network\n",
    "input_size = d_model  # match the output dim of your ff_net\n",
    "num_classes = 5  # our ratings (1 - 5)\n",
    "hidden_size = 2048 # 2^n\n",
    "\n",
    "eps    = 1e-05 # epsilon value to prevent the standard deviation from becoming zero\n",
    "epochs = 200 #<< 100\n",
    "learning_rate = 0.001\n",
    "weight_decay  = 0.01\n",
    "\n",
    "showC('Hyperparameters defined')\n",
    "\n",
    "class NeuralNetClassifier(nn.Module):\n",
    "    def __init__(self, r_size, v_size, num_classes, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout):\n",
    "        super(NeuralNetClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(r_size * v_size, hidden_size))\n",
    "        self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.hidden_layers.append(nn.BatchNorm1d(hidden_size))\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "classifier = NeuralNetClassifier(seq_len, d_model, num_classes, hidden_size, num_layers, dropout)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff3de5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:51:15.014601Z",
     "iopub.status.busy": "2024-04-30T16:51:15.014311Z",
     "iopub.status.idle": "2024-05-01T00:14:08.816072Z",
     "shell.execute_reply": "2024-05-01T00:14:08.814849Z"
    },
    "papermill": {
     "duration": 26573.826968,
     "end_time": "2024-05-01T00:14:08.836814",
     "exception": false,
     "start_time": "2024-04-30T16:51:15.009846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERBOSE: Epoch [1 / 200] Loss: 1.5971527099609375\n",
      "VERBOSE: Epoch [2 / 200] Loss: 1.6561734676361084\n",
      "VERBOSE: Epoch [3 / 200] Loss: 1.697770118713379\n",
      "VERBOSE: Epoch [4 / 200] Loss: 1.6198837757110596\n",
      "VERBOSE: Epoch [5 / 200] Loss: 1.6218584775924683\n",
      "VERBOSE: Epoch [6 / 200] Loss: 1.6017833948135376\n",
      "VERBOSE: Epoch [7 / 200] Loss: 1.6059505939483643\n",
      "VERBOSE: Epoch [8 / 200] Loss: 1.6104587316513062\n",
      "VERBOSE: Epoch [9 / 200] Loss: 1.604677438735962\n",
      "VERBOSE: Epoch [10 / 200] Loss: 1.5903916358947754\n",
      "VERBOSE: Epoch [11 / 200] Loss: 1.6307064294815063\n",
      "VERBOSE: Epoch [12 / 200] Loss: 1.6124794483184814\n",
      "VERBOSE: Epoch [13 / 200] Loss: 1.5857295989990234\n",
      "VERBOSE: Epoch [14 / 200] Loss: 1.575007677078247\n",
      "VERBOSE: Epoch [15 / 200] Loss: 1.5454907417297363\n",
      "VERBOSE: Epoch [16 / 200] Loss: 1.6017513275146484\n",
      "VERBOSE: Epoch [17 / 200] Loss: 1.5645109415054321\n",
      "VERBOSE: Epoch [18 / 200] Loss: 1.3751521110534668\n",
      "VERBOSE: Epoch [19 / 200] Loss: 1.3982983827590942\n",
      "VERBOSE: Epoch [20 / 200] Loss: 1.5476104021072388\n",
      "VERBOSE: Epoch [21 / 200] Loss: 1.4532763957977295\n",
      "VERBOSE: Epoch [22 / 200] Loss: 1.3649086952209473\n",
      "VERBOSE: Epoch [23 / 200] Loss: 1.6870238780975342\n",
      "VERBOSE: Epoch [24 / 200] Loss: 1.2690755128860474\n",
      "VERBOSE: Epoch [25 / 200] Loss: 0.9972608089447021\n",
      "VERBOSE: Epoch [26 / 200] Loss: 1.3399864435195923\n",
      "VERBOSE: Epoch [27 / 200] Loss: 0.8641273975372314\n",
      "VERBOSE: Epoch [28 / 200] Loss: 0.9821414351463318\n",
      "VERBOSE: Epoch [29 / 200] Loss: 1.0321941375732422\n",
      "VERBOSE: Epoch [30 / 200] Loss: 0.9324796795845032\n",
      "VERBOSE: Epoch [31 / 200] Loss: 1.0474385023117065\n",
      "VERBOSE: Epoch [32 / 200] Loss: 0.7958160638809204\n",
      "VERBOSE: Epoch [33 / 200] Loss: 0.9670816659927368\n",
      "VERBOSE: Epoch [34 / 200] Loss: 0.47112080454826355\n",
      "VERBOSE: Epoch [35 / 200] Loss: 0.3029390871524811\n",
      "VERBOSE: Epoch [36 / 200] Loss: 0.45587751269340515\n",
      "VERBOSE: Epoch [37 / 200] Loss: 0.38010233640670776\n",
      "VERBOSE: Epoch [38 / 200] Loss: 0.5159622430801392\n",
      "VERBOSE: Epoch [39 / 200] Loss: 0.5304231643676758\n",
      "VERBOSE: Epoch [40 / 200] Loss: 0.46298497915267944\n",
      "VERBOSE: Epoch [41 / 200] Loss: 0.20026792585849762\n",
      "VERBOSE: Epoch [42 / 200] Loss: 0.23411524295806885\n",
      "VERBOSE: Epoch [43 / 200] Loss: 0.7501349449157715\n",
      "VERBOSE: Epoch [44 / 200] Loss: 0.1635659784078598\n",
      "VERBOSE: Epoch [45 / 200] Loss: 0.2993108928203583\n",
      "VERBOSE: Epoch [46 / 200] Loss: 0.2336895614862442\n",
      "VERBOSE: Epoch [47 / 200] Loss: 0.23480907082557678\n",
      "VERBOSE: Epoch [48 / 200] Loss: 0.28421178460121155\n",
      "VERBOSE: Epoch [49 / 200] Loss: 0.20603056252002716\n",
      "VERBOSE: Epoch [50 / 200] Loss: 0.07669983059167862\n",
      "VERBOSE: Epoch [51 / 200] Loss: 0.14465059340000153\n",
      "VERBOSE: Epoch [52 / 200] Loss: 0.17221182584762573\n",
      "VERBOSE: Epoch [53 / 200] Loss: 0.32528284192085266\n",
      "VERBOSE: Epoch [54 / 200] Loss: 0.2168758511543274\n",
      "VERBOSE: Epoch [55 / 200] Loss: 0.2963063418865204\n",
      "VERBOSE: Epoch [56 / 200] Loss: 0.5290566682815552\n",
      "VERBOSE: Epoch [57 / 200] Loss: 0.17196248471736908\n",
      "VERBOSE: Epoch [58 / 200] Loss: 0.1344081610441208\n",
      "VERBOSE: Epoch [59 / 200] Loss: 0.16449469327926636\n",
      "VERBOSE: Epoch [60 / 200] Loss: 0.31016260385513306\n",
      "VERBOSE: Epoch [61 / 200] Loss: 0.1284039318561554\n",
      "VERBOSE: Epoch [62 / 200] Loss: 0.17955349385738373\n",
      "VERBOSE: Epoch [63 / 200] Loss: 0.23095576465129852\n",
      "VERBOSE: Epoch [64 / 200] Loss: 0.06863344460725784\n",
      "VERBOSE: Epoch [65 / 200] Loss: 0.3472324013710022\n",
      "VERBOSE: Epoch [66 / 200] Loss: 0.0757916122674942\n",
      "VERBOSE: Epoch [67 / 200] Loss: 0.19061174988746643\n",
      "VERBOSE: Epoch [68 / 200] Loss: 0.16579917073249817\n",
      "VERBOSE: Epoch [69 / 200] Loss: 0.09817157685756683\n",
      "VERBOSE: Epoch [70 / 200] Loss: 0.020664328709244728\n",
      "VERBOSE: Epoch [71 / 200] Loss: 0.09296394139528275\n",
      "VERBOSE: Epoch [72 / 200] Loss: 0.46365177631378174\n",
      "VERBOSE: Epoch [73 / 200] Loss: 0.10486441105604172\n",
      "VERBOSE: Epoch [74 / 200] Loss: 0.1415480375289917\n",
      "VERBOSE: Epoch [75 / 200] Loss: 0.1575925350189209\n",
      "VERBOSE: Epoch [76 / 200] Loss: 0.07579880952835083\n",
      "VERBOSE: Epoch [77 / 200] Loss: 0.22512979805469513\n",
      "VERBOSE: Epoch [78 / 200] Loss: 0.3355323076248169\n",
      "VERBOSE: Epoch [79 / 200] Loss: 0.22064803540706635\n",
      "VERBOSE: Epoch [80 / 200] Loss: 0.5358003973960876\n",
      "VERBOSE: Epoch [81 / 200] Loss: 0.06269603222608566\n",
      "VERBOSE: Epoch [82 / 200] Loss: 0.4335218071937561\n",
      "VERBOSE: Epoch [83 / 200] Loss: 0.10373544692993164\n",
      "VERBOSE: Epoch [84 / 200] Loss: 0.23690320551395416\n",
      "VERBOSE: Epoch [85 / 200] Loss: 0.36746281385421753\n",
      "VERBOSE: Epoch [86 / 200] Loss: 0.3002917766571045\n",
      "VERBOSE: Epoch [87 / 200] Loss: 0.2868448495864868\n",
      "VERBOSE: Epoch [88 / 200] Loss: 0.13489413261413574\n",
      "VERBOSE: Epoch [89 / 200] Loss: 0.1471203714609146\n",
      "VERBOSE: Epoch [90 / 200] Loss: 0.2981039583683014\n",
      "VERBOSE: Epoch [91 / 200] Loss: 0.18455767631530762\n",
      "VERBOSE: Epoch [92 / 200] Loss: 0.15064696967601776\n",
      "VERBOSE: Epoch [93 / 200] Loss: 0.1878979355096817\n",
      "VERBOSE: Epoch [94 / 200] Loss: 0.45791923999786377\n",
      "VERBOSE: Epoch [95 / 200] Loss: 0.05609649792313576\n",
      "VERBOSE: Epoch [96 / 200] Loss: 0.20034143328666687\n",
      "VERBOSE: Epoch [97 / 200] Loss: 0.02085909992456436\n",
      "VERBOSE: Epoch [98 / 200] Loss: 0.07561850547790527\n",
      "VERBOSE: Epoch [99 / 200] Loss: 0.14739418029785156\n",
      "VERBOSE: Epoch [100 / 200] Loss: 0.07556266337633133\n",
      "VERBOSE: Epoch [101 / 200] Loss: 0.22422170639038086\n",
      "VERBOSE: Epoch [102 / 200] Loss: 0.11127670109272003\n",
      "VERBOSE: Epoch [103 / 200] Loss: 0.16533637046813965\n",
      "VERBOSE: Epoch [104 / 200] Loss: 0.15740545094013214\n",
      "VERBOSE: Epoch [105 / 200] Loss: 0.03766326606273651\n",
      "VERBOSE: Epoch [106 / 200] Loss: 0.1600325107574463\n",
      "VERBOSE: Epoch [107 / 200] Loss: 0.23355764150619507\n",
      "VERBOSE: Epoch [108 / 200] Loss: 0.12477806210517883\n",
      "VERBOSE: Epoch [109 / 200] Loss: 0.06939680129289627\n",
      "VERBOSE: Epoch [110 / 200] Loss: 0.10117584466934204\n",
      "VERBOSE: Epoch [111 / 200] Loss: 0.0621645487844944\n",
      "VERBOSE: Epoch [112 / 200] Loss: 0.21014636754989624\n",
      "VERBOSE: Epoch [113 / 200] Loss: 0.06213700771331787\n",
      "VERBOSE: Epoch [114 / 200] Loss: 0.24905602633953094\n",
      "VERBOSE: Epoch [115 / 200] Loss: 0.46774718165397644\n",
      "VERBOSE: Epoch [116 / 200] Loss: 0.0768289715051651\n",
      "VERBOSE: Epoch [117 / 200] Loss: 0.13434408605098724\n",
      "VERBOSE: Epoch [118 / 200] Loss: 0.14786657691001892\n",
      "VERBOSE: Epoch [119 / 200] Loss: 0.30894744396209717\n",
      "VERBOSE: Epoch [120 / 200] Loss: 0.08680694550275803\n",
      "VERBOSE: Epoch [121 / 200] Loss: 0.4246971607208252\n",
      "VERBOSE: Epoch [122 / 200] Loss: 0.2532680034637451\n",
      "VERBOSE: Epoch [123 / 200] Loss: 0.45798560976982117\n",
      "VERBOSE: Epoch [124 / 200] Loss: 0.09444872289896011\n",
      "VERBOSE: Epoch [125 / 200] Loss: 0.5385361909866333\n",
      "VERBOSE: Epoch [126 / 200] Loss: 0.1523112654685974\n",
      "VERBOSE: Epoch [127 / 200] Loss: 0.021612003445625305\n",
      "VERBOSE: Epoch [128 / 200] Loss: 0.09754914790391922\n",
      "VERBOSE: Epoch [129 / 200] Loss: 0.11755754053592682\n",
      "VERBOSE: Epoch [130 / 200] Loss: 0.20558738708496094\n",
      "VERBOSE: Epoch [131 / 200] Loss: 0.19156302511692047\n",
      "VERBOSE: Epoch [132 / 200] Loss: 0.18570655584335327\n",
      "VERBOSE: Epoch [133 / 200] Loss: 0.3232247829437256\n",
      "VERBOSE: Epoch [134 / 200] Loss: 0.19582854211330414\n",
      "VERBOSE: Epoch [135 / 200] Loss: 0.34262022376060486\n",
      "VERBOSE: Epoch [136 / 200] Loss: 0.24355798959732056\n",
      "VERBOSE: Epoch [137 / 200] Loss: 0.3815929591655731\n",
      "VERBOSE: Epoch [138 / 200] Loss: 0.06665793806314468\n",
      "VERBOSE: Epoch [139 / 200] Loss: 0.05583478510379791\n",
      "VERBOSE: Epoch [140 / 200] Loss: 0.48614200949668884\n",
      "VERBOSE: Epoch [141 / 200] Loss: 0.16158150136470795\n",
      "VERBOSE: Epoch [142 / 200] Loss: 0.127903014421463\n",
      "VERBOSE: Epoch [143 / 200] Loss: 0.11072816699743271\n",
      "VERBOSE: Epoch [144 / 200] Loss: 0.22884804010391235\n",
      "VERBOSE: Epoch [145 / 200] Loss: 0.18103951215744019\n",
      "VERBOSE: Epoch [146 / 200] Loss: 0.06472936272621155\n",
      "VERBOSE: Epoch [147 / 200] Loss: 0.4144546687602997\n",
      "VERBOSE: Epoch [148 / 200] Loss: 0.18214839696884155\n",
      "VERBOSE: Epoch [149 / 200] Loss: 0.12328032404184341\n",
      "VERBOSE: Epoch [150 / 200] Loss: 0.03958720713853836\n",
      "VERBOSE: Epoch [151 / 200] Loss: 0.25205299258232117\n",
      "VERBOSE: Epoch [152 / 200] Loss: 0.23040825128555298\n",
      "VERBOSE: Epoch [153 / 200] Loss: 0.09448404610157013\n",
      "VERBOSE: Epoch [154 / 200] Loss: 0.08993604779243469\n",
      "VERBOSE: Epoch [155 / 200] Loss: 0.27677321434020996\n",
      "VERBOSE: Epoch [156 / 200] Loss: 0.1082058697938919\n",
      "VERBOSE: Epoch [157 / 200] Loss: 0.2946593165397644\n",
      "VERBOSE: Epoch [158 / 200] Loss: 0.07137174159288406\n",
      "VERBOSE: Epoch [159 / 200] Loss: 0.031637273728847504\n",
      "VERBOSE: Epoch [160 / 200] Loss: 0.21136802434921265\n",
      "VERBOSE: Epoch [161 / 200] Loss: 0.2418043166399002\n",
      "VERBOSE: Epoch [162 / 200] Loss: 0.23574796319007874\n",
      "VERBOSE: Epoch [163 / 200] Loss: 0.11627771705389023\n",
      "VERBOSE: Epoch [164 / 200] Loss: 0.09306551516056061\n",
      "VERBOSE: Epoch [165 / 200] Loss: 0.1024000495672226\n",
      "VERBOSE: Epoch [166 / 200] Loss: 0.3357546627521515\n",
      "VERBOSE: Epoch [167 / 200] Loss: 0.05982745438814163\n",
      "VERBOSE: Epoch [168 / 200] Loss: 0.13774138689041138\n",
      "VERBOSE: Epoch [169 / 200] Loss: 0.05820903927087784\n",
      "VERBOSE: Epoch [170 / 200] Loss: 0.09992572665214539\n",
      "VERBOSE: Epoch [171 / 200] Loss: 0.22221505641937256\n",
      "VERBOSE: Epoch [172 / 200] Loss: 0.09458194673061371\n",
      "VERBOSE: Epoch [173 / 200] Loss: 0.28757429122924805\n",
      "VERBOSE: Epoch [174 / 200] Loss: 0.3183073401451111\n",
      "VERBOSE: Epoch [175 / 200] Loss: 0.04995577037334442\n",
      "VERBOSE: Epoch [176 / 200] Loss: 0.6028363704681396\n",
      "VERBOSE: Epoch [177 / 200] Loss: 0.1014300137758255\n",
      "VERBOSE: Epoch [178 / 200] Loss: 0.1011626273393631\n",
      "VERBOSE: Epoch [179 / 200] Loss: 0.27767348289489746\n",
      "VERBOSE: Epoch [180 / 200] Loss: 0.2870787978172302\n",
      "VERBOSE: Epoch [181 / 200] Loss: 0.06625105440616608\n",
      "VERBOSE: Epoch [182 / 200] Loss: 0.3556087017059326\n",
      "VERBOSE: Epoch [183 / 200] Loss: 0.1318192183971405\n",
      "VERBOSE: Epoch [184 / 200] Loss: 0.2920290529727936\n",
      "VERBOSE: Epoch [185 / 200] Loss: 0.3062976896762848\n",
      "VERBOSE: Epoch [186 / 200] Loss: 0.08115553855895996\n",
      "VERBOSE: Epoch [187 / 200] Loss: 0.10365951806306839\n",
      "VERBOSE: Epoch [188 / 200] Loss: 0.1060653105378151\n",
      "VERBOSE: Epoch [189 / 200] Loss: 0.08873748034238815\n",
      "VERBOSE: Epoch [190 / 200] Loss: 0.11760880798101425\n",
      "VERBOSE: Epoch [191 / 200] Loss: 0.14477787911891937\n",
      "VERBOSE: Epoch [192 / 200] Loss: 0.14472827315330505\n",
      "VERBOSE: Epoch [193 / 200] Loss: 0.3061777651309967\n",
      "VERBOSE: Epoch [194 / 200] Loss: 0.31422728300094604\n",
      "VERBOSE: Epoch [195 / 200] Loss: 0.4310801923274994\n",
      "VERBOSE: Epoch [196 / 200] Loss: 0.1070287749171257\n",
      "VERBOSE: Epoch [197 / 200] Loss: 0.3857078552246094\n",
      "VERBOSE: Epoch [198 / 200] Loss: 0.16408094763755798\n",
      "VERBOSE: Epoch [199 / 200] Loss: 0.24821792542934418\n",
      "VERBOSE: Epoch [200 / 200] Loss: 0.12179183214902878\n",
      "VERBOSE: Last loss: Epoch [200 / 200] Loss: 0.12179183214902878\n",
      "VERBOSE: Smallest losses\n",
      "VERBOSE:     Loss: 0.020664328709244728, epoch = 70\n",
      "VERBOSE:     Loss: 0.02085909992456436, epoch = 97\n",
      "VERBOSE:     Loss: 0.021612003445625305, epoch = 127\n",
      "Cell complete: training complete\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "\n",
    "optimizer = optim.AdamW(classifier.parameters(), \n",
    "                        lr = learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "DEV = True\n",
    "\n",
    "# Training loop\n",
    "losses = {} #<< track losses\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets in train_loader : \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs.to(device)\n",
    "        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n",
    "        inputs = torch.reshape(inputs, (inputs.size(0), -1)).to(device) # get current batch size\n",
    "\n",
    "        outputs = classifier(inputs).to(device)\n",
    "\n",
    "        # output is a 32 x 6 tensor of floats,\n",
    "        # targets will be a 32 x 1 tensor of ints\n",
    "        loss = criterion(outputs.to(device), targets.to(device).long())\n",
    "        loss.backward(retain_graph = True)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    losses[loss.item()] = epoch + 1\n",
    "\n",
    "    printv(f'Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n",
    "\n",
    "if VERBOSE:\n",
    "    printv(f'Last loss: Epoch [{epoch + 1} / {epochs}] Loss: {loss.item()}')\n",
    "    smallest_losses = sorted(list(losses.keys()))\n",
    "    printv('Smallest losses')\n",
    "    for idx in range(3):\n",
    "        l = smallest_losses[idx]\n",
    "        printv(f'    Loss: {l}, epoch = {losses[l]}')\n",
    "showC(f'training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca048898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T00:14:08.876739Z",
     "iopub.status.busy": "2024-05-01T00:14:08.876064Z",
     "iopub.status.idle": "2024-05-01T00:14:14.969020Z",
     "shell.execute_reply": "2024-05-01T00:14:14.967814Z"
    },
    "papermill": {
     "duration": 6.116028,
     "end_time": "2024-05-01T00:14:14.971219",
     "exception": false,
     "start_time": "2024-05-01T00:14:08.855191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5354\n",
      "Validation Precision: 0.5568\n",
      "Validation Recall: 0.5354\n",
      "Validation F1-score: 0.5351\n",
      "Confusion Matrix:\n",
      "[[3648  847  349  584  172]\n",
      " [ 990 3016  472  913  209]\n",
      " [ 641  781 2382 1446  350]\n",
      " [ 500  448  413 3430  809]\n",
      " [ 677  251  214 1943 2515]]\n",
      "The model may have high bias (underfitting). Consider increasing model complexity.\n"
     ]
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "classifier.eval() \n",
    "\n",
    "# Tracking variables\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# Evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.reshape(inputs.shape[0], -1).to(device)\n",
    "        targets = targets.to(device) - 1  # Convert ratings from [1, 5] to [0, 4]\n",
    "\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        predictions.extend(predicted.tolist())\n",
    "        actuals.extend(targets.tolist())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(actuals, predictions)\n",
    "precision = precision_score(actuals, predictions, average='weighted')\n",
    "recall = recall_score(actuals, predictions, average='weighted')\n",
    "f1 = f1_score(actuals, predictions, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation F1-score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Assess bias and variance\n",
    "if accuracy < 0.7:  # Adjust the threshold as per your requirements\n",
    "    print(\"The model may have high bias (underfitting). Consider increasing model complexity.\")\n",
    "elif accuracy > 0.95:  # Adjust the threshold as per your requirements\n",
    "    print(\"The model may have high variance (overfitting). Consider regularization techniques.\")\n",
    "else:\n",
    "    print(\"The model seems to have a good balance between bias and variance.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4882008,
     "sourceId": 8232040,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26785.503372,
   "end_time": "2024-05-01T00:14:18.487378",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-30T16:47:52.984006",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
